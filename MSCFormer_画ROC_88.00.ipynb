{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c6f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 3, 1000]           1,376\n",
      "            Conv2d-2          [-1, 16, 1, 1000]              64\n",
      "       BatchNorm2d-3          [-1, 16, 1, 1000]              32\n",
      "               ELU-4          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-5            [-1, 16, 1, 19]               0\n",
      "           Dropout-6            [-1, 16, 1, 19]               0\n",
      "            Conv2d-7          [-1, 16, 3, 1000]           1,056\n",
      "            Conv2d-8          [-1, 16, 1, 1000]              64\n",
      "       BatchNorm2d-9          [-1, 16, 1, 1000]              32\n",
      "              ELU-10          [-1, 16, 1, 1000]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 19]               0\n",
      "          Dropout-12            [-1, 16, 1, 19]               0\n",
      "           Conv2d-13          [-1, 16, 3, 1000]             736\n",
      "           Conv2d-14          [-1, 16, 1, 1000]              64\n",
      "      BatchNorm2d-15          [-1, 16, 1, 1000]              32\n",
      "              ELU-16          [-1, 16, 1, 1000]               0\n",
      "        AvgPool2d-17            [-1, 16, 1, 19]               0\n",
      "          Dropout-18            [-1, 16, 1, 19]               0\n",
      "        Rearrange-19               [-1, 19, 48]               0\n",
      "PatchEmbeddingCNN-20               [-1, 19, 48]               0\n",
      "          Dropout-21               [-1, 20, 48]               0\n",
      "PositioinalEncoding-22               [-1, 20, 48]               0\n",
      "           Linear-23               [-1, 20, 48]           2,352\n",
      "           Linear-24               [-1, 20, 48]           2,352\n",
      "           Linear-25               [-1, 20, 48]           2,352\n",
      "          Dropout-26            [-1, 8, 20, 20]               0\n",
      "           Linear-27               [-1, 20, 48]           2,352\n",
      "MultiHeadAttention-28               [-1, 20, 48]               0\n",
      "          Dropout-29               [-1, 20, 48]               0\n",
      "        LayerNorm-30               [-1, 20, 48]              96\n",
      "      ResidualAdd-31               [-1, 20, 48]               0\n",
      "           Linear-32              [-1, 20, 192]           9,408\n",
      "             GELU-33              [-1, 20, 192]               0\n",
      "          Dropout-34              [-1, 20, 192]               0\n",
      "           Linear-35               [-1, 20, 48]           9,264\n",
      "          Dropout-36               [-1, 20, 48]               0\n",
      "        LayerNorm-37               [-1, 20, 48]              96\n",
      "      ResidualAdd-38               [-1, 20, 48]               0\n",
      "           Linear-39               [-1, 20, 48]           2,352\n",
      "           Linear-40               [-1, 20, 48]           2,352\n",
      "           Linear-41               [-1, 20, 48]           2,352\n",
      "          Dropout-42            [-1, 8, 20, 20]               0\n",
      "           Linear-43               [-1, 20, 48]           2,352\n",
      "MultiHeadAttention-44               [-1, 20, 48]               0\n",
      "          Dropout-45               [-1, 20, 48]               0\n",
      "        LayerNorm-46               [-1, 20, 48]              96\n",
      "      ResidualAdd-47               [-1, 20, 48]               0\n",
      "           Linear-48              [-1, 20, 192]           9,408\n",
      "             GELU-49              [-1, 20, 192]               0\n",
      "          Dropout-50              [-1, 20, 192]               0\n",
      "           Linear-51               [-1, 20, 48]           9,264\n",
      "          Dropout-52               [-1, 20, 48]               0\n",
      "        LayerNorm-53               [-1, 20, 48]              96\n",
      "      ResidualAdd-54               [-1, 20, 48]               0\n",
      "           Linear-55               [-1, 20, 48]           2,352\n",
      "           Linear-56               [-1, 20, 48]           2,352\n",
      "           Linear-57               [-1, 20, 48]           2,352\n",
      "          Dropout-58            [-1, 8, 20, 20]               0\n",
      "           Linear-59               [-1, 20, 48]           2,352\n",
      "MultiHeadAttention-60               [-1, 20, 48]               0\n",
      "          Dropout-61               [-1, 20, 48]               0\n",
      "        LayerNorm-62               [-1, 20, 48]              96\n",
      "      ResidualAdd-63               [-1, 20, 48]               0\n",
      "           Linear-64              [-1, 20, 192]           9,408\n",
      "             GELU-65              [-1, 20, 192]               0\n",
      "          Dropout-66              [-1, 20, 192]               0\n",
      "           Linear-67               [-1, 20, 48]           9,264\n",
      "          Dropout-68               [-1, 20, 48]               0\n",
      "        LayerNorm-69               [-1, 20, 48]              96\n",
      "      ResidualAdd-70               [-1, 20, 48]               0\n",
      "           Linear-71               [-1, 20, 48]           2,352\n",
      "           Linear-72               [-1, 20, 48]           2,352\n",
      "           Linear-73               [-1, 20, 48]           2,352\n",
      "          Dropout-74            [-1, 8, 20, 20]               0\n",
      "           Linear-75               [-1, 20, 48]           2,352\n",
      "MultiHeadAttention-76               [-1, 20, 48]               0\n",
      "          Dropout-77               [-1, 20, 48]               0\n",
      "        LayerNorm-78               [-1, 20, 48]              96\n",
      "      ResidualAdd-79               [-1, 20, 48]               0\n",
      "           Linear-80              [-1, 20, 192]           9,408\n",
      "             GELU-81              [-1, 20, 192]               0\n",
      "          Dropout-82              [-1, 20, 192]               0\n",
      "           Linear-83               [-1, 20, 48]           9,264\n",
      "          Dropout-84               [-1, 20, 48]               0\n",
      "        LayerNorm-85               [-1, 20, 48]              96\n",
      "      ResidualAdd-86               [-1, 20, 48]               0\n",
      "           Linear-87               [-1, 20, 48]           2,352\n",
      "           Linear-88               [-1, 20, 48]           2,352\n",
      "           Linear-89               [-1, 20, 48]           2,352\n",
      "          Dropout-90            [-1, 8, 20, 20]               0\n",
      "           Linear-91               [-1, 20, 48]           2,352\n",
      "MultiHeadAttention-92               [-1, 20, 48]               0\n",
      "          Dropout-93               [-1, 20, 48]               0\n",
      "        LayerNorm-94               [-1, 20, 48]              96\n",
      "      ResidualAdd-95               [-1, 20, 48]               0\n",
      "           Linear-96              [-1, 20, 192]           9,408\n",
      "             GELU-97              [-1, 20, 192]               0\n",
      "          Dropout-98              [-1, 20, 192]               0\n",
      "           Linear-99               [-1, 20, 48]           9,264\n",
      "         Dropout-100               [-1, 20, 48]               0\n",
      "       LayerNorm-101               [-1, 20, 48]              96\n",
      "     ResidualAdd-102               [-1, 20, 48]               0\n",
      "         Dropout-103                   [-1, 48]               0\n",
      "          Linear-104                    [-1, 2]              98\n",
      "================================================================\n",
      "Total params: 144,914\n",
      "Trainable params: 144,914\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.24\n",
      "Params size (MB): 0.55\n",
      "Estimated Total Size (MB): 3.81\n",
      "----------------------------------------------------------------\n",
      "Sat Aug 17 16:13:18 2024\n",
      "seed is 1170\n",
      "Subject 1\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 1 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "1_0 train_acc: 0.5000 train_loss: 0.730853\tval_acc: 0.500000 val_loss: 0.6972544 test_acc:0.500000\n",
      "1_1 train_acc: 0.4395 train_loss: 0.757973\tval_acc: 0.512500 val_loss: 0.6947318 test_acc:0.493750\n",
      "1_4 train_acc: 0.5645 train_loss: 0.695765\tval_acc: 0.587500 val_loss: 0.6906430 test_acc:0.500000\n",
      "1_5 train_acc: 0.5565 train_loss: 0.692353\tval_acc: 0.600000 val_loss: 0.6877483 test_acc:0.500000\n",
      "1_7 train_acc: 0.5806 train_loss: 0.692834\tval_acc: 0.600000 val_loss: 0.6811911 test_acc:0.534375\n",
      "1_8 train_acc: 0.5605 train_loss: 0.689869\tval_acc: 0.625000 val_loss: 0.6751481 test_acc:0.525000\n",
      "1_9 train_acc: 0.5927 train_loss: 0.681060\tval_acc: 0.662500 val_loss: 0.6503679 test_acc:0.531250\n",
      "1_12 train_acc: 0.6573 train_loss: 0.607328\tval_acc: 0.725000 val_loss: 0.5697752 test_acc:0.653125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_14 train_acc: 0.7702 train_loss: 0.530515\tval_acc: 0.737500 val_loss: 0.5836277 test_acc:0.696875\n",
      "1_16 train_acc: 0.7823 train_loss: 0.515269\tval_acc: 0.775000 val_loss: 0.5249513 test_acc:0.712500\n",
      "1_17 train_acc: 0.7218 train_loss: 0.585335\tval_acc: 0.775000 val_loss: 0.4673623 test_acc:0.646875\n",
      "1_19 train_acc: 0.7379 train_loss: 0.545433\tval_acc: 0.812500 val_loss: 0.4738925 test_acc:0.715625\n",
      "1_22 train_acc: 0.7702 train_loss: 0.488601\tval_acc: 0.850000 val_loss: 0.4012694 test_acc:0.693750\n",
      "1_31 train_acc: 0.7823 train_loss: 0.455490\tval_acc: 0.850000 val_loss: 0.3462326 test_acc:0.728125\n",
      "1_33 train_acc: 0.8266 train_loss: 0.391750\tval_acc: 0.850000 val_loss: 0.3222168 test_acc:0.715625\n",
      "1_36 train_acc: 0.8790 train_loss: 0.342267\tval_acc: 0.875000 val_loss: 0.3378281 test_acc:0.706250\n",
      "1_37 train_acc: 0.8185 train_loss: 0.401773\tval_acc: 0.887500 val_loss: 0.3020096 test_acc:0.718750\n",
      "1_44 train_acc: 0.8871 train_loss: 0.333597\tval_acc: 0.900000 val_loss: 0.3177181 test_acc:0.712500\n",
      "1_62 train_acc: 0.8911 train_loss: 0.267097\tval_acc: 0.900000 val_loss: 0.2648620 test_acc:0.762500\n",
      "1_93 train_acc: 0.8992 train_loss: 0.245877\tval_acc: 0.900000 val_loss: 0.2141814 test_acc:0.756250\n",
      "1_98 train_acc: 0.9032 train_loss: 0.242078\tval_acc: 0.925000 val_loss: 0.2241108 test_acc:0.771875\n",
      "1_115 train_acc: 0.9234 train_loss: 0.215182\tval_acc: 0.925000 val_loss: 0.2065973 test_acc:0.775000\n",
      "1_117 train_acc: 0.8952 train_loss: 0.269395\tval_acc: 0.937500 val_loss: 0.2195743 test_acc:0.781250\n",
      "1_123 train_acc: 0.8952 train_loss: 0.249621\tval_acc: 0.937500 val_loss: 0.1875114 test_acc:0.775000\n",
      "1_144 train_acc: 0.9395 train_loss: 0.198590\tval_acc: 0.950000 val_loss: 0.1831285 test_acc:0.781250\n",
      "1_166 train_acc: 0.9153 train_loss: 0.232134\tval_acc: 0.950000 val_loss: 0.1802711 test_acc:0.790625\n",
      "1_216 train_acc: 0.8548 train_loss: 0.301418\tval_acc: 0.950000 val_loss: 0.1359380 test_acc:0.790625\n",
      "1_241 train_acc: 0.9113 train_loss: 0.198328\tval_acc: 0.962500 val_loss: 0.1299095 test_acc:0.815625\n",
      "1_259 train_acc: 0.9516 train_loss: 0.154758\tval_acc: 0.975000 val_loss: 0.1229180 test_acc:0.800000\n",
      "1_339 train_acc: 0.8871 train_loss: 0.232571\tval_acc: 0.975000 val_loss: 0.0978498 test_acc:0.790625\n",
      "1_342 train_acc: 0.9194 train_loss: 0.204595\tval_acc: 0.975000 val_loss: 0.0944086 test_acc:0.806250\n",
      "1_398 train_acc: 0.9516 train_loss: 0.141986\tval_acc: 0.975000 val_loss: 0.0904304 test_acc:0.806250\n",
      "1_464 train_acc: 0.9153 train_loss: 0.199002\tval_acc: 0.975000 val_loss: 0.0884915 test_acc:0.818750\n",
      "1_470 train_acc: 0.9637 train_loss: 0.135112\tval_acc: 0.987500 val_loss: 0.0821013 test_acc:0.809375\n",
      "1_542 train_acc: 0.9153 train_loss: 0.223964\tval_acc: 0.987500 val_loss: 0.0713700 test_acc:0.800000\n",
      "1_543 train_acc: 0.9637 train_loss: 0.102355\tval_acc: 0.987500 val_loss: 0.0693294 test_acc:0.793750\n",
      "1_605 train_acc: 0.9395 train_loss: 0.155207\tval_acc: 0.987500 val_loss: 0.0666305 test_acc:0.806250\n",
      "1_634 train_acc: 0.9315 train_loss: 0.163483\tval_acc: 0.987500 val_loss: 0.0608972 test_acc:0.796875\n",
      "1_651 train_acc: 0.9556 train_loss: 0.136588\tval_acc: 0.987500 val_loss: 0.0528661 test_acc:0.787500\n",
      "1_839 train_acc: 0.9597 train_loss: 0.120434\tval_acc: 1.000000 val_loss: 0.0513902 test_acc:0.781250\n",
      "1_949 train_acc: 0.9476 train_loss: 0.136423\tval_acc: 1.000000 val_loss: 0.0504237 test_acc:0.790625\n",
      "1_950 train_acc: 0.9234 train_loss: 0.175623\tval_acc: 1.000000 val_loss: 0.0461560 test_acc:0.790625\n",
      "epoch:  950 \tThe test accuracy is: 0.790625\n",
      " THE BEST ACCURACY IS 0.790625\tkappa is 0.58125\n",
      "subject 1 duration: 0:17:15.650700\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 1 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "1_0 train_acc: 0.4677 train_loss: 0.755491\tval_acc: 0.512500 val_loss: 0.6921095 test_acc:0.506250\n",
      "1_1 train_acc: 0.4919 train_loss: 0.768083\tval_acc: 0.512500 val_loss: 0.6880748 test_acc:0.521875\n",
      "1_2 train_acc: 0.5363 train_loss: 0.707192\tval_acc: 0.550000 val_loss: 0.6838537 test_acc:0.528125\n",
      "1_4 train_acc: 0.4879 train_loss: 0.719309\tval_acc: 0.600000 val_loss: 0.6750299 test_acc:0.525000\n",
      "1_5 train_acc: 0.5726 train_loss: 0.694545\tval_acc: 0.600000 val_loss: 0.6672903 test_acc:0.546875\n",
      "1_6 train_acc: 0.5524 train_loss: 0.695522\tval_acc: 0.625000 val_loss: 0.6632621 test_acc:0.575000\n",
      "1_7 train_acc: 0.4960 train_loss: 0.703826\tval_acc: 0.687500 val_loss: 0.6585460 test_acc:0.565625\n",
      "1_8 train_acc: 0.5484 train_loss: 0.687945\tval_acc: 0.687500 val_loss: 0.6484848 test_acc:0.568750\n",
      "1_9 train_acc: 0.5605 train_loss: 0.680502\tval_acc: 0.687500 val_loss: 0.6222821 test_acc:0.575000\n",
      "1_11 train_acc: 0.6734 train_loss: 0.609880\tval_acc: 0.800000 val_loss: 0.4934701 test_acc:0.571875\n",
      "1_17 train_acc: 0.7661 train_loss: 0.505865\tval_acc: 0.812500 val_loss: 0.4364658 test_acc:0.631250\n",
      "1_23 train_acc: 0.8145 train_loss: 0.462036\tval_acc: 0.812500 val_loss: 0.4094559 test_acc:0.712500\n",
      "1_24 train_acc: 0.7863 train_loss: 0.471861\tval_acc: 0.812500 val_loss: 0.4070966 test_acc:0.734375\n",
      "1_25 train_acc: 0.7621 train_loss: 0.485319\tval_acc: 0.875000 val_loss: 0.3715004 test_acc:0.693750\n",
      "1_31 train_acc: 0.8105 train_loss: 0.411843\tval_acc: 0.887500 val_loss: 0.3305150 test_acc:0.690625\n",
      "1_35 train_acc: 0.8750 train_loss: 0.351456\tval_acc: 0.887500 val_loss: 0.3134975 test_acc:0.687500\n",
      "1_41 train_acc: 0.8145 train_loss: 0.412011\tval_acc: 0.887500 val_loss: 0.3134815 test_acc:0.709375\n",
      "1_44 train_acc: 0.8266 train_loss: 0.390519\tval_acc: 0.887500 val_loss: 0.3065270 test_acc:0.706250\n",
      "1_51 train_acc: 0.8750 train_loss: 0.324796\tval_acc: 0.887500 val_loss: 0.3062704 test_acc:0.759375\n",
      "1_54 train_acc: 0.9032 train_loss: 0.275835\tval_acc: 0.900000 val_loss: 0.3135479 test_acc:0.731250\n",
      "1_56 train_acc: 0.8629 train_loss: 0.305889\tval_acc: 0.900000 val_loss: 0.2919958 test_acc:0.737500\n",
      "1_62 train_acc: 0.8871 train_loss: 0.299912\tval_acc: 0.900000 val_loss: 0.2789484 test_acc:0.734375\n",
      "1_64 train_acc: 0.8710 train_loss: 0.307204\tval_acc: 0.900000 val_loss: 0.2785825 test_acc:0.746875\n",
      "1_68 train_acc: 0.8952 train_loss: 0.304761\tval_acc: 0.912500 val_loss: 0.2713224 test_acc:0.775000\n",
      "1_72 train_acc: 0.8710 train_loss: 0.313188\tval_acc: 0.925000 val_loss: 0.2839899 test_acc:0.778125\n",
      "1_108 train_acc: 0.8790 train_loss: 0.307674\tval_acc: 0.925000 val_loss: 0.2750404 test_acc:0.784375\n",
      "1_115 train_acc: 0.8710 train_loss: 0.277789\tval_acc: 0.925000 val_loss: 0.2710835 test_acc:0.784375\n",
      "1_132 train_acc: 0.9113 train_loss: 0.196766\tval_acc: 0.925000 val_loss: 0.2639855 test_acc:0.762500\n",
      "1_134 train_acc: 0.9113 train_loss: 0.226888\tval_acc: 0.937500 val_loss: 0.2331619 test_acc:0.743750\n",
      "1_178 train_acc: 0.9234 train_loss: 0.223422\tval_acc: 0.937500 val_loss: 0.2120763 test_acc:0.793750\n",
      "1_222 train_acc: 0.9234 train_loss: 0.172700\tval_acc: 0.937500 val_loss: 0.2118971 test_acc:0.800000\n",
      "1_229 train_acc: 0.9516 train_loss: 0.145107\tval_acc: 0.937500 val_loss: 0.1544522 test_acc:0.743750\n",
      "1_258 train_acc: 0.9032 train_loss: 0.254172\tval_acc: 0.950000 val_loss: 0.1749260 test_acc:0.712500\n",
      "1_364 train_acc: 0.9677 train_loss: 0.099735\tval_acc: 0.950000 val_loss: 0.1305927 test_acc:0.737500\n",
      "1_392 train_acc: 0.9234 train_loss: 0.178950\tval_acc: 0.950000 val_loss: 0.1167257 test_acc:0.771875\n",
      "1_394 train_acc: 0.9395 train_loss: 0.164235\tval_acc: 0.950000 val_loss: 0.1166400 test_acc:0.718750\n",
      "1_395 train_acc: 0.9476 train_loss: 0.133667\tval_acc: 0.962500 val_loss: 0.1162287 test_acc:0.734375\n",
      "1_417 train_acc: 0.9274 train_loss: 0.162856\tval_acc: 0.962500 val_loss: 0.1117406 test_acc:0.750000\n",
      "1_443 train_acc: 0.9435 train_loss: 0.126076\tval_acc: 0.962500 val_loss: 0.0943181 test_acc:0.756250\n",
      "1_498 train_acc: 0.9718 train_loss: 0.089021\tval_acc: 0.975000 val_loss: 0.0957088 test_acc:0.740625\n",
      "1_563 train_acc: 0.9516 train_loss: 0.105514\tval_acc: 0.975000 val_loss: 0.0809398 test_acc:0.743750\n",
      "1_606 train_acc: 0.9194 train_loss: 0.209090\tval_acc: 0.975000 val_loss: 0.0720448 test_acc:0.768750\n",
      "1_687 train_acc: 0.9435 train_loss: 0.129806\tval_acc: 0.975000 val_loss: 0.0692179 test_acc:0.750000\n",
      "1_741 train_acc: 0.9637 train_loss: 0.132705\tval_acc: 0.987500 val_loss: 0.0808486 test_acc:0.765625\n",
      "1_880 train_acc: 0.9637 train_loss: 0.124698\tval_acc: 0.987500 val_loss: 0.0801174 test_acc:0.750000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_907 train_acc: 0.9516 train_loss: 0.145320\tval_acc: 0.987500 val_loss: 0.0580287 test_acc:0.731250\n",
      "1_952 train_acc: 0.9315 train_loss: 0.172159\tval_acc: 1.000000 val_loss: 0.0562394 test_acc:0.753125\n",
      "epoch:  952 \tThe test accuracy is: 0.753125\n",
      " THE BEST ACCURACY IS 0.753125\tkappa is 0.50625\n",
      "subject 1 duration: 0:34:33.466817\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 1 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "1_0 train_acc: 0.5444 train_loss: 0.718674\tval_acc: 0.500000 val_loss: 0.6950446 test_acc:0.500000\n",
      "1_1 train_acc: 0.5282 train_loss: 0.753140\tval_acc: 0.612500 val_loss: 0.6794239 test_acc:0.528125\n",
      "1_2 train_acc: 0.5444 train_loss: 0.704839\tval_acc: 0.625000 val_loss: 0.6714302 test_acc:0.540625\n",
      "1_4 train_acc: 0.5645 train_loss: 0.685477\tval_acc: 0.662500 val_loss: 0.6551999 test_acc:0.534375\n",
      "1_5 train_acc: 0.5242 train_loss: 0.696659\tval_acc: 0.712500 val_loss: 0.6399666 test_acc:0.543750\n",
      "1_8 train_acc: 0.6089 train_loss: 0.665485\tval_acc: 0.750000 val_loss: 0.5220832 test_acc:0.550000\n",
      "1_9 train_acc: 0.6734 train_loss: 0.620726\tval_acc: 0.787500 val_loss: 0.4607175 test_acc:0.584375\n",
      "1_11 train_acc: 0.7500 train_loss: 0.536589\tval_acc: 0.800000 val_loss: 0.4198748 test_acc:0.581250\n",
      "1_13 train_acc: 0.7177 train_loss: 0.564878\tval_acc: 0.837500 val_loss: 0.3848466 test_acc:0.596875\n",
      "1_17 train_acc: 0.8105 train_loss: 0.435674\tval_acc: 0.850000 val_loss: 0.3709920 test_acc:0.581250\n",
      "1_18 train_acc: 0.6653 train_loss: 0.587763\tval_acc: 0.875000 val_loss: 0.3586393 test_acc:0.593750\n",
      "1_19 train_acc: 0.8145 train_loss: 0.464136\tval_acc: 0.887500 val_loss: 0.3422652 test_acc:0.578125\n",
      "1_23 train_acc: 0.8024 train_loss: 0.477937\tval_acc: 0.900000 val_loss: 0.2944998 test_acc:0.634375\n",
      "1_28 train_acc: 0.8347 train_loss: 0.417159\tval_acc: 0.900000 val_loss: 0.2808598 test_acc:0.596875\n",
      "1_29 train_acc: 0.7944 train_loss: 0.441952\tval_acc: 0.900000 val_loss: 0.2629680 test_acc:0.653125\n",
      "1_30 train_acc: 0.8468 train_loss: 0.374906\tval_acc: 0.900000 val_loss: 0.2608857 test_acc:0.637500\n",
      "1_38 train_acc: 0.8347 train_loss: 0.391714\tval_acc: 0.900000 val_loss: 0.2489938 test_acc:0.684375\n",
      "1_39 train_acc: 0.8669 train_loss: 0.342996\tval_acc: 0.900000 val_loss: 0.2477334 test_acc:0.753125\n",
      "1_41 train_acc: 0.8589 train_loss: 0.376206\tval_acc: 0.912500 val_loss: 0.2639511 test_acc:0.746875\n",
      "1_43 train_acc: 0.8508 train_loss: 0.406787\tval_acc: 0.912500 val_loss: 0.2028312 test_acc:0.750000\n",
      "1_45 train_acc: 0.8548 train_loss: 0.334530\tval_acc: 0.912500 val_loss: 0.1948762 test_acc:0.721875\n",
      "1_57 train_acc: 0.8992 train_loss: 0.256449\tval_acc: 0.925000 val_loss: 0.2043397 test_acc:0.753125\n",
      "1_69 train_acc: 0.8831 train_loss: 0.287590\tval_acc: 0.925000 val_loss: 0.1721111 test_acc:0.700000\n",
      "1_81 train_acc: 0.8589 train_loss: 0.320172\tval_acc: 0.925000 val_loss: 0.1637833 test_acc:0.718750\n",
      "1_96 train_acc: 0.8831 train_loss: 0.277072\tval_acc: 0.937500 val_loss: 0.1604528 test_acc:0.700000\n",
      "1_113 train_acc: 0.8669 train_loss: 0.297298\tval_acc: 0.937500 val_loss: 0.1398370 test_acc:0.768750\n",
      "1_120 train_acc: 0.9073 train_loss: 0.248466\tval_acc: 0.937500 val_loss: 0.1329892 test_acc:0.709375\n",
      "1_123 train_acc: 0.9113 train_loss: 0.237506\tval_acc: 0.937500 val_loss: 0.1236474 test_acc:0.756250\n",
      "1_156 train_acc: 0.8992 train_loss: 0.232881\tval_acc: 0.950000 val_loss: 0.1513542 test_acc:0.771875\n",
      "1_157 train_acc: 0.8992 train_loss: 0.252233\tval_acc: 0.962500 val_loss: 0.1513019 test_acc:0.762500\n",
      "1_198 train_acc: 0.9032 train_loss: 0.242332\tval_acc: 0.962500 val_loss: 0.1214236 test_acc:0.715625\n",
      "1_421 train_acc: 0.8992 train_loss: 0.263161\tval_acc: 0.962500 val_loss: 0.1178513 test_acc:0.775000\n",
      "1_513 train_acc: 0.8589 train_loss: 0.319562\tval_acc: 0.975000 val_loss: 0.1139633 test_acc:0.750000\n",
      "1_984 train_acc: 0.9274 train_loss: 0.176618\tval_acc: 0.975000 val_loss: 0.0890117 test_acc:0.768750\n",
      "epoch:  984 \tThe test accuracy is: 0.76875\n",
      " THE BEST ACCURACY IS 0.76875\tkappa is 0.5375\n",
      "subject 1 duration: 0:51:46.398885\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 1 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "1_0 train_acc: 0.4960 train_loss: 0.740596\tval_acc: 0.500000 val_loss: 0.7133771 test_acc:0.500000\n",
      "1_1 train_acc: 0.4395 train_loss: 0.759855\tval_acc: 0.500000 val_loss: 0.6964730 test_acc:0.500000\n",
      "1_2 train_acc: 0.5484 train_loss: 0.710456\tval_acc: 0.550000 val_loss: 0.6939483 test_acc:0.509375\n",
      "1_5 train_acc: 0.5645 train_loss: 0.696205\tval_acc: 0.550000 val_loss: 0.6840242 test_acc:0.512500\n",
      "1_6 train_acc: 0.5323 train_loss: 0.688506\tval_acc: 0.575000 val_loss: 0.6797059 test_acc:0.525000\n",
      "1_7 train_acc: 0.5161 train_loss: 0.700396\tval_acc: 0.600000 val_loss: 0.6784856 test_acc:0.546875\n",
      "1_8 train_acc: 0.5726 train_loss: 0.690093\tval_acc: 0.662500 val_loss: 0.6674491 test_acc:0.534375\n",
      "1_9 train_acc: 0.5766 train_loss: 0.673265\tval_acc: 0.675000 val_loss: 0.6589752 test_acc:0.550000\n",
      "1_10 train_acc: 0.6371 train_loss: 0.649338\tval_acc: 0.675000 val_loss: 0.6375537 test_acc:0.562500\n",
      "1_11 train_acc: 0.6290 train_loss: 0.662001\tval_acc: 0.712500 val_loss: 0.5904625 test_acc:0.590625\n",
      "1_12 train_acc: 0.5847 train_loss: 0.658418\tval_acc: 0.800000 val_loss: 0.5582328 test_acc:0.593750\n",
      "1_17 train_acc: 0.7056 train_loss: 0.601434\tval_acc: 0.825000 val_loss: 0.4228760 test_acc:0.600000\n",
      "1_29 train_acc: 0.8427 train_loss: 0.375869\tval_acc: 0.837500 val_loss: 0.4512191 test_acc:0.778125\n",
      "1_30 train_acc: 0.7984 train_loss: 0.431261\tval_acc: 0.850000 val_loss: 0.3633408 test_acc:0.740625\n",
      "1_32 train_acc: 0.8145 train_loss: 0.397862\tval_acc: 0.887500 val_loss: 0.3245293 test_acc:0.703125\n",
      "1_48 train_acc: 0.8065 train_loss: 0.393097\tval_acc: 0.887500 val_loss: 0.2803740 test_acc:0.743750\n",
      "1_53 train_acc: 0.8266 train_loss: 0.386891\tval_acc: 0.912500 val_loss: 0.2456903 test_acc:0.721875\n",
      "1_65 train_acc: 0.9194 train_loss: 0.234735\tval_acc: 0.912500 val_loss: 0.2190534 test_acc:0.734375\n",
      "1_74 train_acc: 0.8750 train_loss: 0.274526\tval_acc: 0.912500 val_loss: 0.2173826 test_acc:0.693750\n",
      "1_77 train_acc: 0.8790 train_loss: 0.287428\tval_acc: 0.925000 val_loss: 0.1894559 test_acc:0.728125\n",
      "1_99 train_acc: 0.8992 train_loss: 0.279881\tval_acc: 0.950000 val_loss: 0.1885501 test_acc:0.703125\n",
      "1_102 train_acc: 0.8871 train_loss: 0.306004\tval_acc: 0.950000 val_loss: 0.1741767 test_acc:0.725000\n",
      "1_122 train_acc: 0.9355 train_loss: 0.248029\tval_acc: 0.950000 val_loss: 0.1693929 test_acc:0.721875\n",
      "1_140 train_acc: 0.8992 train_loss: 0.296412\tval_acc: 0.962500 val_loss: 0.1798326 test_acc:0.709375\n",
      "1_261 train_acc: 0.8952 train_loss: 0.242342\tval_acc: 0.962500 val_loss: 0.1638139 test_acc:0.715625\n",
      "1_279 train_acc: 0.9274 train_loss: 0.168719\tval_acc: 0.962500 val_loss: 0.1505243 test_acc:0.753125\n",
      "1_326 train_acc: 0.9355 train_loss: 0.158380\tval_acc: 0.962500 val_loss: 0.1389980 test_acc:0.756250\n",
      "1_353 train_acc: 0.9476 train_loss: 0.137704\tval_acc: 0.962500 val_loss: 0.1290800 test_acc:0.737500\n",
      "1_425 train_acc: 0.9234 train_loss: 0.186342\tval_acc: 0.962500 val_loss: 0.1232409 test_acc:0.759375\n",
      "1_448 train_acc: 0.9315 train_loss: 0.197827\tval_acc: 0.975000 val_loss: 0.1301002 test_acc:0.753125\n",
      "1_467 train_acc: 0.9315 train_loss: 0.161013\tval_acc: 0.975000 val_loss: 0.1264865 test_acc:0.718750\n",
      "1_539 train_acc: 0.9355 train_loss: 0.176951\tval_acc: 0.975000 val_loss: 0.1146898 test_acc:0.740625\n",
      "1_678 train_acc: 0.9274 train_loss: 0.178394\tval_acc: 0.975000 val_loss: 0.1129517 test_acc:0.765625\n",
      "1_710 train_acc: 0.9516 train_loss: 0.136513\tval_acc: 0.975000 val_loss: 0.1120378 test_acc:0.768750\n",
      "1_776 train_acc: 0.9234 train_loss: 0.217960\tval_acc: 0.975000 val_loss: 0.1007295 test_acc:0.759375\n",
      "1_811 train_acc: 0.9516 train_loss: 0.146034\tval_acc: 0.975000 val_loss: 0.0991860 test_acc:0.790625\n",
      "1_943 train_acc: 0.9395 train_loss: 0.166005\tval_acc: 0.987500 val_loss: 0.1029918 test_acc:0.793750\n",
      "epoch:  943 \tThe test accuracy is: 0.79375\n",
      " THE BEST ACCURACY IS 0.79375\tkappa is 0.5875\n",
      "subject 1 duration: 1:09:18.497328\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 1 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_0 train_acc: 0.4274 train_loss: 0.802234\tval_acc: 0.500000 val_loss: 0.7250905 test_acc:0.500000\n",
      "1_1 train_acc: 0.4395 train_loss: 0.784945\tval_acc: 0.500000 val_loss: 0.6893985 test_acc:0.503125\n",
      "1_3 train_acc: 0.4839 train_loss: 0.722601\tval_acc: 0.500000 val_loss: 0.6890133 test_acc:0.500000\n",
      "1_4 train_acc: 0.5282 train_loss: 0.703718\tval_acc: 0.537500 val_loss: 0.6872090 test_acc:0.534375\n",
      "1_5 train_acc: 0.5363 train_loss: 0.702082\tval_acc: 0.575000 val_loss: 0.6861282 test_acc:0.550000\n",
      "1_10 train_acc: 0.5685 train_loss: 0.661645\tval_acc: 0.625000 val_loss: 0.6783231 test_acc:0.556250\n",
      "1_14 train_acc: 0.5887 train_loss: 0.656324\tval_acc: 0.700000 val_loss: 0.6231054 test_acc:0.565625\n",
      "1_15 train_acc: 0.6371 train_loss: 0.621148\tval_acc: 0.712500 val_loss: 0.5879614 test_acc:0.606250\n",
      "1_19 train_acc: 0.6815 train_loss: 0.597222\tval_acc: 0.712500 val_loss: 0.5635213 test_acc:0.587500\n",
      "1_20 train_acc: 0.7177 train_loss: 0.579765\tval_acc: 0.737500 val_loss: 0.6236994 test_acc:0.581250\n",
      "1_21 train_acc: 0.7218 train_loss: 0.543767\tval_acc: 0.750000 val_loss: 0.5720609 test_acc:0.562500\n",
      "1_22 train_acc: 0.6613 train_loss: 0.685797\tval_acc: 0.762500 val_loss: 0.5166767 test_acc:0.690625\n",
      "1_24 train_acc: 0.7621 train_loss: 0.505210\tval_acc: 0.775000 val_loss: 0.5023532 test_acc:0.643750\n",
      "1_26 train_acc: 0.7782 train_loss: 0.454302\tval_acc: 0.775000 val_loss: 0.4827169 test_acc:0.615625\n",
      "1_27 train_acc: 0.7742 train_loss: 0.487434\tval_acc: 0.787500 val_loss: 0.4777175 test_acc:0.631250\n",
      "1_32 train_acc: 0.8266 train_loss: 0.398533\tval_acc: 0.812500 val_loss: 0.4614288 test_acc:0.718750\n",
      "1_35 train_acc: 0.8427 train_loss: 0.412314\tval_acc: 0.825000 val_loss: 0.4669021 test_acc:0.634375\n",
      "1_42 train_acc: 0.7863 train_loss: 0.432578\tval_acc: 0.825000 val_loss: 0.4261897 test_acc:0.681250\n",
      "1_49 train_acc: 0.8306 train_loss: 0.377608\tval_acc: 0.825000 val_loss: 0.4193173 test_acc:0.656250\n",
      "1_50 train_acc: 0.8427 train_loss: 0.361435\tval_acc: 0.850000 val_loss: 0.4236070 test_acc:0.681250\n",
      "1_59 train_acc: 0.8710 train_loss: 0.305832\tval_acc: 0.850000 val_loss: 0.4217145 test_acc:0.706250\n",
      "1_66 train_acc: 0.8992 train_loss: 0.253527\tval_acc: 0.850000 val_loss: 0.3930399 test_acc:0.753125\n",
      "1_76 train_acc: 0.8831 train_loss: 0.303513\tval_acc: 0.850000 val_loss: 0.3713410 test_acc:0.734375\n",
      "1_78 train_acc: 0.8831 train_loss: 0.292811\tval_acc: 0.862500 val_loss: 0.3536220 test_acc:0.737500\n",
      "1_79 train_acc: 0.9194 train_loss: 0.231290\tval_acc: 0.875000 val_loss: 0.3376455 test_acc:0.750000\n",
      "1_113 train_acc: 0.8911 train_loss: 0.246437\tval_acc: 0.875000 val_loss: 0.3159129 test_acc:0.778125\n",
      "1_131 train_acc: 0.8790 train_loss: 0.307211\tval_acc: 0.875000 val_loss: 0.3028192 test_acc:0.737500\n",
      "1_150 train_acc: 0.9032 train_loss: 0.218931\tval_acc: 0.875000 val_loss: 0.2922888 test_acc:0.806250\n",
      "1_153 train_acc: 0.9315 train_loss: 0.199672\tval_acc: 0.912500 val_loss: 0.2887114 test_acc:0.809375\n",
      "1_205 train_acc: 0.9234 train_loss: 0.218834\tval_acc: 0.912500 val_loss: 0.2629960 test_acc:0.784375\n",
      "1_266 train_acc: 0.9274 train_loss: 0.170376\tval_acc: 0.912500 val_loss: 0.2526150 test_acc:0.803125\n",
      "1_279 train_acc: 0.9153 train_loss: 0.242210\tval_acc: 0.912500 val_loss: 0.2356695 test_acc:0.771875\n",
      "1_280 train_acc: 0.9556 train_loss: 0.119436\tval_acc: 0.912500 val_loss: 0.2339553 test_acc:0.759375\n",
      "1_288 train_acc: 0.9234 train_loss: 0.214602\tval_acc: 0.925000 val_loss: 0.2201507 test_acc:0.800000\n",
      "1_332 train_acc: 0.9153 train_loss: 0.184547\tval_acc: 0.925000 val_loss: 0.2058568 test_acc:0.775000\n",
      "1_335 train_acc: 0.9234 train_loss: 0.196278\tval_acc: 0.925000 val_loss: 0.1934523 test_acc:0.806250\n",
      "1_350 train_acc: 0.8710 train_loss: 0.283110\tval_acc: 0.937500 val_loss: 0.1985064 test_acc:0.803125\n",
      "1_497 train_acc: 0.9395 train_loss: 0.131239\tval_acc: 0.937500 val_loss: 0.1521862 test_acc:0.781250\n",
      "1_531 train_acc: 0.9274 train_loss: 0.146479\tval_acc: 0.937500 val_loss: 0.1393807 test_acc:0.768750\n",
      "1_570 train_acc: 0.9355 train_loss: 0.161686\tval_acc: 0.962500 val_loss: 0.1454837 test_acc:0.796875\n",
      "epoch:  570 \tThe test accuracy is: 0.796875\n",
      " THE BEST ACCURACY IS 0.796875\tkappa is 0.59375\n",
      "subject 1 duration: 1:26:55.055851\n",
      "1 subject 5 fold mean: \n",
      " accuray      78.062500\n",
      "precision    82.740546\n",
      "recall       78.062500\n",
      "f1           77.229602\n",
      "kappa        56.125000\n",
      "dtype: float64\n",
      "seed is 1319\n",
      "Subject 2\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (280, 3, 1000) subject: 2 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "2_0 train_acc: 0.5081 train_loss: 0.754208\tval_acc: 0.500000 val_loss: 0.6925743 test_acc:0.496429\n",
      "2_3 train_acc: 0.5645 train_loss: 0.692744\tval_acc: 0.537500 val_loss: 0.6913422 test_acc:0.482143\n",
      "2_5 train_acc: 0.5202 train_loss: 0.708800\tval_acc: 0.550000 val_loss: 0.6897897 test_acc:0.460714\n",
      "2_7 train_acc: 0.5363 train_loss: 0.695791\tval_acc: 0.550000 val_loss: 0.6860422 test_acc:0.482143\n",
      "2_8 train_acc: 0.5685 train_loss: 0.670504\tval_acc: 0.575000 val_loss: 0.6775215 test_acc:0.503571\n",
      "2_9 train_acc: 0.5040 train_loss: 0.724893\tval_acc: 0.612500 val_loss: 0.6774881 test_acc:0.539286\n",
      "2_12 train_acc: 0.6008 train_loss: 0.658664\tval_acc: 0.650000 val_loss: 0.6803106 test_acc:0.560714\n",
      "2_13 train_acc: 0.5968 train_loss: 0.658542\tval_acc: 0.650000 val_loss: 0.6785734 test_acc:0.550000\n",
      "2_15 train_acc: 0.6048 train_loss: 0.655048\tval_acc: 0.650000 val_loss: 0.6716258 test_acc:0.571429\n",
      "2_16 train_acc: 0.6290 train_loss: 0.649202\tval_acc: 0.650000 val_loss: 0.6664286 test_acc:0.575000\n",
      "2_17 train_acc: 0.6290 train_loss: 0.646544\tval_acc: 0.700000 val_loss: 0.6623132 test_acc:0.596429\n",
      "2_28 train_acc: 0.7218 train_loss: 0.566558\tval_acc: 0.737500 val_loss: 0.6071052 test_acc:0.660714\n",
      "2_31 train_acc: 0.7298 train_loss: 0.547899\tval_acc: 0.737500 val_loss: 0.6049793 test_acc:0.657143\n",
      "2_42 train_acc: 0.7823 train_loss: 0.440448\tval_acc: 0.750000 val_loss: 0.5665067 test_acc:0.678571\n",
      "2_48 train_acc: 0.7944 train_loss: 0.461486\tval_acc: 0.775000 val_loss: 0.6176454 test_acc:0.685714\n",
      "2_55 train_acc: 0.7782 train_loss: 0.513135\tval_acc: 0.787500 val_loss: 0.6077231 test_acc:0.710714\n",
      "2_92 train_acc: 0.8226 train_loss: 0.438866\tval_acc: 0.800000 val_loss: 0.5735761 test_acc:0.703571\n",
      "2_133 train_acc: 0.7984 train_loss: 0.488486\tval_acc: 0.800000 val_loss: 0.5284295 test_acc:0.710714\n",
      "2_146 train_acc: 0.7863 train_loss: 0.438666\tval_acc: 0.800000 val_loss: 0.5167506 test_acc:0.703571\n",
      "2_164 train_acc: 0.8347 train_loss: 0.367970\tval_acc: 0.800000 val_loss: 0.5091747 test_acc:0.678571\n",
      "2_172 train_acc: 0.7823 train_loss: 0.428186\tval_acc: 0.800000 val_loss: 0.4772993 test_acc:0.678571\n",
      "2_174 train_acc: 0.8266 train_loss: 0.380962\tval_acc: 0.800000 val_loss: 0.4583360 test_acc:0.700000\n",
      "2_206 train_acc: 0.8387 train_loss: 0.363374\tval_acc: 0.812500 val_loss: 0.4510826 test_acc:0.707143\n",
      "2_229 train_acc: 0.8387 train_loss: 0.358139\tval_acc: 0.812500 val_loss: 0.4358936 test_acc:0.689286\n",
      "2_244 train_acc: 0.8831 train_loss: 0.241581\tval_acc: 0.825000 val_loss: 0.5647661 test_acc:0.710714\n",
      "2_261 train_acc: 0.8387 train_loss: 0.369519\tval_acc: 0.825000 val_loss: 0.4849428 test_acc:0.696429\n",
      "2_284 train_acc: 0.9113 train_loss: 0.271924\tval_acc: 0.825000 val_loss: 0.4328659 test_acc:0.696429\n",
      "2_286 train_acc: 0.8508 train_loss: 0.347584\tval_acc: 0.825000 val_loss: 0.4151365 test_acc:0.714286\n",
      "2_297 train_acc: 0.8790 train_loss: 0.283282\tval_acc: 0.850000 val_loss: 0.4287659 test_acc:0.700000\n",
      "2_344 train_acc: 0.8508 train_loss: 0.347485\tval_acc: 0.850000 val_loss: 0.4144316 test_acc:0.732143\n",
      "2_391 train_acc: 0.8468 train_loss: 0.371894\tval_acc: 0.850000 val_loss: 0.3923393 test_acc:0.685714\n",
      "2_409 train_acc: 0.8790 train_loss: 0.263283\tval_acc: 0.875000 val_loss: 0.4083730 test_acc:0.721429\n",
      "2_546 train_acc: 0.8548 train_loss: 0.296623\tval_acc: 0.887500 val_loss: 0.4154971 test_acc:0.710714\n",
      "2_679 train_acc: 0.8871 train_loss: 0.250622\tval_acc: 0.887500 val_loss: 0.3640889 test_acc:0.671429\n",
      "2_755 train_acc: 0.8750 train_loss: 0.298236\tval_acc: 0.887500 val_loss: 0.3438764 test_acc:0.707143\n",
      "2_809 train_acc: 0.9073 train_loss: 0.232840\tval_acc: 0.900000 val_loss: 0.3394977 test_acc:0.703571\n",
      "epoch:  809 \tThe test accuracy is: 0.7035714285714286\n",
      " THE BEST ACCURACY IS 0.7035714285714286\tkappa is 0.40714285714285714\n",
      "subject 2 duration: 0:17:39.123271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (280, 3, 1000) subject: 2 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "2_0 train_acc: 0.4476 train_loss: 0.753676\tval_acc: 0.500000 val_loss: 0.6939727 test_acc:0.500000\n",
      "2_1 train_acc: 0.5363 train_loss: 0.713642\tval_acc: 0.500000 val_loss: 0.6900002 test_acc:0.500000\n",
      "2_2 train_acc: 0.5000 train_loss: 0.721695\tval_acc: 0.500000 val_loss: 0.6892195 test_acc:0.500000\n",
      "2_3 train_acc: 0.5121 train_loss: 0.710267\tval_acc: 0.575000 val_loss: 0.6868281 test_acc:0.467857\n",
      "2_5 train_acc: 0.5202 train_loss: 0.699205\tval_acc: 0.587500 val_loss: 0.6831863 test_acc:0.439286\n",
      "2_10 train_acc: 0.5040 train_loss: 0.707498\tval_acc: 0.587500 val_loss: 0.6759554 test_acc:0.485714\n",
      "2_11 train_acc: 0.5565 train_loss: 0.679644\tval_acc: 0.612500 val_loss: 0.6662125 test_acc:0.492857\n",
      "2_12 train_acc: 0.5685 train_loss: 0.675106\tval_acc: 0.625000 val_loss: 0.6609021 test_acc:0.496429\n",
      "2_13 train_acc: 0.5363 train_loss: 0.672350\tval_acc: 0.650000 val_loss: 0.6445286 test_acc:0.478571\n",
      "2_15 train_acc: 0.5968 train_loss: 0.685608\tval_acc: 0.675000 val_loss: 0.6195670 test_acc:0.492857\n",
      "2_16 train_acc: 0.6008 train_loss: 0.662578\tval_acc: 0.687500 val_loss: 0.6110572 test_acc:0.496429\n",
      "2_17 train_acc: 0.6492 train_loss: 0.640421\tval_acc: 0.712500 val_loss: 0.5818239 test_acc:0.542857\n",
      "2_20 train_acc: 0.6169 train_loss: 0.674294\tval_acc: 0.712500 val_loss: 0.5701694 test_acc:0.560714\n",
      "2_26 train_acc: 0.6694 train_loss: 0.610451\tval_acc: 0.750000 val_loss: 0.5495101 test_acc:0.628571\n",
      "2_31 train_acc: 0.6331 train_loss: 0.624248\tval_acc: 0.787500 val_loss: 0.5199257 test_acc:0.660714\n",
      "2_35 train_acc: 0.7097 train_loss: 0.564851\tval_acc: 0.787500 val_loss: 0.4844372 test_acc:0.632143\n",
      "2_37 train_acc: 0.7056 train_loss: 0.577524\tval_acc: 0.812500 val_loss: 0.4724860 test_acc:0.625000\n",
      "2_68 train_acc: 0.8226 train_loss: 0.438967\tval_acc: 0.825000 val_loss: 0.4491249 test_acc:0.703571\n",
      "2_76 train_acc: 0.7944 train_loss: 0.443547\tval_acc: 0.850000 val_loss: 0.4649122 test_acc:0.696429\n",
      "2_119 train_acc: 0.7984 train_loss: 0.431941\tval_acc: 0.850000 val_loss: 0.4283467 test_acc:0.728571\n",
      "2_136 train_acc: 0.8669 train_loss: 0.337298\tval_acc: 0.850000 val_loss: 0.4186290 test_acc:0.696429\n",
      "2_146 train_acc: 0.8427 train_loss: 0.359350\tval_acc: 0.850000 val_loss: 0.4136175 test_acc:0.707143\n",
      "2_148 train_acc: 0.8347 train_loss: 0.356250\tval_acc: 0.850000 val_loss: 0.3998814 test_acc:0.710714\n",
      "2_161 train_acc: 0.8508 train_loss: 0.394859\tval_acc: 0.862500 val_loss: 0.3606349 test_acc:0.717857\n",
      "2_163 train_acc: 0.8226 train_loss: 0.405447\tval_acc: 0.875000 val_loss: 0.3775867 test_acc:0.696429\n",
      "2_224 train_acc: 0.8266 train_loss: 0.398179\tval_acc: 0.887500 val_loss: 0.3358321 test_acc:0.714286\n",
      "2_276 train_acc: 0.8911 train_loss: 0.304043\tval_acc: 0.887500 val_loss: 0.3233642 test_acc:0.742857\n",
      "2_280 train_acc: 0.8790 train_loss: 0.307607\tval_acc: 0.887500 val_loss: 0.3172202 test_acc:0.700000\n",
      "2_282 train_acc: 0.8548 train_loss: 0.315748\tval_acc: 0.887500 val_loss: 0.2803097 test_acc:0.710714\n",
      "2_359 train_acc: 0.8952 train_loss: 0.291847\tval_acc: 0.900000 val_loss: 0.3149488 test_acc:0.728571\n",
      "2_362 train_acc: 0.8468 train_loss: 0.329818\tval_acc: 0.912500 val_loss: 0.3047582 test_acc:0.714286\n",
      "2_426 train_acc: 0.8266 train_loss: 0.423539\tval_acc: 0.912500 val_loss: 0.2446373 test_acc:0.757143\n",
      "2_475 train_acc: 0.8911 train_loss: 0.281784\tval_acc: 0.925000 val_loss: 0.2328338 test_acc:0.742857\n",
      "2_663 train_acc: 0.8992 train_loss: 0.233345\tval_acc: 0.937500 val_loss: 0.2202814 test_acc:0.710714\n",
      "2_713 train_acc: 0.9032 train_loss: 0.262347\tval_acc: 0.937500 val_loss: 0.1994878 test_acc:0.742857\n",
      "epoch:  713 \tThe test accuracy is: 0.7428571428571429\n",
      " THE BEST ACCURACY IS 0.7428571428571429\tkappa is 0.48571428571428577\n",
      "subject 2 duration: 0:35:07.070389\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (280, 3, 1000) subject: 2 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "2_0 train_acc: 0.4798 train_loss: 0.756132\tval_acc: 0.500000 val_loss: 0.7228864 test_acc:0.500000\n",
      "2_1 train_acc: 0.5121 train_loss: 0.744396\tval_acc: 0.562500 val_loss: 0.6827016 test_acc:0.471429\n",
      "2_2 train_acc: 0.5121 train_loss: 0.721384\tval_acc: 0.600000 val_loss: 0.6789030 test_acc:0.482143\n",
      "2_4 train_acc: 0.5847 train_loss: 0.688101\tval_acc: 0.625000 val_loss: 0.6779479 test_acc:0.489286\n",
      "2_8 train_acc: 0.5484 train_loss: 0.692536\tval_acc: 0.637500 val_loss: 0.6665764 test_acc:0.496429\n",
      "2_12 train_acc: 0.5444 train_loss: 0.673305\tval_acc: 0.662500 val_loss: 0.6545229 test_acc:0.503571\n",
      "2_16 train_acc: 0.5524 train_loss: 0.683710\tval_acc: 0.687500 val_loss: 0.6473151 test_acc:0.503571\n",
      "2_22 train_acc: 0.6169 train_loss: 0.657779\tval_acc: 0.687500 val_loss: 0.6153498 test_acc:0.535714\n",
      "2_25 train_acc: 0.5887 train_loss: 0.675201\tval_acc: 0.700000 val_loss: 0.5767850 test_acc:0.585714\n",
      "2_26 train_acc: 0.6734 train_loss: 0.629003\tval_acc: 0.725000 val_loss: 0.5813645 test_acc:0.603571\n",
      "2_29 train_acc: 0.6250 train_loss: 0.650225\tval_acc: 0.750000 val_loss: 0.5396992 test_acc:0.585714\n",
      "2_41 train_acc: 0.7137 train_loss: 0.569838\tval_acc: 0.750000 val_loss: 0.4693573 test_acc:0.678571\n",
      "2_42 train_acc: 0.7379 train_loss: 0.538686\tval_acc: 0.775000 val_loss: 0.4666776 test_acc:0.660714\n",
      "2_43 train_acc: 0.7540 train_loss: 0.550496\tval_acc: 0.787500 val_loss: 0.4631340 test_acc:0.667857\n",
      "2_49 train_acc: 0.7621 train_loss: 0.487951\tval_acc: 0.800000 val_loss: 0.4369012 test_acc:0.689286\n",
      "2_54 train_acc: 0.8065 train_loss: 0.444579\tval_acc: 0.800000 val_loss: 0.4271150 test_acc:0.692857\n",
      "2_60 train_acc: 0.7702 train_loss: 0.464324\tval_acc: 0.800000 val_loss: 0.4077874 test_acc:0.675000\n",
      "2_66 train_acc: 0.7903 train_loss: 0.473696\tval_acc: 0.825000 val_loss: 0.3948948 test_acc:0.667857\n",
      "2_90 train_acc: 0.8145 train_loss: 0.438162\tval_acc: 0.825000 val_loss: 0.3644187 test_acc:0.696429\n",
      "2_92 train_acc: 0.8065 train_loss: 0.446206\tval_acc: 0.837500 val_loss: 0.3714270 test_acc:0.696429\n",
      "2_97 train_acc: 0.7984 train_loss: 0.426735\tval_acc: 0.850000 val_loss: 0.3693224 test_acc:0.689286\n",
      "2_103 train_acc: 0.7742 train_loss: 0.489079\tval_acc: 0.850000 val_loss: 0.3526776 test_acc:0.671429\n",
      "2_116 train_acc: 0.8024 train_loss: 0.436979\tval_acc: 0.862500 val_loss: 0.3466839 test_acc:0.689286\n",
      "2_121 train_acc: 0.8468 train_loss: 0.387706\tval_acc: 0.875000 val_loss: 0.3444991 test_acc:0.700000\n",
      "2_124 train_acc: 0.8105 train_loss: 0.389900\tval_acc: 0.875000 val_loss: 0.3360873 test_acc:0.714286\n",
      "2_125 train_acc: 0.8266 train_loss: 0.388904\tval_acc: 0.875000 val_loss: 0.3117146 test_acc:0.703571\n",
      "2_161 train_acc: 0.8266 train_loss: 0.462025\tval_acc: 0.887500 val_loss: 0.3013774 test_acc:0.675000\n",
      "2_168 train_acc: 0.8468 train_loss: 0.337371\tval_acc: 0.887500 val_loss: 0.3005225 test_acc:0.689286\n",
      "2_192 train_acc: 0.8105 train_loss: 0.422971\tval_acc: 0.887500 val_loss: 0.2983062 test_acc:0.703571\n",
      "2_199 train_acc: 0.8508 train_loss: 0.361329\tval_acc: 0.900000 val_loss: 0.2890502 test_acc:0.717857\n",
      "2_212 train_acc: 0.8306 train_loss: 0.335548\tval_acc: 0.900000 val_loss: 0.2733150 test_acc:0.710714\n",
      "2_217 train_acc: 0.8629 train_loss: 0.339691\tval_acc: 0.900000 val_loss: 0.2716609 test_acc:0.703571\n",
      "2_218 train_acc: 0.8427 train_loss: 0.375036\tval_acc: 0.912500 val_loss: 0.2736811 test_acc:0.703571\n",
      "2_219 train_acc: 0.8427 train_loss: 0.388118\tval_acc: 0.912500 val_loss: 0.2577419 test_acc:0.696429\n",
      "2_220 train_acc: 0.8629 train_loss: 0.308320\tval_acc: 0.912500 val_loss: 0.2500351 test_acc:0.692857\n",
      "2_231 train_acc: 0.8669 train_loss: 0.357847\tval_acc: 0.912500 val_loss: 0.2416318 test_acc:0.710714\n",
      "2_258 train_acc: 0.8024 train_loss: 0.442649\tval_acc: 0.912500 val_loss: 0.2399651 test_acc:0.696429\n",
      "2_262 train_acc: 0.8387 train_loss: 0.368643\tval_acc: 0.925000 val_loss: 0.2420060 test_acc:0.707143\n",
      "2_320 train_acc: 0.8911 train_loss: 0.293177\tval_acc: 0.925000 val_loss: 0.2331231 test_acc:0.703571\n",
      "2_373 train_acc: 0.8992 train_loss: 0.258855\tval_acc: 0.925000 val_loss: 0.2330713 test_acc:0.707143\n",
      "2_381 train_acc: 0.8427 train_loss: 0.325987\tval_acc: 0.937500 val_loss: 0.2165593 test_acc:0.707143\n",
      "2_514 train_acc: 0.8911 train_loss: 0.274399\tval_acc: 0.937500 val_loss: 0.2157419 test_acc:0.692857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_515 train_acc: 0.8992 train_loss: 0.244166\tval_acc: 0.937500 val_loss: 0.2109477 test_acc:0.682143\n",
      "2_521 train_acc: 0.8427 train_loss: 0.326522\tval_acc: 0.937500 val_loss: 0.1980005 test_acc:0.707143\n",
      "2_598 train_acc: 0.8992 train_loss: 0.227365\tval_acc: 0.937500 val_loss: 0.1951571 test_acc:0.696429\n",
      "2_602 train_acc: 0.8629 train_loss: 0.348106\tval_acc: 0.950000 val_loss: 0.2010336 test_acc:0.692857\n",
      "2_643 train_acc: 0.8710 train_loss: 0.270516\tval_acc: 0.950000 val_loss: 0.1952489 test_acc:0.703571\n",
      "2_706 train_acc: 0.8871 train_loss: 0.283793\tval_acc: 0.962500 val_loss: 0.1930168 test_acc:0.675000\n",
      "epoch:  706 \tThe test accuracy is: 0.675\n",
      " THE BEST ACCURACY IS 0.675\tkappa is 0.35\n",
      "subject 2 duration: 0:52:20.295119\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (280, 3, 1000) subject: 2 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "2_0 train_acc: 0.5363 train_loss: 0.741428\tval_acc: 0.500000 val_loss: 0.6955260 test_acc:0.500000\n",
      "2_1 train_acc: 0.4919 train_loss: 0.745139\tval_acc: 0.512500 val_loss: 0.6977482 test_acc:0.500000\n",
      "2_8 train_acc: 0.5806 train_loss: 0.690240\tval_acc: 0.512500 val_loss: 0.6932944 test_acc:0.478571\n",
      "2_12 train_acc: 0.5363 train_loss: 0.689988\tval_acc: 0.512500 val_loss: 0.6879319 test_acc:0.471429\n",
      "2_16 train_acc: 0.5605 train_loss: 0.674055\tval_acc: 0.537500 val_loss: 0.6827253 test_acc:0.478571\n",
      "2_17 train_acc: 0.5927 train_loss: 0.675020\tval_acc: 0.550000 val_loss: 0.6687586 test_acc:0.503571\n",
      "2_20 train_acc: 0.5524 train_loss: 0.694790\tval_acc: 0.562500 val_loss: 0.6904243 test_acc:0.510714\n",
      "2_24 train_acc: 0.6250 train_loss: 0.644967\tval_acc: 0.687500 val_loss: 0.6367946 test_acc:0.582143\n",
      "2_39 train_acc: 0.7218 train_loss: 0.580705\tval_acc: 0.700000 val_loss: 0.6202944 test_acc:0.628571\n",
      "2_42 train_acc: 0.6694 train_loss: 0.584538\tval_acc: 0.700000 val_loss: 0.5999668 test_acc:0.639286\n",
      "2_44 train_acc: 0.6895 train_loss: 0.599935\tval_acc: 0.712500 val_loss: 0.5704448 test_acc:0.592857\n",
      "2_51 train_acc: 0.7984 train_loss: 0.487341\tval_acc: 0.725000 val_loss: 0.5429980 test_acc:0.625000\n",
      "2_54 train_acc: 0.7419 train_loss: 0.533460\tval_acc: 0.737500 val_loss: 0.5490655 test_acc:0.646429\n",
      "2_55 train_acc: 0.7419 train_loss: 0.531975\tval_acc: 0.800000 val_loss: 0.4784731 test_acc:0.664286\n",
      "2_61 train_acc: 0.7742 train_loss: 0.495155\tval_acc: 0.825000 val_loss: 0.4784523 test_acc:0.664286\n",
      "2_91 train_acc: 0.7339 train_loss: 0.502468\tval_acc: 0.837500 val_loss: 0.4641330 test_acc:0.692857\n",
      "2_124 train_acc: 0.8065 train_loss: 0.413942\tval_acc: 0.862500 val_loss: 0.4292565 test_acc:0.710714\n",
      "2_168 train_acc: 0.8185 train_loss: 0.417022\tval_acc: 0.862500 val_loss: 0.3603829 test_acc:0.689286\n",
      "2_178 train_acc: 0.8105 train_loss: 0.394277\tval_acc: 0.875000 val_loss: 0.3474935 test_acc:0.682143\n",
      "2_211 train_acc: 0.8831 train_loss: 0.309306\tval_acc: 0.875000 val_loss: 0.3299678 test_acc:0.660714\n",
      "2_215 train_acc: 0.8306 train_loss: 0.392100\tval_acc: 0.875000 val_loss: 0.3015832 test_acc:0.675000\n",
      "2_220 train_acc: 0.8427 train_loss: 0.350093\tval_acc: 0.887500 val_loss: 0.2983381 test_acc:0.696429\n",
      "2_232 train_acc: 0.8831 train_loss: 0.297380\tval_acc: 0.900000 val_loss: 0.2982810 test_acc:0.689286\n",
      "2_252 train_acc: 0.8710 train_loss: 0.295539\tval_acc: 0.900000 val_loss: 0.2792987 test_acc:0.682143\n",
      "2_299 train_acc: 0.8427 train_loss: 0.363054\tval_acc: 0.900000 val_loss: 0.2746855 test_acc:0.707143\n",
      "2_313 train_acc: 0.8669 train_loss: 0.302665\tval_acc: 0.900000 val_loss: 0.2731875 test_acc:0.678571\n",
      "2_322 train_acc: 0.8629 train_loss: 0.310386\tval_acc: 0.900000 val_loss: 0.2692201 test_acc:0.732143\n",
      "2_326 train_acc: 0.8710 train_loss: 0.343270\tval_acc: 0.900000 val_loss: 0.2619275 test_acc:0.717857\n",
      "2_334 train_acc: 0.8468 train_loss: 0.377971\tval_acc: 0.912500 val_loss: 0.2235842 test_acc:0.714286\n",
      "2_389 train_acc: 0.8790 train_loss: 0.284629\tval_acc: 0.925000 val_loss: 0.2498064 test_acc:0.721429\n",
      "2_408 train_acc: 0.8871 train_loss: 0.305842\tval_acc: 0.925000 val_loss: 0.2491954 test_acc:0.725000\n",
      "2_423 train_acc: 0.8871 train_loss: 0.288756\tval_acc: 0.925000 val_loss: 0.2485127 test_acc:0.692857\n",
      "2_486 train_acc: 0.8831 train_loss: 0.296089\tval_acc: 0.925000 val_loss: 0.2433230 test_acc:0.721429\n",
      "2_496 train_acc: 0.8669 train_loss: 0.350856\tval_acc: 0.925000 val_loss: 0.2273428 test_acc:0.678571\n",
      "2_501 train_acc: 0.8911 train_loss: 0.292545\tval_acc: 0.925000 val_loss: 0.2245249 test_acc:0.714286\n",
      "2_506 train_acc: 0.8871 train_loss: 0.306288\tval_acc: 0.925000 val_loss: 0.2148360 test_acc:0.710714\n",
      "2_523 train_acc: 0.8871 train_loss: 0.281644\tval_acc: 0.937500 val_loss: 0.2303263 test_acc:0.717857\n",
      "2_547 train_acc: 0.8710 train_loss: 0.322659\tval_acc: 0.937500 val_loss: 0.2097784 test_acc:0.714286\n",
      "2_603 train_acc: 0.8911 train_loss: 0.303936\tval_acc: 0.950000 val_loss: 0.2150243 test_acc:0.703571\n",
      "2_668 train_acc: 0.9113 train_loss: 0.210267\tval_acc: 0.950000 val_loss: 0.1889286 test_acc:0.700000\n",
      "2_705 train_acc: 0.9073 train_loss: 0.231917\tval_acc: 0.962500 val_loss: 0.1697913 test_acc:0.714286\n",
      "epoch:  705 \tThe test accuracy is: 0.7142857142857143\n",
      " THE BEST ACCURACY IS 0.7142857142857143\tkappa is 0.4285714285714286\n",
      "subject 2 duration: 1:10:02.099431\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (280, 3, 1000) subject: 2 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "2_0 train_acc: 0.4758 train_loss: 0.774877\tval_acc: 0.500000 val_loss: 0.6944173 test_acc:0.500000\n",
      "2_1 train_acc: 0.5444 train_loss: 0.703555\tval_acc: 0.525000 val_loss: 0.6933247 test_acc:0.510714\n",
      "2_2 train_acc: 0.5484 train_loss: 0.705080\tval_acc: 0.562500 val_loss: 0.6930152 test_acc:0.496429\n",
      "2_3 train_acc: 0.5484 train_loss: 0.699280\tval_acc: 0.575000 val_loss: 0.6926914 test_acc:0.492857\n",
      "2_18 train_acc: 0.5968 train_loss: 0.682379\tval_acc: 0.587500 val_loss: 0.6950926 test_acc:0.553571\n",
      "2_20 train_acc: 0.5565 train_loss: 0.705981\tval_acc: 0.637500 val_loss: 0.6914518 test_acc:0.542857\n",
      "2_24 train_acc: 0.6331 train_loss: 0.643619\tval_acc: 0.675000 val_loss: 0.6271819 test_acc:0.625000\n",
      "2_30 train_acc: 0.7258 train_loss: 0.544384\tval_acc: 0.675000 val_loss: 0.6093515 test_acc:0.632143\n",
      "2_31 train_acc: 0.7500 train_loss: 0.531092\tval_acc: 0.750000 val_loss: 0.6051480 test_acc:0.664286\n",
      "2_48 train_acc: 0.7258 train_loss: 0.529603\tval_acc: 0.762500 val_loss: 0.5828282 test_acc:0.678571\n",
      "2_56 train_acc: 0.7500 train_loss: 0.521831\tval_acc: 0.762500 val_loss: 0.5692915 test_acc:0.678571\n",
      "2_61 train_acc: 0.7903 train_loss: 0.431912\tval_acc: 0.762500 val_loss: 0.5664985 test_acc:0.696429\n",
      "2_63 train_acc: 0.7581 train_loss: 0.467357\tval_acc: 0.775000 val_loss: 0.5734666 test_acc:0.657143\n",
      "2_64 train_acc: 0.7782 train_loss: 0.486724\tval_acc: 0.800000 val_loss: 0.5465006 test_acc:0.678571\n",
      "2_67 train_acc: 0.7419 train_loss: 0.500789\tval_acc: 0.800000 val_loss: 0.5089617 test_acc:0.678571\n",
      "2_69 train_acc: 0.8306 train_loss: 0.384930\tval_acc: 0.812500 val_loss: 0.5405871 test_acc:0.689286\n",
      "2_85 train_acc: 0.7742 train_loss: 0.493687\tval_acc: 0.812500 val_loss: 0.5037131 test_acc:0.685714\n",
      "2_89 train_acc: 0.7903 train_loss: 0.442043\tval_acc: 0.825000 val_loss: 0.5042509 test_acc:0.667857\n",
      "2_105 train_acc: 0.7944 train_loss: 0.434788\tval_acc: 0.837500 val_loss: 0.4815933 test_acc:0.689286\n",
      "2_119 train_acc: 0.7782 train_loss: 0.447067\tval_acc: 0.850000 val_loss: 0.4654332 test_acc:0.696429\n",
      "2_128 train_acc: 0.8226 train_loss: 0.422720\tval_acc: 0.850000 val_loss: 0.4536152 test_acc:0.703571\n",
      "2_129 train_acc: 0.8145 train_loss: 0.408092\tval_acc: 0.850000 val_loss: 0.4281929 test_acc:0.696429\n",
      "2_135 train_acc: 0.8226 train_loss: 0.405674\tval_acc: 0.862500 val_loss: 0.4228021 test_acc:0.707143\n",
      "2_157 train_acc: 0.8629 train_loss: 0.369573\tval_acc: 0.862500 val_loss: 0.4005189 test_acc:0.696429\n",
      "2_165 train_acc: 0.8306 train_loss: 0.395031\tval_acc: 0.887500 val_loss: 0.3926395 test_acc:0.682143\n",
      "2_247 train_acc: 0.8589 train_loss: 0.324498\tval_acc: 0.887500 val_loss: 0.3844239 test_acc:0.703571\n",
      "2_249 train_acc: 0.8669 train_loss: 0.363278\tval_acc: 0.900000 val_loss: 0.3536321 test_acc:0.689286\n",
      "2_293 train_acc: 0.8871 train_loss: 0.323399\tval_acc: 0.900000 val_loss: 0.3491045 test_acc:0.707143\n",
      "2_297 train_acc: 0.8911 train_loss: 0.281891\tval_acc: 0.900000 val_loss: 0.3379930 test_acc:0.696429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_321 train_acc: 0.8347 train_loss: 0.369875\tval_acc: 0.900000 val_loss: 0.3258873 test_acc:0.696429\n",
      "2_351 train_acc: 0.8226 train_loss: 0.365924\tval_acc: 0.900000 val_loss: 0.3005592 test_acc:0.689286\n",
      "2_498 train_acc: 0.8790 train_loss: 0.305876\tval_acc: 0.912500 val_loss: 0.3135300 test_acc:0.692857\n",
      "2_715 train_acc: 0.9234 train_loss: 0.216241\tval_acc: 0.912500 val_loss: 0.2569596 test_acc:0.703571\n",
      "2_726 train_acc: 0.8710 train_loss: 0.348070\tval_acc: 0.925000 val_loss: 0.3109320 test_acc:0.696429\n",
      "2_986 train_acc: 0.8790 train_loss: 0.285302\tval_acc: 0.925000 val_loss: 0.2419203 test_acc:0.725000\n",
      "epoch:  986 \tThe test accuracy is: 0.725\n",
      " THE BEST ACCURACY IS 0.725\tkappa is 0.44999999999999996\n",
      "subject 2 duration: 1:27:50.504354\n",
      "2 subject 5 fold mean: \n",
      " accuray      71.214286\n",
      "precision    71.435455\n",
      "recall       71.214286\n",
      "f1           71.141761\n",
      "kappa        42.428571\n",
      "dtype: float64\n",
      "seed is 629\n",
      "Subject 3\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 3 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "3_0 train_acc: 0.4879 train_loss: 0.768854\tval_acc: 0.587500 val_loss: 0.6904317 test_acc:0.503125\n",
      "3_12 train_acc: 0.5040 train_loss: 0.708203\tval_acc: 0.600000 val_loss: 0.6885793 test_acc:0.553125\n",
      "3_15 train_acc: 0.5565 train_loss: 0.684673\tval_acc: 0.675000 val_loss: 0.6863600 test_acc:0.534375\n",
      "3_23 train_acc: 0.6169 train_loss: 0.655891\tval_acc: 0.675000 val_loss: 0.6398115 test_acc:0.634375\n",
      "3_26 train_acc: 0.6250 train_loss: 0.647931\tval_acc: 0.700000 val_loss: 0.5976442 test_acc:0.690625\n",
      "3_27 train_acc: 0.6331 train_loss: 0.640222\tval_acc: 0.700000 val_loss: 0.5895752 test_acc:0.681250\n",
      "3_30 train_acc: 0.6411 train_loss: 0.630523\tval_acc: 0.700000 val_loss: 0.5472105 test_acc:0.718750\n",
      "3_31 train_acc: 0.6734 train_loss: 0.626312\tval_acc: 0.712500 val_loss: 0.5394171 test_acc:0.703125\n",
      "3_35 train_acc: 0.6935 train_loss: 0.592862\tval_acc: 0.750000 val_loss: 0.5047797 test_acc:0.793750\n",
      "3_36 train_acc: 0.6210 train_loss: 0.613728\tval_acc: 0.750000 val_loss: 0.5005184 test_acc:0.806250\n",
      "3_37 train_acc: 0.7339 train_loss: 0.573068\tval_acc: 0.762500 val_loss: 0.4988186 test_acc:0.821875\n",
      "3_46 train_acc: 0.7339 train_loss: 0.545075\tval_acc: 0.762500 val_loss: 0.4635450 test_acc:0.840625\n",
      "3_50 train_acc: 0.7056 train_loss: 0.594213\tval_acc: 0.775000 val_loss: 0.4724019 test_acc:0.853125\n",
      "3_57 train_acc: 0.7097 train_loss: 0.568379\tval_acc: 0.800000 val_loss: 0.4826007 test_acc:0.825000\n",
      "3_59 train_acc: 0.7782 train_loss: 0.506976\tval_acc: 0.800000 val_loss: 0.4538854 test_acc:0.846875\n",
      "3_81 train_acc: 0.8065 train_loss: 0.445420\tval_acc: 0.812500 val_loss: 0.4315275 test_acc:0.837500\n",
      "3_90 train_acc: 0.7581 train_loss: 0.487417\tval_acc: 0.825000 val_loss: 0.4410909 test_acc:0.859375\n",
      "3_95 train_acc: 0.7782 train_loss: 0.481007\tval_acc: 0.837500 val_loss: 0.4024712 test_acc:0.865625\n",
      "3_221 train_acc: 0.8750 train_loss: 0.344603\tval_acc: 0.850000 val_loss: 0.3464050 test_acc:0.846875\n",
      "3_270 train_acc: 0.8347 train_loss: 0.363064\tval_acc: 0.850000 val_loss: 0.3422473 test_acc:0.843750\n",
      "3_304 train_acc: 0.8105 train_loss: 0.378139\tval_acc: 0.862500 val_loss: 0.3448333 test_acc:0.859375\n",
      "3_358 train_acc: 0.8347 train_loss: 0.381783\tval_acc: 0.862500 val_loss: 0.3198520 test_acc:0.843750\n",
      "3_409 train_acc: 0.8427 train_loss: 0.339679\tval_acc: 0.875000 val_loss: 0.3185440 test_acc:0.843750\n",
      "3_418 train_acc: 0.8266 train_loss: 0.378037\tval_acc: 0.875000 val_loss: 0.3054785 test_acc:0.834375\n",
      "3_535 train_acc: 0.8508 train_loss: 0.331606\tval_acc: 0.887500 val_loss: 0.3577451 test_acc:0.850000\n",
      "3_543 train_acc: 0.8629 train_loss: 0.320085\tval_acc: 0.887500 val_loss: 0.3067389 test_acc:0.828125\n",
      "3_547 train_acc: 0.8911 train_loss: 0.268850\tval_acc: 0.900000 val_loss: 0.3422660 test_acc:0.831250\n",
      "3_596 train_acc: 0.8831 train_loss: 0.311518\tval_acc: 0.900000 val_loss: 0.2706660 test_acc:0.843750\n",
      "3_645 train_acc: 0.8911 train_loss: 0.323081\tval_acc: 0.900000 val_loss: 0.2556586 test_acc:0.846875\n",
      "3_745 train_acc: 0.8589 train_loss: 0.291836\tval_acc: 0.900000 val_loss: 0.2553469 test_acc:0.840625\n",
      "3_761 train_acc: 0.8548 train_loss: 0.316032\tval_acc: 0.912500 val_loss: 0.2842771 test_acc:0.843750\n",
      "3_783 train_acc: 0.8750 train_loss: 0.258024\tval_acc: 0.912500 val_loss: 0.2282936 test_acc:0.834375\n",
      "3_838 train_acc: 0.8831 train_loss: 0.290146\tval_acc: 0.925000 val_loss: 0.2341376 test_acc:0.840625\n",
      "epoch:  838 \tThe test accuracy is: 0.840625\n",
      " THE BEST ACCURACY IS 0.840625\tkappa is 0.68125\n",
      "subject 3 duration: 0:17:43.102211\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 3 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "3_0 train_acc: 0.4395 train_loss: 0.809168\tval_acc: 0.500000 val_loss: 0.7335898 test_acc:0.500000\n",
      "3_1 train_acc: 0.5242 train_loss: 0.732582\tval_acc: 0.500000 val_loss: 0.7122583 test_acc:0.500000\n",
      "3_2 train_acc: 0.4798 train_loss: 0.747864\tval_acc: 0.500000 val_loss: 0.6928371 test_acc:0.509375\n",
      "3_3 train_acc: 0.4758 train_loss: 0.760710\tval_acc: 0.537500 val_loss: 0.6922549 test_acc:0.543750\n",
      "3_5 train_acc: 0.5000 train_loss: 0.714869\tval_acc: 0.537500 val_loss: 0.6907715 test_acc:0.575000\n",
      "3_6 train_acc: 0.5081 train_loss: 0.699040\tval_acc: 0.550000 val_loss: 0.6903177 test_acc:0.515625\n",
      "3_22 train_acc: 0.5282 train_loss: 0.689764\tval_acc: 0.550000 val_loss: 0.6780688 test_acc:0.565625\n",
      "3_24 train_acc: 0.5444 train_loss: 0.691942\tval_acc: 0.562500 val_loss: 0.6736630 test_acc:0.571875\n",
      "3_29 train_acc: 0.5524 train_loss: 0.689876\tval_acc: 0.650000 val_loss: 0.6537072 test_acc:0.643750\n",
      "3_36 train_acc: 0.6331 train_loss: 0.668370\tval_acc: 0.687500 val_loss: 0.6012870 test_acc:0.725000\n",
      "3_42 train_acc: 0.5847 train_loss: 0.732388\tval_acc: 0.687500 val_loss: 0.5952628 test_acc:0.756250\n",
      "3_44 train_acc: 0.6774 train_loss: 0.577249\tval_acc: 0.712500 val_loss: 0.5651066 test_acc:0.809375\n",
      "3_46 train_acc: 0.7177 train_loss: 0.566868\tval_acc: 0.750000 val_loss: 0.5396312 test_acc:0.806250\n",
      "3_55 train_acc: 0.7339 train_loss: 0.536465\tval_acc: 0.762500 val_loss: 0.5230810 test_acc:0.793750\n",
      "3_57 train_acc: 0.7379 train_loss: 0.534354\tval_acc: 0.762500 val_loss: 0.5161534 test_acc:0.806250\n",
      "3_58 train_acc: 0.7056 train_loss: 0.583831\tval_acc: 0.775000 val_loss: 0.5213715 test_acc:0.825000\n",
      "3_59 train_acc: 0.7137 train_loss: 0.540450\tval_acc: 0.787500 val_loss: 0.5219709 test_acc:0.831250\n",
      "3_68 train_acc: 0.7581 train_loss: 0.516328\tval_acc: 0.800000 val_loss: 0.4862238 test_acc:0.818750\n",
      "3_69 train_acc: 0.8024 train_loss: 0.456585\tval_acc: 0.812500 val_loss: 0.4966653 test_acc:0.806250\n",
      "3_82 train_acc: 0.7177 train_loss: 0.523223\tval_acc: 0.837500 val_loss: 0.4664653 test_acc:0.828125\n",
      "3_83 train_acc: 0.7782 train_loss: 0.470336\tval_acc: 0.837500 val_loss: 0.4624914 test_acc:0.834375\n",
      "3_90 train_acc: 0.7742 train_loss: 0.483165\tval_acc: 0.837500 val_loss: 0.4496628 test_acc:0.831250\n",
      "3_106 train_acc: 0.7621 train_loss: 0.472800\tval_acc: 0.837500 val_loss: 0.4174946 test_acc:0.843750\n",
      "3_114 train_acc: 0.7944 train_loss: 0.457702\tval_acc: 0.850000 val_loss: 0.4111063 test_acc:0.840625\n",
      "3_133 train_acc: 0.8427 train_loss: 0.379413\tval_acc: 0.850000 val_loss: 0.4017474 test_acc:0.831250\n",
      "3_171 train_acc: 0.8347 train_loss: 0.375870\tval_acc: 0.850000 val_loss: 0.4014569 test_acc:0.840625\n",
      "3_176 train_acc: 0.7782 train_loss: 0.462541\tval_acc: 0.862500 val_loss: 0.3587725 test_acc:0.828125\n",
      "3_216 train_acc: 0.8266 train_loss: 0.403975\tval_acc: 0.862500 val_loss: 0.3375831 test_acc:0.834375\n",
      "3_253 train_acc: 0.8226 train_loss: 0.445115\tval_acc: 0.862500 val_loss: 0.3274047 test_acc:0.828125\n",
      "3_255 train_acc: 0.8468 train_loss: 0.369160\tval_acc: 0.875000 val_loss: 0.3567962 test_acc:0.837500\n",
      "3_261 train_acc: 0.8185 train_loss: 0.385415\tval_acc: 0.875000 val_loss: 0.3321217 test_acc:0.837500\n",
      "3_275 train_acc: 0.8669 train_loss: 0.320038\tval_acc: 0.875000 val_loss: 0.3306009 test_acc:0.840625\n",
      "3_276 train_acc: 0.8629 train_loss: 0.334633\tval_acc: 0.887500 val_loss: 0.3039155 test_acc:0.828125\n",
      "3_404 train_acc: 0.8589 train_loss: 0.326143\tval_acc: 0.887500 val_loss: 0.2767668 test_acc:0.831250\n",
      "3_407 train_acc: 0.8427 train_loss: 0.361595\tval_acc: 0.900000 val_loss: 0.2633615 test_acc:0.825000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_497 train_acc: 0.8790 train_loss: 0.315837\tval_acc: 0.900000 val_loss: 0.2612152 test_acc:0.828125\n",
      "3_536 train_acc: 0.8468 train_loss: 0.341511\tval_acc: 0.900000 val_loss: 0.2410101 test_acc:0.821875\n",
      "3_552 train_acc: 0.8548 train_loss: 0.351045\tval_acc: 0.912500 val_loss: 0.2304839 test_acc:0.825000\n",
      "3_620 train_acc: 0.8710 train_loss: 0.331571\tval_acc: 0.912500 val_loss: 0.2274104 test_acc:0.843750\n",
      "3_631 train_acc: 0.8306 train_loss: 0.375688\tval_acc: 0.925000 val_loss: 0.2315803 test_acc:0.825000\n",
      "3_787 train_acc: 0.8871 train_loss: 0.279292\tval_acc: 0.925000 val_loss: 0.2116683 test_acc:0.815625\n",
      "3_825 train_acc: 0.8952 train_loss: 0.261759\tval_acc: 0.925000 val_loss: 0.1971940 test_acc:0.834375\n",
      "3_862 train_acc: 0.8952 train_loss: 0.257443\tval_acc: 0.937500 val_loss: 0.2191662 test_acc:0.818750\n",
      "epoch:  862 \tThe test accuracy is: 0.81875\n",
      " THE BEST ACCURACY IS 0.81875\tkappa is 0.6375\n",
      "subject 3 duration: 0:35:30.459526\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 3 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "3_0 train_acc: 0.5121 train_loss: 0.730337\tval_acc: 0.500000 val_loss: 0.7055513 test_acc:0.500000\n",
      "3_1 train_acc: 0.5242 train_loss: 0.723858\tval_acc: 0.537500 val_loss: 0.6908538 test_acc:0.546875\n",
      "3_3 train_acc: 0.5081 train_loss: 0.723059\tval_acc: 0.537500 val_loss: 0.6893564 test_acc:0.525000\n",
      "3_5 train_acc: 0.5242 train_loss: 0.693784\tval_acc: 0.562500 val_loss: 0.6885686 test_acc:0.559375\n",
      "3_7 train_acc: 0.5323 train_loss: 0.688412\tval_acc: 0.575000 val_loss: 0.6875527 test_acc:0.640625\n",
      "3_8 train_acc: 0.5121 train_loss: 0.694237\tval_acc: 0.575000 val_loss: 0.6851981 test_acc:0.556250\n",
      "3_10 train_acc: 0.5403 train_loss: 0.690411\tval_acc: 0.612500 val_loss: 0.6814818 test_acc:0.653125\n",
      "3_11 train_acc: 0.5161 train_loss: 0.693940\tval_acc: 0.637500 val_loss: 0.6799203 test_acc:0.640625\n",
      "3_12 train_acc: 0.5444 train_loss: 0.696495\tval_acc: 0.650000 val_loss: 0.6766849 test_acc:0.631250\n",
      "3_16 train_acc: 0.6613 train_loss: 0.636812\tval_acc: 0.675000 val_loss: 0.6286598 test_acc:0.675000\n",
      "3_18 train_acc: 0.6331 train_loss: 0.655857\tval_acc: 0.687500 val_loss: 0.5932277 test_acc:0.687500\n",
      "3_19 train_acc: 0.6290 train_loss: 0.658432\tval_acc: 0.700000 val_loss: 0.6023617 test_acc:0.681250\n",
      "3_22 train_acc: 0.5847 train_loss: 0.678375\tval_acc: 0.712500 val_loss: 0.5674047 test_acc:0.718750\n",
      "3_23 train_acc: 0.6573 train_loss: 0.611472\tval_acc: 0.737500 val_loss: 0.5732290 test_acc:0.696875\n",
      "3_26 train_acc: 0.6532 train_loss: 0.608852\tval_acc: 0.737500 val_loss: 0.5531104 test_acc:0.725000\n",
      "3_31 train_acc: 0.6774 train_loss: 0.605150\tval_acc: 0.762500 val_loss: 0.5183957 test_acc:0.765625\n",
      "3_40 train_acc: 0.7460 train_loss: 0.536322\tval_acc: 0.762500 val_loss: 0.5075923 test_acc:0.793750\n",
      "3_45 train_acc: 0.7177 train_loss: 0.555102\tval_acc: 0.787500 val_loss: 0.4969185 test_acc:0.793750\n",
      "3_51 train_acc: 0.6935 train_loss: 0.556533\tval_acc: 0.787500 val_loss: 0.4836202 test_acc:0.806250\n",
      "3_57 train_acc: 0.7298 train_loss: 0.508240\tval_acc: 0.800000 val_loss: 0.4753766 test_acc:0.809375\n",
      "3_68 train_acc: 0.7903 train_loss: 0.476858\tval_acc: 0.800000 val_loss: 0.4639707 test_acc:0.812500\n",
      "3_75 train_acc: 0.8105 train_loss: 0.450131\tval_acc: 0.812500 val_loss: 0.4921076 test_acc:0.812500\n",
      "3_77 train_acc: 0.7419 train_loss: 0.546818\tval_acc: 0.825000 val_loss: 0.4634880 test_acc:0.825000\n",
      "3_81 train_acc: 0.7823 train_loss: 0.455955\tval_acc: 0.825000 val_loss: 0.4605292 test_acc:0.821875\n",
      "3_88 train_acc: 0.7944 train_loss: 0.426475\tval_acc: 0.825000 val_loss: 0.4439959 test_acc:0.831250\n",
      "3_90 train_acc: 0.7661 train_loss: 0.529902\tval_acc: 0.837500 val_loss: 0.4296146 test_acc:0.806250\n",
      "3_100 train_acc: 0.7621 train_loss: 0.496022\tval_acc: 0.837500 val_loss: 0.4270909 test_acc:0.825000\n",
      "3_101 train_acc: 0.7581 train_loss: 0.483108\tval_acc: 0.837500 val_loss: 0.4190777 test_acc:0.825000\n",
      "3_106 train_acc: 0.7984 train_loss: 0.452528\tval_acc: 0.837500 val_loss: 0.4089512 test_acc:0.815625\n",
      "3_117 train_acc: 0.7782 train_loss: 0.470858\tval_acc: 0.862500 val_loss: 0.4139990 test_acc:0.803125\n",
      "3_118 train_acc: 0.8065 train_loss: 0.386004\tval_acc: 0.862500 val_loss: 0.4013295 test_acc:0.818750\n",
      "3_144 train_acc: 0.8065 train_loss: 0.458673\tval_acc: 0.875000 val_loss: 0.3867278 test_acc:0.818750\n",
      "3_151 train_acc: 0.8065 train_loss: 0.460189\tval_acc: 0.875000 val_loss: 0.3806717 test_acc:0.821875\n",
      "3_230 train_acc: 0.8065 train_loss: 0.423105\tval_acc: 0.875000 val_loss: 0.3745683 test_acc:0.821875\n",
      "3_370 train_acc: 0.7984 train_loss: 0.471391\tval_acc: 0.875000 val_loss: 0.3674139 test_acc:0.825000\n",
      "3_445 train_acc: 0.8589 train_loss: 0.334083\tval_acc: 0.875000 val_loss: 0.3554295 test_acc:0.843750\n",
      "3_524 train_acc: 0.8952 train_loss: 0.284770\tval_acc: 0.875000 val_loss: 0.3512781 test_acc:0.840625\n",
      "3_532 train_acc: 0.8226 train_loss: 0.343160\tval_acc: 0.875000 val_loss: 0.3493270 test_acc:0.846875\n",
      "3_570 train_acc: 0.9032 train_loss: 0.272167\tval_acc: 0.875000 val_loss: 0.3305737 test_acc:0.837500\n",
      "3_578 train_acc: 0.8992 train_loss: 0.244166\tval_acc: 0.875000 val_loss: 0.3242480 test_acc:0.831250\n",
      "3_594 train_acc: 0.8871 train_loss: 0.299826\tval_acc: 0.887500 val_loss: 0.3216593 test_acc:0.840625\n",
      "3_599 train_acc: 0.9113 train_loss: 0.231347\tval_acc: 0.887500 val_loss: 0.3084789 test_acc:0.834375\n",
      "3_622 train_acc: 0.8347 train_loss: 0.353766\tval_acc: 0.900000 val_loss: 0.3173169 test_acc:0.828125\n",
      "3_624 train_acc: 0.8911 train_loss: 0.266703\tval_acc: 0.900000 val_loss: 0.3095617 test_acc:0.837500\n",
      "3_643 train_acc: 0.8589 train_loss: 0.334927\tval_acc: 0.900000 val_loss: 0.2969297 test_acc:0.815625\n",
      "3_721 train_acc: 0.8508 train_loss: 0.331448\tval_acc: 0.900000 val_loss: 0.2693730 test_acc:0.831250\n",
      "3_746 train_acc: 0.8831 train_loss: 0.269486\tval_acc: 0.912500 val_loss: 0.3027164 test_acc:0.825000\n",
      "3_782 train_acc: 0.8952 train_loss: 0.276926\tval_acc: 0.912500 val_loss: 0.2773878 test_acc:0.837500\n",
      "3_798 train_acc: 0.8629 train_loss: 0.304412\tval_acc: 0.925000 val_loss: 0.2872925 test_acc:0.834375\n",
      "3_876 train_acc: 0.8952 train_loss: 0.276891\tval_acc: 0.925000 val_loss: 0.2817433 test_acc:0.831250\n",
      "3_924 train_acc: 0.9032 train_loss: 0.250908\tval_acc: 0.925000 val_loss: 0.2754499 test_acc:0.840625\n",
      "3_933 train_acc: 0.8952 train_loss: 0.231855\tval_acc: 0.925000 val_loss: 0.2518190 test_acc:0.828125\n",
      "epoch:  933 \tThe test accuracy is: 0.828125\n",
      " THE BEST ACCURACY IS 0.828125\tkappa is 0.65625\n",
      "subject 3 duration: 0:53:30.018763\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 3 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "3_0 train_acc: 0.4435 train_loss: 0.788756\tval_acc: 0.500000 val_loss: 0.6969642 test_acc:0.500000\n",
      "3_2 train_acc: 0.5806 train_loss: 0.694562\tval_acc: 0.500000 val_loss: 0.6967393 test_acc:0.500000\n",
      "3_9 train_acc: 0.5000 train_loss: 0.688786\tval_acc: 0.525000 val_loss: 0.6959366 test_acc:0.484375\n",
      "3_12 train_acc: 0.5000 train_loss: 0.701676\tval_acc: 0.525000 val_loss: 0.6928367 test_acc:0.571875\n",
      "3_13 train_acc: 0.5323 train_loss: 0.693895\tval_acc: 0.525000 val_loss: 0.6921918 test_acc:0.581250\n",
      "3_18 train_acc: 0.5524 train_loss: 0.687153\tval_acc: 0.525000 val_loss: 0.6799960 test_acc:0.603125\n",
      "3_19 train_acc: 0.5484 train_loss: 0.684671\tval_acc: 0.575000 val_loss: 0.6752280 test_acc:0.615625\n",
      "3_21 train_acc: 0.4919 train_loss: 0.710501\tval_acc: 0.625000 val_loss: 0.6660293 test_acc:0.700000\n",
      "3_23 train_acc: 0.5685 train_loss: 0.659507\tval_acc: 0.700000 val_loss: 0.6344506 test_acc:0.659375\n",
      "3_24 train_acc: 0.5766 train_loss: 0.669554\tval_acc: 0.712500 val_loss: 0.6360210 test_acc:0.650000\n",
      "3_26 train_acc: 0.5685 train_loss: 0.683262\tval_acc: 0.712500 val_loss: 0.6257160 test_acc:0.709375\n",
      "3_27 train_acc: 0.6169 train_loss: 0.657881\tval_acc: 0.712500 val_loss: 0.6136956 test_acc:0.715625\n",
      "3_32 train_acc: 0.6048 train_loss: 0.653919\tval_acc: 0.725000 val_loss: 0.6130181 test_acc:0.768750\n",
      "3_34 train_acc: 0.6694 train_loss: 0.613876\tval_acc: 0.725000 val_loss: 0.5789976 test_acc:0.809375\n",
      "3_38 train_acc: 0.6452 train_loss: 0.621886\tval_acc: 0.762500 val_loss: 0.5543156 test_acc:0.787500\n",
      "3_42 train_acc: 0.6895 train_loss: 0.591342\tval_acc: 0.775000 val_loss: 0.5181996 test_acc:0.815625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_47 train_acc: 0.6774 train_loss: 0.584470\tval_acc: 0.775000 val_loss: 0.5109367 test_acc:0.837500\n",
      "3_65 train_acc: 0.7258 train_loss: 0.538360\tval_acc: 0.775000 val_loss: 0.4946293 test_acc:0.843750\n",
      "3_68 train_acc: 0.7419 train_loss: 0.524675\tval_acc: 0.787500 val_loss: 0.4746709 test_acc:0.846875\n",
      "3_77 train_acc: 0.7218 train_loss: 0.544410\tval_acc: 0.800000 val_loss: 0.4564450 test_acc:0.840625\n",
      "3_98 train_acc: 0.7863 train_loss: 0.471331\tval_acc: 0.800000 val_loss: 0.4435409 test_acc:0.837500\n",
      "3_110 train_acc: 0.8347 train_loss: 0.396775\tval_acc: 0.800000 val_loss: 0.4355713 test_acc:0.828125\n",
      "3_113 train_acc: 0.7621 train_loss: 0.492736\tval_acc: 0.825000 val_loss: 0.4375253 test_acc:0.828125\n",
      "3_114 train_acc: 0.7944 train_loss: 0.468530\tval_acc: 0.825000 val_loss: 0.4175421 test_acc:0.843750\n",
      "3_123 train_acc: 0.8065 train_loss: 0.433712\tval_acc: 0.837500 val_loss: 0.4092407 test_acc:0.825000\n",
      "3_149 train_acc: 0.7984 train_loss: 0.413807\tval_acc: 0.850000 val_loss: 0.4206168 test_acc:0.834375\n",
      "3_166 train_acc: 0.8024 train_loss: 0.448288\tval_acc: 0.850000 val_loss: 0.3794979 test_acc:0.843750\n",
      "3_192 train_acc: 0.8306 train_loss: 0.399690\tval_acc: 0.850000 val_loss: 0.3790368 test_acc:0.837500\n",
      "3_196 train_acc: 0.8387 train_loss: 0.393242\tval_acc: 0.862500 val_loss: 0.3314170 test_acc:0.831250\n",
      "3_205 train_acc: 0.7823 train_loss: 0.479050\tval_acc: 0.875000 val_loss: 0.3441379 test_acc:0.834375\n",
      "3_255 train_acc: 0.8508 train_loss: 0.349014\tval_acc: 0.875000 val_loss: 0.3216698 test_acc:0.834375\n",
      "3_260 train_acc: 0.8306 train_loss: 0.388807\tval_acc: 0.887500 val_loss: 0.3340813 test_acc:0.837500\n",
      "3_276 train_acc: 0.8548 train_loss: 0.307198\tval_acc: 0.900000 val_loss: 0.3198742 test_acc:0.840625\n",
      "3_330 train_acc: 0.8226 train_loss: 0.357132\tval_acc: 0.900000 val_loss: 0.2691543 test_acc:0.828125\n",
      "3_566 train_acc: 0.8952 train_loss: 0.300546\tval_acc: 0.900000 val_loss: 0.2601922 test_acc:0.840625\n",
      "3_629 train_acc: 0.8347 train_loss: 0.383471\tval_acc: 0.900000 val_loss: 0.2463830 test_acc:0.834375\n",
      "3_651 train_acc: 0.8710 train_loss: 0.277963\tval_acc: 0.900000 val_loss: 0.2460253 test_acc:0.831250\n",
      "3_661 train_acc: 0.8831 train_loss: 0.270742\tval_acc: 0.900000 val_loss: 0.2409863 test_acc:0.837500\n",
      "3_687 train_acc: 0.8952 train_loss: 0.271078\tval_acc: 0.900000 val_loss: 0.2279309 test_acc:0.828125\n",
      "3_702 train_acc: 0.9032 train_loss: 0.229895\tval_acc: 0.900000 val_loss: 0.2184833 test_acc:0.831250\n",
      "3_748 train_acc: 0.8911 train_loss: 0.277443\tval_acc: 0.912500 val_loss: 0.2359157 test_acc:0.825000\n",
      "3_856 train_acc: 0.9153 train_loss: 0.223558\tval_acc: 0.912500 val_loss: 0.2213147 test_acc:0.828125\n",
      "3_907 train_acc: 0.8790 train_loss: 0.319933\tval_acc: 0.912500 val_loss: 0.2200560 test_acc:0.837500\n",
      "epoch:  907 \tThe test accuracy is: 0.8375\n",
      " THE BEST ACCURACY IS 0.8375\tkappa is 0.675\n",
      "subject 3 duration: 1:06:38.195175\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 3 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "3_0 train_acc: 0.4516 train_loss: 0.777090\tval_acc: 0.512500 val_loss: 0.6931744 test_acc:0.578125\n",
      "3_2 train_acc: 0.5726 train_loss: 0.683452\tval_acc: 0.512500 val_loss: 0.6930829 test_acc:0.506250\n",
      "3_4 train_acc: 0.4919 train_loss: 0.718393\tval_acc: 0.537500 val_loss: 0.6920698 test_acc:0.571875\n",
      "3_6 train_acc: 0.4677 train_loss: 0.706654\tval_acc: 0.537500 val_loss: 0.6906611 test_acc:0.540625\n",
      "3_9 train_acc: 0.5484 train_loss: 0.683964\tval_acc: 0.562500 val_loss: 0.6900193 test_acc:0.553125\n",
      "3_13 train_acc: 0.5202 train_loss: 0.696716\tval_acc: 0.612500 val_loss: 0.6867715 test_acc:0.609375\n",
      "3_19 train_acc: 0.5202 train_loss: 0.691129\tval_acc: 0.650000 val_loss: 0.6701649 test_acc:0.631250\n",
      "3_26 train_acc: 0.6371 train_loss: 0.642411\tval_acc: 0.662500 val_loss: 0.6517099 test_acc:0.693750\n",
      "3_28 train_acc: 0.6734 train_loss: 0.611299\tval_acc: 0.675000 val_loss: 0.6132361 test_acc:0.728125\n",
      "3_29 train_acc: 0.6694 train_loss: 0.636522\tval_acc: 0.675000 val_loss: 0.6027368 test_acc:0.750000\n",
      "3_30 train_acc: 0.6532 train_loss: 0.614779\tval_acc: 0.700000 val_loss: 0.6029173 test_acc:0.746875\n",
      "3_33 train_acc: 0.7016 train_loss: 0.582111\tval_acc: 0.700000 val_loss: 0.5857313 test_acc:0.765625\n",
      "3_34 train_acc: 0.6371 train_loss: 0.636002\tval_acc: 0.737500 val_loss: 0.5702546 test_acc:0.778125\n",
      "3_41 train_acc: 0.7540 train_loss: 0.544613\tval_acc: 0.775000 val_loss: 0.5546314 test_acc:0.812500\n",
      "3_46 train_acc: 0.7742 train_loss: 0.492644\tval_acc: 0.775000 val_loss: 0.5168152 test_acc:0.800000\n",
      "3_70 train_acc: 0.7702 train_loss: 0.483501\tval_acc: 0.775000 val_loss: 0.4973097 test_acc:0.828125\n",
      "3_76 train_acc: 0.7339 train_loss: 0.518105\tval_acc: 0.775000 val_loss: 0.4959878 test_acc:0.825000\n",
      "3_88 train_acc: 0.8105 train_loss: 0.437581\tval_acc: 0.787500 val_loss: 0.5086248 test_acc:0.828125\n",
      "3_123 train_acc: 0.8185 train_loss: 0.413936\tval_acc: 0.825000 val_loss: 0.4816836 test_acc:0.806250\n",
      "3_149 train_acc: 0.8306 train_loss: 0.383980\tval_acc: 0.837500 val_loss: 0.4317607 test_acc:0.815625\n",
      "3_224 train_acc: 0.8024 train_loss: 0.444390\tval_acc: 0.850000 val_loss: 0.4564453 test_acc:0.825000\n",
      "3_235 train_acc: 0.8427 train_loss: 0.378003\tval_acc: 0.850000 val_loss: 0.4202465 test_acc:0.809375\n",
      "3_263 train_acc: 0.8185 train_loss: 0.417874\tval_acc: 0.875000 val_loss: 0.4007300 test_acc:0.825000\n",
      "3_293 train_acc: 0.8589 train_loss: 0.371453\tval_acc: 0.887500 val_loss: 0.4106719 test_acc:0.809375\n",
      "3_417 train_acc: 0.8185 train_loss: 0.409865\tval_acc: 0.887500 val_loss: 0.3929272 test_acc:0.818750\n",
      "3_451 train_acc: 0.8871 train_loss: 0.330939\tval_acc: 0.887500 val_loss: 0.3529051 test_acc:0.834375\n",
      "3_453 train_acc: 0.8669 train_loss: 0.298940\tval_acc: 0.900000 val_loss: 0.3659605 test_acc:0.837500\n",
      "3_821 train_acc: 0.8790 train_loss: 0.284888\tval_acc: 0.900000 val_loss: 0.3507032 test_acc:0.831250\n",
      "3_823 train_acc: 0.8911 train_loss: 0.291215\tval_acc: 0.912500 val_loss: 0.3133782 test_acc:0.831250\n",
      "3_941 train_acc: 0.8911 train_loss: 0.272653\tval_acc: 0.912500 val_loss: 0.3028030 test_acc:0.812500\n",
      "epoch:  941 \tThe test accuracy is: 0.8125\n",
      " THE BEST ACCURACY IS 0.8125\tkappa is 0.625\n",
      "subject 3 duration: 1:19:23.554959\n",
      "3 subject 5 fold mean: \n",
      " accuray      82.750000\n",
      "precision    83.089528\n",
      "recall       82.750000\n",
      "f1           82.703109\n",
      "kappa        65.500000\n",
      "dtype: float64\n",
      "seed is 1722\n",
      "Subject 4\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 4 fold: 1\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "4_0 train_acc: 0.4621 train_loss: 0.723050\tval_acc: 0.500000 val_loss: 0.7095869 test_acc:0.500000\n",
      "4_1 train_acc: 0.5417 train_loss: 0.715044\tval_acc: 0.500000 val_loss: 0.6880081 test_acc:0.500000\n",
      "4_2 train_acc: 0.6250 train_loss: 0.674291\tval_acc: 0.511905 val_loss: 0.6688592 test_acc:0.500000\n",
      "4_3 train_acc: 0.5568 train_loss: 0.681731\tval_acc: 0.511905 val_loss: 0.6454476 test_acc:0.500000\n",
      "4_4 train_acc: 0.7197 train_loss: 0.573731\tval_acc: 0.857143 val_loss: 0.4184750 test_acc:0.750000\n",
      "4_6 train_acc: 0.8788 train_loss: 0.326205\tval_acc: 0.880952 val_loss: 0.3384153 test_acc:0.828125\n",
      "4_7 train_acc: 0.9091 train_loss: 0.259317\tval_acc: 0.892857 val_loss: 0.2923567 test_acc:0.821875\n",
      "4_8 train_acc: 0.9091 train_loss: 0.214050\tval_acc: 0.892857 val_loss: 0.2791158 test_acc:0.865625\n",
      "4_9 train_acc: 0.9205 train_loss: 0.241005\tval_acc: 0.928571 val_loss: 0.2235261 test_acc:0.846875\n",
      "4_11 train_acc: 0.9356 train_loss: 0.202551\tval_acc: 0.940476 val_loss: 0.2059834 test_acc:0.831250\n",
      "4_12 train_acc: 0.9470 train_loss: 0.147027\tval_acc: 0.952381 val_loss: 0.1881915 test_acc:0.840625\n",
      "4_16 train_acc: 0.9280 train_loss: 0.177237\tval_acc: 0.964286 val_loss: 0.1780664 test_acc:0.862500\n",
      "4_29 train_acc: 0.9508 train_loss: 0.140078\tval_acc: 0.964286 val_loss: 0.1674051 test_acc:0.906250\n",
      "4_68 train_acc: 0.9773 train_loss: 0.072787\tval_acc: 0.964286 val_loss: 0.1653317 test_acc:0.946875\n",
      "4_128 train_acc: 0.9773 train_loss: 0.075848\tval_acc: 0.964286 val_loss: 0.1513415 test_acc:0.953125\n",
      "4_174 train_acc: 0.9811 train_loss: 0.049611\tval_acc: 0.964286 val_loss: 0.1496560 test_acc:0.953125\n",
      "4_185 train_acc: 0.9697 train_loss: 0.076640\tval_acc: 0.976190 val_loss: 0.1503972 test_acc:0.962500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_221 train_acc: 0.9886 train_loss: 0.037371\tval_acc: 0.976190 val_loss: 0.1293990 test_acc:0.950000\n",
      "4_242 train_acc: 0.9924 train_loss: 0.024103\tval_acc: 0.976190 val_loss: 0.1253626 test_acc:0.968750\n",
      "4_320 train_acc: 0.9886 train_loss: 0.032823\tval_acc: 0.976190 val_loss: 0.1205523 test_acc:0.968750\n",
      "4_322 train_acc: 0.9735 train_loss: 0.055207\tval_acc: 0.976190 val_loss: 0.1053252 test_acc:0.956250\n",
      "4_389 train_acc: 0.9848 train_loss: 0.051930\tval_acc: 0.976190 val_loss: 0.0888669 test_acc:0.978125\n",
      "4_714 train_acc: 0.9924 train_loss: 0.031529\tval_acc: 0.976190 val_loss: 0.0850956 test_acc:0.978125\n",
      "4_804 train_acc: 0.9811 train_loss: 0.048090\tval_acc: 0.976190 val_loss: 0.0840593 test_acc:0.984375\n",
      "4_826 train_acc: 0.9886 train_loss: 0.039879\tval_acc: 0.988095 val_loss: 0.0840576 test_acc:0.978125\n",
      "4_830 train_acc: 0.9886 train_loss: 0.039940\tval_acc: 0.988095 val_loss: 0.0833713 test_acc:0.978125\n",
      "4_845 train_acc: 0.9811 train_loss: 0.043742\tval_acc: 0.988095 val_loss: 0.0764157 test_acc:0.975000\n",
      "4_905 train_acc: 0.9773 train_loss: 0.052752\tval_acc: 0.988095 val_loss: 0.0683822 test_acc:0.981250\n",
      "4_930 train_acc: 0.9848 train_loss: 0.056044\tval_acc: 0.988095 val_loss: 0.0633225 test_acc:0.975000\n",
      "epoch:  930 \tThe test accuracy is: 0.975\n",
      " THE BEST ACCURACY IS 0.975\tkappa is 0.95\n",
      "subject 4 duration: 0:12:48.976731\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 4 fold: 2\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "4_0 train_acc: 0.5265 train_loss: 0.740749\tval_acc: 0.500000 val_loss: 0.6958007 test_acc:0.500000\n",
      "4_1 train_acc: 0.5795 train_loss: 0.692353\tval_acc: 0.535714 val_loss: 0.6829054 test_acc:0.540625\n",
      "4_2 train_acc: 0.6212 train_loss: 0.658619\tval_acc: 0.607143 val_loss: 0.6563984 test_acc:0.559375\n",
      "4_3 train_acc: 0.6591 train_loss: 0.617287\tval_acc: 0.678571 val_loss: 0.5588096 test_acc:0.571875\n",
      "4_4 train_acc: 0.6364 train_loss: 0.725443\tval_acc: 0.869048 val_loss: 0.3527288 test_acc:0.634375\n",
      "4_5 train_acc: 0.8258 train_loss: 0.396037\tval_acc: 0.869048 val_loss: 0.2654420 test_acc:0.815625\n",
      "4_6 train_acc: 0.8561 train_loss: 0.355896\tval_acc: 0.880952 val_loss: 0.3106240 test_acc:0.840625\n",
      "4_7 train_acc: 0.8523 train_loss: 0.401094\tval_acc: 0.928571 val_loss: 0.1834451 test_acc:0.825000\n",
      "4_8 train_acc: 0.8523 train_loss: 0.347998\tval_acc: 0.928571 val_loss: 0.1264469 test_acc:0.790625\n",
      "4_9 train_acc: 0.8864 train_loss: 0.262045\tval_acc: 0.940476 val_loss: 0.1233081 test_acc:0.809375\n",
      "4_10 train_acc: 0.9394 train_loss: 0.210466\tval_acc: 0.952381 val_loss: 0.1352874 test_acc:0.784375\n",
      "4_12 train_acc: 0.9280 train_loss: 0.198208\tval_acc: 0.952381 val_loss: 0.0936867 test_acc:0.828125\n",
      "4_13 train_acc: 0.9167 train_loss: 0.224320\tval_acc: 0.964286 val_loss: 0.0945833 test_acc:0.818750\n",
      "4_14 train_acc: 0.9167 train_loss: 0.224126\tval_acc: 0.976190 val_loss: 0.0872159 test_acc:0.818750\n",
      "4_18 train_acc: 0.9356 train_loss: 0.150752\tval_acc: 0.976190 val_loss: 0.0644875 test_acc:0.834375\n",
      "4_19 train_acc: 0.9318 train_loss: 0.181927\tval_acc: 0.976190 val_loss: 0.0624433 test_acc:0.831250\n",
      "4_22 train_acc: 0.9508 train_loss: 0.133692\tval_acc: 0.976190 val_loss: 0.0555093 test_acc:0.862500\n",
      "4_25 train_acc: 0.9432 train_loss: 0.146858\tval_acc: 0.976190 val_loss: 0.0531278 test_acc:0.859375\n",
      "4_26 train_acc: 0.9735 train_loss: 0.111486\tval_acc: 0.976190 val_loss: 0.0377382 test_acc:0.865625\n",
      "4_33 train_acc: 0.9470 train_loss: 0.124983\tval_acc: 0.988095 val_loss: 0.0446685 test_acc:0.843750\n",
      "4_36 train_acc: 0.9508 train_loss: 0.124354\tval_acc: 0.988095 val_loss: 0.0390169 test_acc:0.890625\n",
      "4_42 train_acc: 0.9583 train_loss: 0.113691\tval_acc: 0.988095 val_loss: 0.0308917 test_acc:0.856250\n",
      "4_53 train_acc: 0.9735 train_loss: 0.072893\tval_acc: 0.988095 val_loss: 0.0268640 test_acc:0.856250\n",
      "4_54 train_acc: 0.9621 train_loss: 0.103717\tval_acc: 0.988095 val_loss: 0.0241631 test_acc:0.912500\n",
      "4_58 train_acc: 0.9545 train_loss: 0.129195\tval_acc: 0.988095 val_loss: 0.0182247 test_acc:0.921875\n",
      "4_65 train_acc: 0.9811 train_loss: 0.059645\tval_acc: 1.000000 val_loss: 0.0174317 test_acc:0.890625\n",
      "4_124 train_acc: 0.9735 train_loss: 0.060650\tval_acc: 1.000000 val_loss: 0.0119141 test_acc:0.934375\n",
      "4_358 train_acc: 0.9848 train_loss: 0.021421\tval_acc: 1.000000 val_loss: 0.0118054 test_acc:0.968750\n",
      "4_540 train_acc: 0.9773 train_loss: 0.061631\tval_acc: 1.000000 val_loss: 0.0111949 test_acc:0.968750\n",
      "4_541 train_acc: 0.9773 train_loss: 0.050253\tval_acc: 1.000000 val_loss: 0.0110808 test_acc:0.975000\n",
      "4_586 train_acc: 0.9962 train_loss: 0.016346\tval_acc: 1.000000 val_loss: 0.0091884 test_acc:0.975000\n",
      "4_906 train_acc: 0.9773 train_loss: 0.033566\tval_acc: 1.000000 val_loss: 0.0086484 test_acc:0.978125\n",
      "epoch:  906 \tThe test accuracy is: 0.978125\n",
      " THE BEST ACCURACY IS 0.978125\tkappa is 0.95625\n",
      "subject 4 duration: 0:19:36.474677\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 4 fold: 3\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "4_0 train_acc: 0.5038 train_loss: 0.754853\tval_acc: 0.500000 val_loss: 0.8008194 test_acc:0.500000\n",
      "4_3 train_acc: 0.7992 train_loss: 0.475404\tval_acc: 0.535714 val_loss: 1.0867931 test_acc:0.740625\n",
      "4_4 train_acc: 0.8144 train_loss: 0.466521\tval_acc: 0.690476 val_loss: 0.8319157 test_acc:0.915625\n",
      "4_5 train_acc: 0.8712 train_loss: 0.305692\tval_acc: 0.821429 val_loss: 0.4406783 test_acc:0.843750\n",
      "4_8 train_acc: 0.9091 train_loss: 0.233211\tval_acc: 0.892857 val_loss: 0.2885324 test_acc:0.837500\n",
      "4_10 train_acc: 0.9242 train_loss: 0.199939\tval_acc: 0.940476 val_loss: 0.1882919 test_acc:0.818750\n",
      "4_16 train_acc: 0.9583 train_loss: 0.150874\tval_acc: 0.952381 val_loss: 0.2008083 test_acc:0.806250\n",
      "4_18 train_acc: 0.9242 train_loss: 0.147069\tval_acc: 0.952381 val_loss: 0.1720438 test_acc:0.834375\n",
      "4_31 train_acc: 0.9621 train_loss: 0.120298\tval_acc: 0.952381 val_loss: 0.1674297 test_acc:0.840625\n",
      "4_33 train_acc: 0.9508 train_loss: 0.151043\tval_acc: 0.952381 val_loss: 0.1561061 test_acc:0.846875\n",
      "4_37 train_acc: 0.9583 train_loss: 0.144977\tval_acc: 0.964286 val_loss: 0.1752670 test_acc:0.865625\n",
      "4_44 train_acc: 0.9886 train_loss: 0.050581\tval_acc: 0.964286 val_loss: 0.1470405 test_acc:0.868750\n",
      "4_49 train_acc: 0.9886 train_loss: 0.047685\tval_acc: 0.964286 val_loss: 0.1416416 test_acc:0.868750\n",
      "4_63 train_acc: 0.9697 train_loss: 0.082977\tval_acc: 0.964286 val_loss: 0.1404663 test_acc:0.871875\n",
      "4_65 train_acc: 0.9886 train_loss: 0.028785\tval_acc: 0.964286 val_loss: 0.1307085 test_acc:0.868750\n",
      "4_76 train_acc: 0.9697 train_loss: 0.075836\tval_acc: 0.964286 val_loss: 0.1252161 test_acc:0.906250\n",
      "4_88 train_acc: 0.9811 train_loss: 0.057959\tval_acc: 0.976190 val_loss: 0.1226981 test_acc:0.893750\n",
      "4_109 train_acc: 0.9508 train_loss: 0.110972\tval_acc: 0.976190 val_loss: 0.1103976 test_acc:0.928125\n",
      "4_171 train_acc: 0.9924 train_loss: 0.029034\tval_acc: 0.976190 val_loss: 0.1053861 test_acc:0.925000\n",
      "4_189 train_acc: 0.9811 train_loss: 0.078601\tval_acc: 0.976190 val_loss: 0.1030106 test_acc:0.953125\n",
      "4_197 train_acc: 1.0000 train_loss: 0.014668\tval_acc: 0.976190 val_loss: 0.0940189 test_acc:0.943750\n",
      "4_210 train_acc: 0.9924 train_loss: 0.027989\tval_acc: 0.976190 val_loss: 0.0934673 test_acc:0.937500\n",
      "4_215 train_acc: 0.9962 train_loss: 0.037435\tval_acc: 0.976190 val_loss: 0.0927282 test_acc:0.956250\n",
      "4_217 train_acc: 0.9924 train_loss: 0.025209\tval_acc: 0.988095 val_loss: 0.0978045 test_acc:0.946875\n",
      "4_230 train_acc: 0.9811 train_loss: 0.063804\tval_acc: 0.988095 val_loss: 0.0836950 test_acc:0.962500\n",
      "4_414 train_acc: 0.9848 train_loss: 0.030520\tval_acc: 0.988095 val_loss: 0.0813254 test_acc:0.981250\n",
      "4_432 train_acc: 1.0000 train_loss: 0.011864\tval_acc: 0.988095 val_loss: 0.0748184 test_acc:0.971875\n",
      "4_566 train_acc: 0.9886 train_loss: 0.023268\tval_acc: 0.988095 val_loss: 0.0684095 test_acc:0.978125\n",
      "4_708 train_acc: 0.9811 train_loss: 0.045677\tval_acc: 0.988095 val_loss: 0.0614104 test_acc:0.978125\n",
      "4_723 train_acc: 0.9924 train_loss: 0.024380\tval_acc: 0.988095 val_loss: 0.0483053 test_acc:0.984375\n",
      "4_980 train_acc: 0.9886 train_loss: 0.030244\tval_acc: 0.988095 val_loss: 0.0468838 test_acc:0.968750\n",
      "4_984 train_acc: 0.9697 train_loss: 0.113580\tval_acc: 0.988095 val_loss: 0.0443068 test_acc:0.968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  984 \tThe test accuracy is: 0.96875\n",
      " THE BEST ACCURACY IS 0.96875\tkappa is 0.9375\n",
      "subject 4 duration: 0:26:23.753517\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 4 fold: 4\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "4_0 train_acc: 0.5341 train_loss: 0.729927\tval_acc: 0.500000 val_loss: 0.7409879 test_acc:0.500000\n",
      "4_1 train_acc: 0.5189 train_loss: 0.748656\tval_acc: 0.500000 val_loss: 0.7363310 test_acc:0.500000\n",
      "4_2 train_acc: 0.5303 train_loss: 0.702019\tval_acc: 0.500000 val_loss: 0.7119554 test_acc:0.500000\n",
      "4_5 train_acc: 0.8068 train_loss: 0.453220\tval_acc: 0.761905 val_loss: 0.6239733 test_acc:0.843750\n",
      "4_7 train_acc: 0.8750 train_loss: 0.273490\tval_acc: 0.892857 val_loss: 0.2269846 test_acc:0.768750\n",
      "4_8 train_acc: 0.9318 train_loss: 0.200438\tval_acc: 0.928571 val_loss: 0.1803121 test_acc:0.812500\n",
      "4_11 train_acc: 0.9205 train_loss: 0.222794\tval_acc: 0.940476 val_loss: 0.1825447 test_acc:0.771875\n",
      "4_14 train_acc: 0.9470 train_loss: 0.153782\tval_acc: 0.952381 val_loss: 0.1290926 test_acc:0.806250\n",
      "4_15 train_acc: 0.9205 train_loss: 0.183845\tval_acc: 0.952381 val_loss: 0.1272280 test_acc:0.825000\n",
      "4_20 train_acc: 0.9167 train_loss: 0.186238\tval_acc: 0.964286 val_loss: 0.1013791 test_acc:0.818750\n",
      "4_27 train_acc: 0.9583 train_loss: 0.108290\tval_acc: 0.976190 val_loss: 0.0891940 test_acc:0.825000\n",
      "4_29 train_acc: 0.9470 train_loss: 0.171582\tval_acc: 0.976190 val_loss: 0.0691889 test_acc:0.846875\n",
      "4_33 train_acc: 0.9394 train_loss: 0.175983\tval_acc: 0.976190 val_loss: 0.0623809 test_acc:0.837500\n",
      "4_44 train_acc: 0.9811 train_loss: 0.086234\tval_acc: 0.976190 val_loss: 0.0453456 test_acc:0.850000\n",
      "4_45 train_acc: 0.9659 train_loss: 0.100606\tval_acc: 0.988095 val_loss: 0.0474061 test_acc:0.846875\n",
      "4_46 train_acc: 0.9508 train_loss: 0.126776\tval_acc: 0.988095 val_loss: 0.0465854 test_acc:0.853125\n",
      "4_51 train_acc: 0.9621 train_loss: 0.100127\tval_acc: 0.988095 val_loss: 0.0456096 test_acc:0.850000\n",
      "4_53 train_acc: 0.9659 train_loss: 0.072577\tval_acc: 0.988095 val_loss: 0.0440975 test_acc:0.862500\n",
      "4_58 train_acc: 0.9659 train_loss: 0.117422\tval_acc: 0.988095 val_loss: 0.0410161 test_acc:0.884375\n",
      "4_63 train_acc: 0.9886 train_loss: 0.056504\tval_acc: 0.988095 val_loss: 0.0378803 test_acc:0.868750\n",
      "4_66 train_acc: 0.9697 train_loss: 0.074871\tval_acc: 0.988095 val_loss: 0.0351425 test_acc:0.875000\n",
      "4_69 train_acc: 0.9773 train_loss: 0.065276\tval_acc: 0.988095 val_loss: 0.0343541 test_acc:0.890625\n",
      "4_80 train_acc: 0.9848 train_loss: 0.046277\tval_acc: 0.988095 val_loss: 0.0342801 test_acc:0.887500\n",
      "4_84 train_acc: 0.9735 train_loss: 0.100626\tval_acc: 0.988095 val_loss: 0.0331806 test_acc:0.893750\n",
      "4_106 train_acc: 0.9924 train_loss: 0.035985\tval_acc: 0.988095 val_loss: 0.0327096 test_acc:0.884375\n",
      "4_146 train_acc: 0.9659 train_loss: 0.075788\tval_acc: 0.988095 val_loss: 0.0310145 test_acc:0.915625\n",
      "4_343 train_acc: 0.9886 train_loss: 0.035640\tval_acc: 0.988095 val_loss: 0.0257660 test_acc:0.940625\n",
      "4_367 train_acc: 0.9811 train_loss: 0.042648\tval_acc: 1.000000 val_loss: 0.0190563 test_acc:0.965625\n",
      "4_402 train_acc: 0.9962 train_loss: 0.015981\tval_acc: 1.000000 val_loss: 0.0154601 test_acc:0.956250\n",
      "4_820 train_acc: 0.9924 train_loss: 0.049795\tval_acc: 1.000000 val_loss: 0.0143130 test_acc:0.975000\n",
      "4_895 train_acc: 0.9848 train_loss: 0.029096\tval_acc: 1.000000 val_loss: 0.0100555 test_acc:0.984375\n",
      "4_903 train_acc: 0.9962 train_loss: 0.018092\tval_acc: 1.000000 val_loss: 0.0082523 test_acc:0.981250\n",
      "4_905 train_acc: 0.9924 train_loss: 0.025078\tval_acc: 1.000000 val_loss: 0.0078530 test_acc:0.978125\n",
      "4_906 train_acc: 0.9924 train_loss: 0.027922\tval_acc: 1.000000 val_loss: 0.0056359 test_acc:0.981250\n",
      "epoch:  906 \tThe test accuracy is: 0.98125\n",
      " THE BEST ACCURACY IS 0.98125\tkappa is 0.9625\n",
      "subject 4 duration: 0:33:14.653507\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 4 fold: 5\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "4_0 train_acc: 0.4924 train_loss: 0.754051\tval_acc: 0.488095 val_loss: 0.7524940 test_acc:0.500000\n",
      "4_1 train_acc: 0.5758 train_loss: 0.685898\tval_acc: 0.500000 val_loss: 0.7400359 test_acc:0.500000\n",
      "4_2 train_acc: 0.6818 train_loss: 0.610333\tval_acc: 0.690476 val_loss: 0.5916378 test_acc:0.706250\n",
      "4_3 train_acc: 0.7765 train_loss: 0.515469\tval_acc: 0.738095 val_loss: 0.5751752 test_acc:0.865625\n",
      "4_5 train_acc: 0.8485 train_loss: 0.349197\tval_acc: 0.809524 val_loss: 0.5775832 test_acc:0.946875\n",
      "4_7 train_acc: 0.8788 train_loss: 0.284107\tval_acc: 0.845238 val_loss: 0.5113142 test_acc:0.931250\n",
      "4_9 train_acc: 0.9356 train_loss: 0.194876\tval_acc: 0.857143 val_loss: 0.4273703 test_acc:0.953125\n",
      "4_10 train_acc: 0.8674 train_loss: 0.341993\tval_acc: 0.880952 val_loss: 0.3899845 test_acc:0.956250\n",
      "4_11 train_acc: 0.9280 train_loss: 0.182801\tval_acc: 0.904762 val_loss: 0.3270737 test_acc:0.940625\n",
      "4_13 train_acc: 0.9697 train_loss: 0.126130\tval_acc: 0.928571 val_loss: 0.2980579 test_acc:0.906250\n",
      "4_24 train_acc: 0.9735 train_loss: 0.097273\tval_acc: 0.928571 val_loss: 0.2372121 test_acc:0.918750\n",
      "4_25 train_acc: 0.9545 train_loss: 0.128041\tval_acc: 0.928571 val_loss: 0.2252883 test_acc:0.918750\n",
      "4_32 train_acc: 0.9811 train_loss: 0.086387\tval_acc: 0.940476 val_loss: 0.2244276 test_acc:0.946875\n",
      "4_49 train_acc: 0.9773 train_loss: 0.074642\tval_acc: 0.940476 val_loss: 0.1918004 test_acc:0.934375\n",
      "4_51 train_acc: 0.9508 train_loss: 0.130548\tval_acc: 0.940476 val_loss: 0.1732735 test_acc:0.928125\n",
      "4_747 train_acc: 0.9924 train_loss: 0.018708\tval_acc: 0.940476 val_loss: 0.1661786 test_acc:0.978125\n",
      "4_755 train_acc: 0.9886 train_loss: 0.026411\tval_acc: 0.940476 val_loss: 0.1616678 test_acc:0.981250\n",
      "4_817 train_acc: 0.9848 train_loss: 0.044279\tval_acc: 0.940476 val_loss: 0.1308495 test_acc:0.975000\n",
      "4_818 train_acc: 0.9962 train_loss: 0.028625\tval_acc: 0.952381 val_loss: 0.1768893 test_acc:0.984375\n",
      "4_820 train_acc: 0.9848 train_loss: 0.035796\tval_acc: 0.952381 val_loss: 0.1724397 test_acc:0.981250\n",
      "4_840 train_acc: 0.9962 train_loss: 0.034958\tval_acc: 0.952381 val_loss: 0.1478540 test_acc:0.981250\n",
      "epoch:  840 \tThe test accuracy is: 0.98125\n",
      " THE BEST ACCURACY IS 0.98125\tkappa is 0.9625\n",
      "subject 4 duration: 0:40:01.033178\n",
      "4 subject 5 fold mean: \n",
      " accuray      97.687500\n",
      "precision    97.705734\n",
      "recall       97.687500\n",
      "f1           97.687271\n",
      "kappa        95.375000\n",
      "dtype: float64\n",
      "seed is 993\n",
      "Subject 5\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 5 fold: 1\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "5_0 train_acc: 0.4886 train_loss: 0.754353\tval_acc: 0.500000 val_loss: 0.7074289 test_acc:0.500000\n",
      "5_2 train_acc: 0.5038 train_loss: 0.740678\tval_acc: 0.500000 val_loss: 0.7006116 test_acc:0.496875\n",
      "5_5 train_acc: 0.5455 train_loss: 0.693099\tval_acc: 0.547619 val_loss: 0.6919637 test_acc:0.618750\n",
      "5_6 train_acc: 0.5417 train_loss: 0.698909\tval_acc: 0.559524 val_loss: 0.6908606 test_acc:0.668750\n",
      "5_7 train_acc: 0.4205 train_loss: 0.714963\tval_acc: 0.595238 val_loss: 0.6913786 test_acc:0.600000\n",
      "5_16 train_acc: 0.5038 train_loss: 0.694682\tval_acc: 0.654762 val_loss: 0.6833272 test_acc:0.781250\n",
      "5_18 train_acc: 0.5417 train_loss: 0.685955\tval_acc: 0.666667 val_loss: 0.6813170 test_acc:0.775000\n",
      "5_26 train_acc: 0.5417 train_loss: 0.683585\tval_acc: 0.714286 val_loss: 0.6570917 test_acc:0.884375\n",
      "5_31 train_acc: 0.5758 train_loss: 0.684648\tval_acc: 0.750000 val_loss: 0.6276540 test_acc:0.900000\n",
      "5_40 train_acc: 0.5985 train_loss: 0.659118\tval_acc: 0.785714 val_loss: 0.5407339 test_acc:0.934375\n",
      "5_52 train_acc: 0.7348 train_loss: 0.581270\tval_acc: 0.797619 val_loss: 0.4256917 test_acc:0.971875\n",
      "5_63 train_acc: 0.7614 train_loss: 0.514694\tval_acc: 0.797619 val_loss: 0.4193651 test_acc:0.971875\n",
      "5_66 train_acc: 0.7727 train_loss: 0.487153\tval_acc: 0.833333 val_loss: 0.3890680 test_acc:0.968750\n",
      "5_75 train_acc: 0.7614 train_loss: 0.520781\tval_acc: 0.857143 val_loss: 0.3705549 test_acc:0.971875\n",
      "5_93 train_acc: 0.7841 train_loss: 0.425177\tval_acc: 0.857143 val_loss: 0.3317437 test_acc:0.984375\n",
      "5_94 train_acc: 0.7917 train_loss: 0.427990\tval_acc: 0.869048 val_loss: 0.2995806 test_acc:0.984375\n",
      "5_100 train_acc: 0.7955 train_loss: 0.433854\tval_acc: 0.892857 val_loss: 0.2417759 test_acc:0.981250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_123 train_acc: 0.8561 train_loss: 0.324633\tval_acc: 0.892857 val_loss: 0.2318776 test_acc:0.987500\n",
      "5_131 train_acc: 0.8371 train_loss: 0.389686\tval_acc: 0.904762 val_loss: 0.2501477 test_acc:0.987500\n",
      "5_136 train_acc: 0.8636 train_loss: 0.330001\tval_acc: 0.904762 val_loss: 0.2414875 test_acc:0.984375\n",
      "5_139 train_acc: 0.8712 train_loss: 0.314029\tval_acc: 0.916667 val_loss: 0.2444827 test_acc:0.981250\n",
      "5_160 train_acc: 0.8939 train_loss: 0.245031\tval_acc: 0.916667 val_loss: 0.2339142 test_acc:0.993750\n",
      "5_162 train_acc: 0.8864 train_loss: 0.291151\tval_acc: 0.940476 val_loss: 0.2057809 test_acc:0.987500\n",
      "5_466 train_acc: 0.9242 train_loss: 0.175417\tval_acc: 0.940476 val_loss: 0.1666596 test_acc:0.981250\n",
      "5_744 train_acc: 0.9583 train_loss: 0.119452\tval_acc: 0.940476 val_loss: 0.1575481 test_acc:0.968750\n",
      "5_752 train_acc: 0.9470 train_loss: 0.122725\tval_acc: 0.940476 val_loss: 0.1573568 test_acc:0.981250\n",
      "5_766 train_acc: 0.9091 train_loss: 0.207574\tval_acc: 0.952381 val_loss: 0.2093232 test_acc:0.965625\n",
      "5_769 train_acc: 0.8826 train_loss: 0.270430\tval_acc: 0.952381 val_loss: 0.1985766 test_acc:0.975000\n",
      "5_811 train_acc: 0.9318 train_loss: 0.196188\tval_acc: 0.952381 val_loss: 0.1890815 test_acc:0.984375\n",
      "5_830 train_acc: 0.9356 train_loss: 0.193314\tval_acc: 0.952381 val_loss: 0.1795653 test_acc:0.981250\n",
      "5_940 train_acc: 0.9242 train_loss: 0.169782\tval_acc: 0.952381 val_loss: 0.1779890 test_acc:0.975000\n",
      "5_977 train_acc: 0.9432 train_loss: 0.147731\tval_acc: 0.952381 val_loss: 0.1762603 test_acc:0.990625\n",
      "epoch:  977 \tThe test accuracy is: 0.990625\n",
      " THE BEST ACCURACY IS 0.990625\tkappa is 0.98125\n",
      "subject 5 duration: 0:06:47.356614\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 5 fold: 2\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "5_0 train_acc: 0.4735 train_loss: 0.766623\tval_acc: 0.440476 val_loss: 0.6939154 test_acc:0.496875\n",
      "5_1 train_acc: 0.5189 train_loss: 0.721701\tval_acc: 0.476190 val_loss: 0.6951900 test_acc:0.540625\n",
      "5_4 train_acc: 0.5644 train_loss: 0.694573\tval_acc: 0.488095 val_loss: 0.6959383 test_acc:0.540625\n",
      "5_10 train_acc: 0.5303 train_loss: 0.698103\tval_acc: 0.500000 val_loss: 0.6968195 test_acc:0.665625\n",
      "5_13 train_acc: 0.4924 train_loss: 0.695026\tval_acc: 0.523810 val_loss: 0.6839858 test_acc:0.743750\n",
      "5_14 train_acc: 0.5606 train_loss: 0.660908\tval_acc: 0.559524 val_loss: 0.6695790 test_acc:0.725000\n",
      "5_17 train_acc: 0.5720 train_loss: 0.699880\tval_acc: 0.619048 val_loss: 0.6560908 test_acc:0.828125\n",
      "5_18 train_acc: 0.6439 train_loss: 0.665585\tval_acc: 0.690476 val_loss: 0.6200401 test_acc:0.815625\n",
      "5_29 train_acc: 0.7083 train_loss: 0.595900\tval_acc: 0.714286 val_loss: 0.5434117 test_acc:0.950000\n",
      "5_45 train_acc: 0.7348 train_loss: 0.542976\tval_acc: 0.738095 val_loss: 0.4310634 test_acc:0.850000\n",
      "5_48 train_acc: 0.7083 train_loss: 0.552082\tval_acc: 0.773810 val_loss: 0.4716750 test_acc:0.878125\n",
      "5_54 train_acc: 0.8144 train_loss: 0.450656\tval_acc: 0.785714 val_loss: 0.4344503 test_acc:0.868750\n",
      "5_65 train_acc: 0.8182 train_loss: 0.413286\tval_acc: 0.785714 val_loss: 0.3982621 test_acc:0.856250\n",
      "5_67 train_acc: 0.7841 train_loss: 0.420243\tval_acc: 0.797619 val_loss: 0.4519154 test_acc:0.831250\n",
      "5_80 train_acc: 0.7576 train_loss: 0.496990\tval_acc: 0.821429 val_loss: 0.4033019 test_acc:0.790625\n",
      "5_94 train_acc: 0.8788 train_loss: 0.312811\tval_acc: 0.821429 val_loss: 0.3587195 test_acc:0.840625\n",
      "5_99 train_acc: 0.8258 train_loss: 0.379982\tval_acc: 0.857143 val_loss: 0.3296134 test_acc:0.862500\n",
      "5_119 train_acc: 0.8030 train_loss: 0.446775\tval_acc: 0.869048 val_loss: 0.3323484 test_acc:0.878125\n",
      "5_126 train_acc: 0.8409 train_loss: 0.368533\tval_acc: 0.880952 val_loss: 0.2795705 test_acc:0.868750\n",
      "5_139 train_acc: 0.8674 train_loss: 0.321824\tval_acc: 0.880952 val_loss: 0.2697225 test_acc:0.931250\n",
      "5_153 train_acc: 0.8826 train_loss: 0.321588\tval_acc: 0.916667 val_loss: 0.2367755 test_acc:0.875000\n",
      "5_188 train_acc: 0.8674 train_loss: 0.320013\tval_acc: 0.916667 val_loss: 0.2044471 test_acc:0.921875\n",
      "5_190 train_acc: 0.8977 train_loss: 0.250949\tval_acc: 0.916667 val_loss: 0.1635848 test_acc:0.893750\n",
      "5_225 train_acc: 0.9053 train_loss: 0.213449\tval_acc: 0.928571 val_loss: 0.1841496 test_acc:0.943750\n",
      "5_233 train_acc: 0.9205 train_loss: 0.204199\tval_acc: 0.928571 val_loss: 0.1807073 test_acc:0.928125\n",
      "5_240 train_acc: 0.8902 train_loss: 0.306366\tval_acc: 0.928571 val_loss: 0.1650040 test_acc:0.925000\n",
      "5_263 train_acc: 0.8788 train_loss: 0.301160\tval_acc: 0.940476 val_loss: 0.1788491 test_acc:0.940625\n",
      "5_268 train_acc: 0.9205 train_loss: 0.223373\tval_acc: 0.964286 val_loss: 0.1256700 test_acc:0.912500\n",
      "5_411 train_acc: 0.9129 train_loss: 0.255362\tval_acc: 0.976190 val_loss: 0.1216241 test_acc:0.909375\n",
      "5_434 train_acc: 0.9091 train_loss: 0.225955\tval_acc: 0.976190 val_loss: 0.1200675 test_acc:0.918750\n",
      "epoch:  434 \tThe test accuracy is: 0.91875\n",
      " THE BEST ACCURACY IS 0.91875\tkappa is 0.8375\n",
      "subject 5 duration: 0:13:34.468136\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 5 fold: 3\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "5_0 train_acc: 0.4470 train_loss: 0.800827\tval_acc: 0.511905 val_loss: 0.7019124 test_acc:0.500000\n",
      "5_3 train_acc: 0.4886 train_loss: 0.699503\tval_acc: 0.535714 val_loss: 0.6935328 test_acc:0.671875\n",
      "5_7 train_acc: 0.5076 train_loss: 0.706946\tval_acc: 0.571429 val_loss: 0.6904837 test_acc:0.828125\n",
      "5_12 train_acc: 0.5568 train_loss: 0.688697\tval_acc: 0.571429 val_loss: 0.6825047 test_acc:0.840625\n",
      "5_13 train_acc: 0.5644 train_loss: 0.690672\tval_acc: 0.571429 val_loss: 0.6773635 test_acc:0.771875\n",
      "5_18 train_acc: 0.5795 train_loss: 0.672834\tval_acc: 0.583333 val_loss: 0.6568215 test_acc:0.846875\n",
      "5_22 train_acc: 0.6326 train_loss: 0.635490\tval_acc: 0.630952 val_loss: 0.6390681 test_acc:0.784375\n",
      "5_28 train_acc: 0.6553 train_loss: 0.604696\tval_acc: 0.630952 val_loss: 0.6209882 test_acc:0.784375\n",
      "5_29 train_acc: 0.6061 train_loss: 0.687319\tval_acc: 0.654762 val_loss: 0.6381980 test_acc:0.700000\n",
      "5_31 train_acc: 0.6629 train_loss: 0.623460\tval_acc: 0.666667 val_loss: 0.6230130 test_acc:0.715625\n",
      "5_33 train_acc: 0.7348 train_loss: 0.562637\tval_acc: 0.702381 val_loss: 0.5947265 test_acc:0.909375\n",
      "5_37 train_acc: 0.7576 train_loss: 0.507737\tval_acc: 0.738095 val_loss: 0.5306116 test_acc:0.837500\n",
      "5_44 train_acc: 0.7538 train_loss: 0.530524\tval_acc: 0.738095 val_loss: 0.5135057 test_acc:0.784375\n",
      "5_45 train_acc: 0.6553 train_loss: 0.667336\tval_acc: 0.750000 val_loss: 0.4887875 test_acc:0.728125\n",
      "5_46 train_acc: 0.7538 train_loss: 0.507002\tval_acc: 0.773810 val_loss: 0.5197317 test_acc:0.725000\n",
      "5_47 train_acc: 0.8144 train_loss: 0.456873\tval_acc: 0.809524 val_loss: 0.4643987 test_acc:0.809375\n",
      "5_64 train_acc: 0.8030 train_loss: 0.417128\tval_acc: 0.821429 val_loss: 0.3796690 test_acc:0.793750\n",
      "5_69 train_acc: 0.7386 train_loss: 0.526313\tval_acc: 0.833333 val_loss: 0.3946199 test_acc:0.796875\n",
      "5_73 train_acc: 0.7803 train_loss: 0.457052\tval_acc: 0.845238 val_loss: 0.3742079 test_acc:0.821875\n",
      "5_89 train_acc: 0.7803 train_loss: 0.449985\tval_acc: 0.857143 val_loss: 0.3346832 test_acc:0.803125\n",
      "5_101 train_acc: 0.8523 train_loss: 0.365448\tval_acc: 0.869048 val_loss: 0.3415337 test_acc:0.871875\n",
      "5_112 train_acc: 0.8295 train_loss: 0.379118\tval_acc: 0.869048 val_loss: 0.2900485 test_acc:0.856250\n",
      "5_138 train_acc: 0.8333 train_loss: 0.381670\tval_acc: 0.880952 val_loss: 0.3369366 test_acc:0.896875\n",
      "5_141 train_acc: 0.8295 train_loss: 0.350654\tval_acc: 0.904762 val_loss: 0.2593327 test_acc:0.893750\n",
      "5_157 train_acc: 0.8523 train_loss: 0.341817\tval_acc: 0.904762 val_loss: 0.2312977 test_acc:0.928125\n",
      "5_158 train_acc: 0.8788 train_loss: 0.295838\tval_acc: 0.916667 val_loss: 0.2508376 test_acc:0.909375\n",
      "5_210 train_acc: 0.9167 train_loss: 0.218243\tval_acc: 0.916667 val_loss: 0.2432740 test_acc:0.950000\n",
      "5_213 train_acc: 0.8674 train_loss: 0.301059\tval_acc: 0.916667 val_loss: 0.2431868 test_acc:0.925000\n",
      "5_216 train_acc: 0.9129 train_loss: 0.215185\tval_acc: 0.928571 val_loss: 0.2570944 test_acc:0.962500\n",
      "5_223 train_acc: 0.8636 train_loss: 0.278693\tval_acc: 0.928571 val_loss: 0.2127820 test_acc:0.921875\n",
      "5_414 train_acc: 0.9205 train_loss: 0.182517\tval_acc: 0.940476 val_loss: 0.2070216 test_acc:0.925000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_539 train_acc: 0.9205 train_loss: 0.219909\tval_acc: 0.940476 val_loss: 0.1786373 test_acc:0.925000\n",
      "5_698 train_acc: 0.9394 train_loss: 0.150688\tval_acc: 0.952381 val_loss: 0.2002592 test_acc:0.959375\n",
      "epoch:  698 \tThe test accuracy is: 0.959375\n",
      " THE BEST ACCURACY IS 0.959375\tkappa is 0.91875\n",
      "subject 5 duration: 0:20:18.843883\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 5 fold: 4\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "5_0 train_acc: 0.5000 train_loss: 0.766244\tval_acc: 0.511905 val_loss: 0.7000568 test_acc:0.500000\n",
      "5_1 train_acc: 0.5189 train_loss: 0.734119\tval_acc: 0.523810 val_loss: 0.6907372 test_acc:0.500000\n",
      "5_2 train_acc: 0.5455 train_loss: 0.697166\tval_acc: 0.583333 val_loss: 0.6794089 test_acc:0.684375\n",
      "5_3 train_acc: 0.4735 train_loss: 0.740458\tval_acc: 0.595238 val_loss: 0.6811785 test_acc:0.621875\n",
      "5_4 train_acc: 0.5265 train_loss: 0.711941\tval_acc: 0.619048 val_loss: 0.6809248 test_acc:0.671875\n",
      "5_6 train_acc: 0.4356 train_loss: 0.737253\tval_acc: 0.630952 val_loss: 0.6663308 test_acc:0.700000\n",
      "5_13 train_acc: 0.5985 train_loss: 0.672010\tval_acc: 0.630952 val_loss: 0.6436648 test_acc:0.681250\n",
      "5_14 train_acc: 0.5871 train_loss: 0.657996\tval_acc: 0.666667 val_loss: 0.6305283 test_acc:0.909375\n",
      "5_15 train_acc: 0.5871 train_loss: 0.658606\tval_acc: 0.678571 val_loss: 0.6095080 test_acc:0.784375\n",
      "5_19 train_acc: 0.5720 train_loss: 0.675246\tval_acc: 0.738095 val_loss: 0.5777481 test_acc:0.856250\n",
      "5_21 train_acc: 0.6364 train_loss: 0.661495\tval_acc: 0.773810 val_loss: 0.5657743 test_acc:0.778125\n",
      "5_22 train_acc: 0.6402 train_loss: 0.628819\tval_acc: 0.797619 val_loss: 0.5353397 test_acc:0.831250\n",
      "5_82 train_acc: 0.7879 train_loss: 0.447728\tval_acc: 0.797619 val_loss: 0.5008497 test_acc:0.768750\n",
      "5_101 train_acc: 0.8598 train_loss: 0.345761\tval_acc: 0.869048 val_loss: 0.3917568 test_acc:0.768750\n",
      "5_110 train_acc: 0.8068 train_loss: 0.411903\tval_acc: 0.869048 val_loss: 0.3648122 test_acc:0.809375\n",
      "5_125 train_acc: 0.8144 train_loss: 0.412989\tval_acc: 0.880952 val_loss: 0.3387437 test_acc:0.787500\n",
      "5_138 train_acc: 0.8826 train_loss: 0.323503\tval_acc: 0.880952 val_loss: 0.3084933 test_acc:0.871875\n",
      "5_145 train_acc: 0.8561 train_loss: 0.370119\tval_acc: 0.904762 val_loss: 0.2956024 test_acc:0.871875\n",
      "5_207 train_acc: 0.9318 train_loss: 0.180712\tval_acc: 0.916667 val_loss: 0.2353068 test_acc:0.918750\n",
      "5_247 train_acc: 0.8826 train_loss: 0.234675\tval_acc: 0.916667 val_loss: 0.2351168 test_acc:0.906250\n",
      "5_260 train_acc: 0.8750 train_loss: 0.266253\tval_acc: 0.928571 val_loss: 0.2054274 test_acc:0.934375\n",
      "5_373 train_acc: 0.9015 train_loss: 0.253410\tval_acc: 0.928571 val_loss: 0.1990687 test_acc:0.937500\n",
      "5_399 train_acc: 0.9280 train_loss: 0.227287\tval_acc: 0.940476 val_loss: 0.2153890 test_acc:0.956250\n",
      "5_451 train_acc: 0.9015 train_loss: 0.258055\tval_acc: 0.940476 val_loss: 0.1966222 test_acc:0.962500\n",
      "5_465 train_acc: 0.9167 train_loss: 0.219230\tval_acc: 0.940476 val_loss: 0.1924641 test_acc:0.975000\n",
      "5_562 train_acc: 0.9280 train_loss: 0.178426\tval_acc: 0.940476 val_loss: 0.1647433 test_acc:0.950000\n",
      "5_607 train_acc: 0.9508 train_loss: 0.182365\tval_acc: 0.952381 val_loss: 0.1812504 test_acc:0.946875\n",
      "5_803 train_acc: 0.9583 train_loss: 0.107228\tval_acc: 0.952381 val_loss: 0.1690543 test_acc:0.962500\n",
      "5_976 train_acc: 0.9205 train_loss: 0.193232\tval_acc: 0.952381 val_loss: 0.1494206 test_acc:0.978125\n",
      "5_990 train_acc: 0.9356 train_loss: 0.190120\tval_acc: 0.952381 val_loss: 0.1409308 test_acc:0.996875\n",
      "epoch:  990 \tThe test accuracy is: 0.996875\n",
      " THE BEST ACCURACY IS 0.996875\tkappa is 0.99375\n",
      "subject 5 duration: 0:27:04.778909\n",
      "-------------------- raw train size： (420, 1, 3, 1000) test size： (320, 3, 1000) subject: 5 fold: 5\n",
      "-------------------- train size： (336, 1, 3, 1000) val size： (84, 1, 3, 1000)\n",
      "5_0 train_acc: 0.4659 train_loss: 0.782787\tval_acc: 0.488095 val_loss: 0.6920825 test_acc:0.537500\n",
      "5_1 train_acc: 0.4811 train_loss: 0.734376\tval_acc: 0.511905 val_loss: 0.6919696 test_acc:0.500000\n",
      "5_2 train_acc: 0.5227 train_loss: 0.686296\tval_acc: 0.511905 val_loss: 0.6908771 test_acc:0.500000\n",
      "5_3 train_acc: 0.4962 train_loss: 0.725264\tval_acc: 0.523810 val_loss: 0.6900468 test_acc:0.603125\n",
      "5_5 train_acc: 0.5455 train_loss: 0.690436\tval_acc: 0.535714 val_loss: 0.6836582 test_acc:0.612500\n",
      "5_6 train_acc: 0.5303 train_loss: 0.697572\tval_acc: 0.583333 val_loss: 0.6827614 test_acc:0.659375\n",
      "5_7 train_acc: 0.5682 train_loss: 0.687870\tval_acc: 0.583333 val_loss: 0.6807019 test_acc:0.734375\n",
      "5_8 train_acc: 0.5038 train_loss: 0.694868\tval_acc: 0.607143 val_loss: 0.6785572 test_acc:0.825000\n",
      "5_9 train_acc: 0.5189 train_loss: 0.689924\tval_acc: 0.619048 val_loss: 0.6775275 test_acc:0.771875\n",
      "5_10 train_acc: 0.5076 train_loss: 0.704929\tval_acc: 0.630952 val_loss: 0.6765304 test_acc:0.856250\n",
      "5_12 train_acc: 0.4962 train_loss: 0.699641\tval_acc: 0.630952 val_loss: 0.6703435 test_acc:0.856250\n",
      "5_13 train_acc: 0.5303 train_loss: 0.691840\tval_acc: 0.654762 val_loss: 0.6647907 test_acc:0.790625\n",
      "5_14 train_acc: 0.5833 train_loss: 0.660757\tval_acc: 0.654762 val_loss: 0.6508470 test_acc:0.862500\n",
      "5_16 train_acc: 0.6098 train_loss: 0.675658\tval_acc: 0.666667 val_loss: 0.6380308 test_acc:0.659375\n",
      "5_17 train_acc: 0.5682 train_loss: 0.687301\tval_acc: 0.678571 val_loss: 0.6193892 test_acc:0.843750\n",
      "5_19 train_acc: 0.5947 train_loss: 0.665216\tval_acc: 0.714286 val_loss: 0.6174838 test_acc:0.868750\n",
      "5_24 train_acc: 0.5682 train_loss: 0.675214\tval_acc: 0.726190 val_loss: 0.5938882 test_acc:0.828125\n",
      "5_25 train_acc: 0.6629 train_loss: 0.616235\tval_acc: 0.738095 val_loss: 0.5717233 test_acc:0.909375\n",
      "5_28 train_acc: 0.6439 train_loss: 0.627044\tval_acc: 0.785714 val_loss: 0.5282918 test_acc:0.921875\n",
      "5_34 train_acc: 0.7008 train_loss: 0.578784\tval_acc: 0.833333 val_loss: 0.4893064 test_acc:0.925000\n",
      "5_60 train_acc: 0.8068 train_loss: 0.459983\tval_acc: 0.857143 val_loss: 0.3448408 test_acc:0.928125\n",
      "5_71 train_acc: 0.8144 train_loss: 0.420195\tval_acc: 0.869048 val_loss: 0.2944634 test_acc:0.912500\n",
      "5_79 train_acc: 0.7955 train_loss: 0.437130\tval_acc: 0.880952 val_loss: 0.3105842 test_acc:0.887500\n",
      "5_82 train_acc: 0.7727 train_loss: 0.448304\tval_acc: 0.904762 val_loss: 0.2912667 test_acc:0.887500\n",
      "5_96 train_acc: 0.8371 train_loss: 0.358890\tval_acc: 0.904762 val_loss: 0.2604405 test_acc:0.893750\n",
      "5_102 train_acc: 0.8523 train_loss: 0.331555\tval_acc: 0.916667 val_loss: 0.2454685 test_acc:0.925000\n",
      "5_215 train_acc: 0.8826 train_loss: 0.273651\tval_acc: 0.916667 val_loss: 0.2190181 test_acc:0.940625\n",
      "5_331 train_acc: 0.9053 train_loss: 0.225936\tval_acc: 0.928571 val_loss: 0.2045606 test_acc:0.971875\n",
      "5_519 train_acc: 0.9432 train_loss: 0.176860\tval_acc: 0.940476 val_loss: 0.2383617 test_acc:0.981250\n",
      "5_635 train_acc: 0.9242 train_loss: 0.177903\tval_acc: 0.940476 val_loss: 0.1796386 test_acc:0.971875\n",
      "5_722 train_acc: 0.9318 train_loss: 0.220969\tval_acc: 0.952381 val_loss: 0.1906141 test_acc:0.987500\n",
      "5_982 train_acc: 0.9205 train_loss: 0.188570\tval_acc: 0.952381 val_loss: 0.1858789 test_acc:0.975000\n",
      "epoch:  982 \tThe test accuracy is: 0.975\n",
      " THE BEST ACCURACY IS 0.975\tkappa is 0.95\n",
      "subject 5 duration: 0:33:51.289026\n",
      "5 subject 5 fold mean: \n",
      " accuray      96.812500\n",
      "precision    97.128354\n",
      "recall       96.812500\n",
      "f1           96.800028\n",
      "kappa        93.625000\n",
      "dtype: float64\n",
      "seed is 448\n",
      "Subject 6\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 6 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "6_0 train_acc: 0.4677 train_loss: 0.777696\tval_acc: 0.500000 val_loss: 0.6987935 test_acc:0.500000\n",
      "6_2 train_acc: 0.5565 train_loss: 0.715600\tval_acc: 0.500000 val_loss: 0.6917254 test_acc:0.503125\n",
      "6_3 train_acc: 0.5121 train_loss: 0.718088\tval_acc: 0.500000 val_loss: 0.6903697 test_acc:0.506250\n",
      "6_4 train_acc: 0.5121 train_loss: 0.697347\tval_acc: 0.600000 val_loss: 0.6891184 test_acc:0.506250\n",
      "6_7 train_acc: 0.5524 train_loss: 0.694779\tval_acc: 0.637500 val_loss: 0.6805234 test_acc:0.556250\n",
      "6_9 train_acc: 0.5403 train_loss: 0.680579\tval_acc: 0.712500 val_loss: 0.6644154 test_acc:0.581250\n",
      "6_10 train_acc: 0.5605 train_loss: 0.692019\tval_acc: 0.737500 val_loss: 0.6533922 test_acc:0.596875\n",
      "6_12 train_acc: 0.5766 train_loss: 0.679761\tval_acc: 0.737500 val_loss: 0.6091068 test_acc:0.634375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_17 train_acc: 0.7137 train_loss: 0.579358\tval_acc: 0.750000 val_loss: 0.5147780 test_acc:0.750000\n",
      "6_19 train_acc: 0.7661 train_loss: 0.516005\tval_acc: 0.762500 val_loss: 0.5029281 test_acc:0.784375\n",
      "6_21 train_acc: 0.7661 train_loss: 0.481603\tval_acc: 0.800000 val_loss: 0.4548945 test_acc:0.784375\n",
      "6_22 train_acc: 0.7379 train_loss: 0.524080\tval_acc: 0.800000 val_loss: 0.3849216 test_acc:0.818750\n",
      "6_23 train_acc: 0.7460 train_loss: 0.517063\tval_acc: 0.837500 val_loss: 0.4318218 test_acc:0.800000\n",
      "6_24 train_acc: 0.7702 train_loss: 0.509250\tval_acc: 0.875000 val_loss: 0.3399148 test_acc:0.850000\n",
      "6_26 train_acc: 0.8710 train_loss: 0.328195\tval_acc: 0.875000 val_loss: 0.3222870 test_acc:0.843750\n",
      "6_40 train_acc: 0.8548 train_loss: 0.363826\tval_acc: 0.912500 val_loss: 0.2796374 test_acc:0.837500\n",
      "6_75 train_acc: 0.8548 train_loss: 0.334173\tval_acc: 0.912500 val_loss: 0.2462497 test_acc:0.868750\n",
      "6_82 train_acc: 0.8710 train_loss: 0.302166\tval_acc: 0.925000 val_loss: 0.2488469 test_acc:0.865625\n",
      "6_203 train_acc: 0.8831 train_loss: 0.314564\tval_acc: 0.937500 val_loss: 0.2693565 test_acc:0.846875\n",
      "6_218 train_acc: 0.8629 train_loss: 0.293687\tval_acc: 0.950000 val_loss: 0.2687314 test_acc:0.875000\n",
      "6_260 train_acc: 0.9234 train_loss: 0.193995\tval_acc: 0.950000 val_loss: 0.2654337 test_acc:0.840625\n",
      "6_507 train_acc: 0.9073 train_loss: 0.214738\tval_acc: 0.950000 val_loss: 0.2427045 test_acc:0.834375\n",
      "6_511 train_acc: 0.9194 train_loss: 0.209344\tval_acc: 0.950000 val_loss: 0.1762909 test_acc:0.875000\n",
      "6_682 train_acc: 0.9073 train_loss: 0.222255\tval_acc: 0.962500 val_loss: 0.1863166 test_acc:0.859375\n",
      "6_732 train_acc: 0.9556 train_loss: 0.173468\tval_acc: 0.962500 val_loss: 0.1594018 test_acc:0.887500\n",
      "6_945 train_acc: 0.9153 train_loss: 0.216022\tval_acc: 0.962500 val_loss: 0.1557129 test_acc:0.893750\n",
      "epoch:  945 \tThe test accuracy is: 0.89375\n",
      " THE BEST ACCURACY IS 0.89375\tkappa is 0.7875\n",
      "subject 6 duration: 0:06:45.881972\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 6 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "6_0 train_acc: 0.5121 train_loss: 0.753631\tval_acc: 0.500000 val_loss: 0.7449082 test_acc:0.500000\n",
      "6_1 train_acc: 0.5202 train_loss: 0.728205\tval_acc: 0.500000 val_loss: 0.7308705 test_acc:0.500000\n",
      "6_2 train_acc: 0.5040 train_loss: 0.723899\tval_acc: 0.512500 val_loss: 0.6960948 test_acc:0.500000\n",
      "6_4 train_acc: 0.5040 train_loss: 0.716585\tval_acc: 0.512500 val_loss: 0.6903526 test_acc:0.500000\n",
      "6_5 train_acc: 0.5323 train_loss: 0.727006\tval_acc: 0.525000 val_loss: 0.6886440 test_acc:0.512500\n",
      "6_6 train_acc: 0.5887 train_loss: 0.685247\tval_acc: 0.575000 val_loss: 0.6871492 test_acc:0.518750\n",
      "6_8 train_acc: 0.5161 train_loss: 0.702862\tval_acc: 0.600000 val_loss: 0.6835397 test_acc:0.565625\n",
      "6_13 train_acc: 0.5000 train_loss: 0.697095\tval_acc: 0.600000 val_loss: 0.6682452 test_acc:0.606250\n",
      "6_14 train_acc: 0.6331 train_loss: 0.646972\tval_acc: 0.600000 val_loss: 0.6579583 test_acc:0.593750\n",
      "6_16 train_acc: 0.6048 train_loss: 0.687488\tval_acc: 0.650000 val_loss: 0.6354783 test_acc:0.584375\n",
      "6_17 train_acc: 0.5685 train_loss: 0.682680\tval_acc: 0.687500 val_loss: 0.6053452 test_acc:0.600000\n",
      "6_20 train_acc: 0.6653 train_loss: 0.606322\tval_acc: 0.700000 val_loss: 0.5394289 test_acc:0.693750\n",
      "6_21 train_acc: 0.7258 train_loss: 0.558375\tval_acc: 0.737500 val_loss: 0.5058051 test_acc:0.756250\n",
      "6_23 train_acc: 0.7742 train_loss: 0.512083\tval_acc: 0.775000 val_loss: 0.4945658 test_acc:0.778125\n",
      "6_25 train_acc: 0.7056 train_loss: 0.556357\tval_acc: 0.775000 val_loss: 0.4518677 test_acc:0.775000\n",
      "6_26 train_acc: 0.7863 train_loss: 0.472849\tval_acc: 0.800000 val_loss: 0.4177343 test_acc:0.784375\n",
      "6_27 train_acc: 0.7621 train_loss: 0.542165\tval_acc: 0.850000 val_loss: 0.4191447 test_acc:0.818750\n",
      "6_31 train_acc: 0.7581 train_loss: 0.509288\tval_acc: 0.862500 val_loss: 0.3434783 test_acc:0.809375\n",
      "6_32 train_acc: 0.8105 train_loss: 0.447247\tval_acc: 0.862500 val_loss: 0.3404004 test_acc:0.821875\n",
      "6_33 train_acc: 0.7903 train_loss: 0.494039\tval_acc: 0.875000 val_loss: 0.3616947 test_acc:0.815625\n",
      "6_34 train_acc: 0.7500 train_loss: 0.529859\tval_acc: 0.900000 val_loss: 0.3161875 test_acc:0.846875\n",
      "6_43 train_acc: 0.8266 train_loss: 0.423283\tval_acc: 0.900000 val_loss: 0.2647291 test_acc:0.853125\n",
      "6_46 train_acc: 0.8427 train_loss: 0.362532\tval_acc: 0.925000 val_loss: 0.2457967 test_acc:0.843750\n",
      "6_55 train_acc: 0.8427 train_loss: 0.358447\tval_acc: 0.937500 val_loss: 0.2110059 test_acc:0.834375\n",
      "6_59 train_acc: 0.8185 train_loss: 0.399811\tval_acc: 0.937500 val_loss: 0.2017561 test_acc:0.846875\n",
      "6_60 train_acc: 0.8347 train_loss: 0.382702\tval_acc: 0.937500 val_loss: 0.1938137 test_acc:0.853125\n",
      "6_87 train_acc: 0.8669 train_loss: 0.311706\tval_acc: 0.937500 val_loss: 0.1931911 test_acc:0.846875\n",
      "6_97 train_acc: 0.9073 train_loss: 0.260533\tval_acc: 0.950000 val_loss: 0.2080942 test_acc:0.871875\n",
      "6_100 train_acc: 0.8871 train_loss: 0.278842\tval_acc: 0.950000 val_loss: 0.2076838 test_acc:0.875000\n",
      "6_112 train_acc: 0.8548 train_loss: 0.350705\tval_acc: 0.950000 val_loss: 0.1812514 test_acc:0.856250\n",
      "6_118 train_acc: 0.8911 train_loss: 0.280193\tval_acc: 0.950000 val_loss: 0.1754350 test_acc:0.853125\n",
      "6_215 train_acc: 0.9032 train_loss: 0.246837\tval_acc: 0.950000 val_loss: 0.1753363 test_acc:0.865625\n",
      "6_241 train_acc: 0.8750 train_loss: 0.284969\tval_acc: 0.950000 val_loss: 0.1632025 test_acc:0.878125\n",
      "6_263 train_acc: 0.8992 train_loss: 0.231821\tval_acc: 0.950000 val_loss: 0.1601345 test_acc:0.878125\n",
      "6_290 train_acc: 0.9153 train_loss: 0.226786\tval_acc: 0.950000 val_loss: 0.1583261 test_acc:0.865625\n",
      "6_293 train_acc: 0.9194 train_loss: 0.224909\tval_acc: 0.950000 val_loss: 0.1554191 test_acc:0.865625\n",
      "6_296 train_acc: 0.9355 train_loss: 0.183532\tval_acc: 0.950000 val_loss: 0.1506923 test_acc:0.871875\n",
      "6_364 train_acc: 0.9234 train_loss: 0.210217\tval_acc: 0.950000 val_loss: 0.1459976 test_acc:0.859375\n",
      "6_425 train_acc: 0.9355 train_loss: 0.175967\tval_acc: 0.962500 val_loss: 0.1586860 test_acc:0.865625\n",
      "6_638 train_acc: 0.9194 train_loss: 0.201384\tval_acc: 0.962500 val_loss: 0.1277908 test_acc:0.862500\n",
      "6_715 train_acc: 0.9073 train_loss: 0.192139\tval_acc: 0.962500 val_loss: 0.1199881 test_acc:0.868750\n",
      "6_844 train_acc: 0.9315 train_loss: 0.217152\tval_acc: 0.962500 val_loss: 0.1179163 test_acc:0.868750\n",
      "epoch:  844 \tThe test accuracy is: 0.86875\n",
      " THE BEST ACCURACY IS 0.86875\tkappa is 0.7375\n",
      "subject 6 duration: 0:13:33.970373\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 6 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "6_0 train_acc: 0.5161 train_loss: 0.747390\tval_acc: 0.487500 val_loss: 0.6978456 test_acc:0.500000\n",
      "6_1 train_acc: 0.4516 train_loss: 0.761798\tval_acc: 0.500000 val_loss: 0.6962221 test_acc:0.500000\n",
      "6_3 train_acc: 0.5484 train_loss: 0.699222\tval_acc: 0.500000 val_loss: 0.6921234 test_acc:0.518750\n",
      "6_4 train_acc: 0.4839 train_loss: 0.718081\tval_acc: 0.575000 val_loss: 0.6918832 test_acc:0.484375\n",
      "6_15 train_acc: 0.5726 train_loss: 0.675361\tval_acc: 0.600000 val_loss: 0.6792381 test_acc:0.556250\n",
      "6_19 train_acc: 0.6371 train_loss: 0.626609\tval_acc: 0.637500 val_loss: 0.6493847 test_acc:0.634375\n",
      "6_20 train_acc: 0.6290 train_loss: 0.637083\tval_acc: 0.687500 val_loss: 0.5867339 test_acc:0.796875\n",
      "6_23 train_acc: 0.7218 train_loss: 0.565830\tval_acc: 0.712500 val_loss: 0.5662180 test_acc:0.703125\n",
      "6_28 train_acc: 0.7460 train_loss: 0.495152\tval_acc: 0.725000 val_loss: 0.5362946 test_acc:0.737500\n",
      "6_33 train_acc: 0.7863 train_loss: 0.483572\tval_acc: 0.725000 val_loss: 0.5236328 test_acc:0.765625\n",
      "6_36 train_acc: 0.7419 train_loss: 0.537627\tval_acc: 0.762500 val_loss: 0.4755569 test_acc:0.815625\n",
      "6_40 train_acc: 0.7581 train_loss: 0.489781\tval_acc: 0.775000 val_loss: 0.5017537 test_acc:0.706250\n",
      "6_42 train_acc: 0.8266 train_loss: 0.426081\tval_acc: 0.800000 val_loss: 0.4512932 test_acc:0.781250\n",
      "6_43 train_acc: 0.7661 train_loss: 0.472545\tval_acc: 0.812500 val_loss: 0.4551405 test_acc:0.834375\n",
      "6_44 train_acc: 0.8065 train_loss: 0.412345\tval_acc: 0.812500 val_loss: 0.4412321 test_acc:0.831250\n",
      "6_48 train_acc: 0.7863 train_loss: 0.461118\tval_acc: 0.812500 val_loss: 0.4236391 test_acc:0.843750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_50 train_acc: 0.7984 train_loss: 0.457573\tval_acc: 0.812500 val_loss: 0.3867066 test_acc:0.871875\n",
      "6_51 train_acc: 0.8024 train_loss: 0.435206\tval_acc: 0.825000 val_loss: 0.3851470 test_acc:0.796875\n",
      "6_56 train_acc: 0.8548 train_loss: 0.381751\tval_acc: 0.837500 val_loss: 0.3968234 test_acc:0.846875\n",
      "6_62 train_acc: 0.7823 train_loss: 0.460117\tval_acc: 0.862500 val_loss: 0.3799202 test_acc:0.890625\n",
      "6_71 train_acc: 0.8508 train_loss: 0.322710\tval_acc: 0.887500 val_loss: 0.3736117 test_acc:0.909375\n",
      "6_90 train_acc: 0.8871 train_loss: 0.293846\tval_acc: 0.900000 val_loss: 0.3546025 test_acc:0.896875\n",
      "6_125 train_acc: 0.8508 train_loss: 0.307904\tval_acc: 0.900000 val_loss: 0.2877376 test_acc:0.893750\n",
      "6_167 train_acc: 0.8468 train_loss: 0.319771\tval_acc: 0.900000 val_loss: 0.2867590 test_acc:0.893750\n",
      "6_194 train_acc: 0.8831 train_loss: 0.273394\tval_acc: 0.912500 val_loss: 0.2639467 test_acc:0.878125\n",
      "6_315 train_acc: 0.9234 train_loss: 0.185661\tval_acc: 0.925000 val_loss: 0.2817964 test_acc:0.884375\n",
      "6_944 train_acc: 0.9153 train_loss: 0.228341\tval_acc: 0.925000 val_loss: 0.2717742 test_acc:0.881250\n",
      "epoch:  944 \tThe test accuracy is: 0.88125\n",
      " THE BEST ACCURACY IS 0.88125\tkappa is 0.7625\n",
      "subject 6 duration: 0:20:22.064789\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 6 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "6_0 train_acc: 0.4798 train_loss: 0.744838\tval_acc: 0.550000 val_loss: 0.6913303 test_acc:0.500000\n",
      "6_5 train_acc: 0.4597 train_loss: 0.720051\tval_acc: 0.600000 val_loss: 0.6862341 test_acc:0.625000\n",
      "6_7 train_acc: 0.5363 train_loss: 0.681821\tval_acc: 0.600000 val_loss: 0.6816636 test_acc:0.606250\n",
      "6_8 train_acc: 0.5605 train_loss: 0.696508\tval_acc: 0.625000 val_loss: 0.6798868 test_acc:0.587500\n",
      "6_9 train_acc: 0.5484 train_loss: 0.692569\tval_acc: 0.675000 val_loss: 0.6682441 test_acc:0.690625\n",
      "6_11 train_acc: 0.5484 train_loss: 0.701898\tval_acc: 0.675000 val_loss: 0.6431946 test_acc:0.675000\n",
      "6_16 train_acc: 0.6573 train_loss: 0.657810\tval_acc: 0.762500 val_loss: 0.5014268 test_acc:0.846875\n",
      "6_17 train_acc: 0.7097 train_loss: 0.554821\tval_acc: 0.775000 val_loss: 0.4908524 test_acc:0.840625\n",
      "6_18 train_acc: 0.6774 train_loss: 0.594611\tval_acc: 0.775000 val_loss: 0.4807622 test_acc:0.840625\n",
      "6_19 train_acc: 0.7540 train_loss: 0.496696\tval_acc: 0.800000 val_loss: 0.4523895 test_acc:0.865625\n",
      "6_22 train_acc: 0.7581 train_loss: 0.497929\tval_acc: 0.825000 val_loss: 0.4173203 test_acc:0.881250\n",
      "6_23 train_acc: 0.7782 train_loss: 0.447852\tval_acc: 0.825000 val_loss: 0.3706124 test_acc:0.875000\n",
      "6_24 train_acc: 0.7621 train_loss: 0.499334\tval_acc: 0.837500 val_loss: 0.3802425 test_acc:0.862500\n",
      "6_28 train_acc: 0.7581 train_loss: 0.481007\tval_acc: 0.850000 val_loss: 0.3742543 test_acc:0.887500\n",
      "6_29 train_acc: 0.7863 train_loss: 0.477369\tval_acc: 0.862500 val_loss: 0.4033969 test_acc:0.828125\n",
      "6_36 train_acc: 0.6976 train_loss: 0.583883\tval_acc: 0.875000 val_loss: 0.3862550 test_acc:0.887500\n",
      "6_63 train_acc: 0.8508 train_loss: 0.344801\tval_acc: 0.887500 val_loss: 0.3161230 test_acc:0.878125\n",
      "6_64 train_acc: 0.8548 train_loss: 0.360297\tval_acc: 0.900000 val_loss: 0.2970243 test_acc:0.878125\n",
      "6_83 train_acc: 0.8306 train_loss: 0.367871\tval_acc: 0.900000 val_loss: 0.2932145 test_acc:0.878125\n",
      "6_93 train_acc: 0.8266 train_loss: 0.435394\tval_acc: 0.900000 val_loss: 0.2873486 test_acc:0.868750\n",
      "6_100 train_acc: 0.8629 train_loss: 0.304349\tval_acc: 0.912500 val_loss: 0.2717285 test_acc:0.871875\n",
      "6_139 train_acc: 0.8871 train_loss: 0.276001\tval_acc: 0.912500 val_loss: 0.2716390 test_acc:0.868750\n",
      "6_140 train_acc: 0.8710 train_loss: 0.326928\tval_acc: 0.912500 val_loss: 0.2606667 test_acc:0.871875\n",
      "6_148 train_acc: 0.8871 train_loss: 0.257862\tval_acc: 0.925000 val_loss: 0.2225709 test_acc:0.865625\n",
      "6_160 train_acc: 0.8871 train_loss: 0.262259\tval_acc: 0.937500 val_loss: 0.2534757 test_acc:0.881250\n",
      "6_168 train_acc: 0.9032 train_loss: 0.264231\tval_acc: 0.937500 val_loss: 0.2396404 test_acc:0.884375\n",
      "6_184 train_acc: 0.8911 train_loss: 0.254263\tval_acc: 0.937500 val_loss: 0.2328430 test_acc:0.871875\n",
      "6_205 train_acc: 0.8831 train_loss: 0.294883\tval_acc: 0.937500 val_loss: 0.2098396 test_acc:0.878125\n",
      "6_210 train_acc: 0.9032 train_loss: 0.280775\tval_acc: 0.937500 val_loss: 0.2019253 test_acc:0.868750\n",
      "6_230 train_acc: 0.9032 train_loss: 0.236505\tval_acc: 0.937500 val_loss: 0.2008439 test_acc:0.865625\n",
      "6_254 train_acc: 0.8992 train_loss: 0.261285\tval_acc: 0.950000 val_loss: 0.2072096 test_acc:0.859375\n",
      "6_334 train_acc: 0.8992 train_loss: 0.243738\tval_acc: 0.950000 val_loss: 0.1900423 test_acc:0.865625\n",
      "6_364 train_acc: 0.9234 train_loss: 0.176588\tval_acc: 0.950000 val_loss: 0.1889454 test_acc:0.862500\n",
      "6_396 train_acc: 0.9556 train_loss: 0.159276\tval_acc: 0.950000 val_loss: 0.1862958 test_acc:0.887500\n",
      "6_405 train_acc: 0.9194 train_loss: 0.211722\tval_acc: 0.950000 val_loss: 0.1761611 test_acc:0.887500\n",
      "6_426 train_acc: 0.9234 train_loss: 0.260383\tval_acc: 0.950000 val_loss: 0.1724973 test_acc:0.881250\n",
      "6_441 train_acc: 0.9274 train_loss: 0.210727\tval_acc: 0.950000 val_loss: 0.1621380 test_acc:0.896875\n",
      "6_493 train_acc: 0.9435 train_loss: 0.187194\tval_acc: 0.950000 val_loss: 0.1408797 test_acc:0.878125\n",
      "6_515 train_acc: 0.9194 train_loss: 0.185280\tval_acc: 0.962500 val_loss: 0.1746304 test_acc:0.865625\n",
      "6_543 train_acc: 0.9476 train_loss: 0.154095\tval_acc: 0.962500 val_loss: 0.1609087 test_acc:0.846875\n",
      "6_589 train_acc: 0.9355 train_loss: 0.175966\tval_acc: 0.962500 val_loss: 0.1545675 test_acc:0.850000\n",
      "6_612 train_acc: 0.9395 train_loss: 0.167115\tval_acc: 0.962500 val_loss: 0.1303377 test_acc:0.871875\n",
      "6_687 train_acc: 0.9234 train_loss: 0.211770\tval_acc: 0.975000 val_loss: 0.1343011 test_acc:0.859375\n",
      "6_817 train_acc: 0.9274 train_loss: 0.149535\tval_acc: 0.975000 val_loss: 0.1331223 test_acc:0.862500\n",
      "6_850 train_acc: 0.8992 train_loss: 0.251083\tval_acc: 0.975000 val_loss: 0.1169255 test_acc:0.856250\n",
      "epoch:  850 \tThe test accuracy is: 0.85625\n",
      " THE BEST ACCURACY IS 0.85625\tkappa is 0.7125\n",
      "subject 6 duration: 0:27:08.518352\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 6 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "6_0 train_acc: 0.4798 train_loss: 0.751802\tval_acc: 0.500000 val_loss: 0.7245948 test_acc:0.500000\n",
      "6_4 train_acc: 0.4879 train_loss: 0.724847\tval_acc: 0.512500 val_loss: 0.6880461 test_acc:0.534375\n",
      "6_5 train_acc: 0.4879 train_loss: 0.706449\tval_acc: 0.537500 val_loss: 0.6855574 test_acc:0.537500\n",
      "6_7 train_acc: 0.5282 train_loss: 0.698393\tval_acc: 0.562500 val_loss: 0.6782456 test_acc:0.534375\n",
      "6_8 train_acc: 0.5645 train_loss: 0.677417\tval_acc: 0.575000 val_loss: 0.6731126 test_acc:0.534375\n",
      "6_9 train_acc: 0.4879 train_loss: 0.706463\tval_acc: 0.687500 val_loss: 0.6650268 test_acc:0.590625\n",
      "6_13 train_acc: 0.6048 train_loss: 0.664808\tval_acc: 0.762500 val_loss: 0.5779164 test_acc:0.615625\n",
      "6_19 train_acc: 0.7581 train_loss: 0.516954\tval_acc: 0.800000 val_loss: 0.4553033 test_acc:0.762500\n",
      "6_20 train_acc: 0.6774 train_loss: 0.612729\tval_acc: 0.812500 val_loss: 0.4661474 test_acc:0.784375\n",
      "6_23 train_acc: 0.7944 train_loss: 0.475753\tval_acc: 0.837500 val_loss: 0.3634496 test_acc:0.837500\n",
      "6_25 train_acc: 0.7661 train_loss: 0.524327\tval_acc: 0.850000 val_loss: 0.3842502 test_acc:0.837500\n",
      "6_31 train_acc: 0.7984 train_loss: 0.480275\tval_acc: 0.875000 val_loss: 0.2928253 test_acc:0.843750\n",
      "6_40 train_acc: 0.8387 train_loss: 0.413671\tval_acc: 0.887500 val_loss: 0.3194002 test_acc:0.859375\n",
      "6_45 train_acc: 0.8468 train_loss: 0.364521\tval_acc: 0.887500 val_loss: 0.3029541 test_acc:0.856250\n",
      "6_59 train_acc: 0.8468 train_loss: 0.359060\tval_acc: 0.900000 val_loss: 0.2890873 test_acc:0.865625\n",
      "6_78 train_acc: 0.8669 train_loss: 0.316877\tval_acc: 0.900000 val_loss: 0.2746857 test_acc:0.853125\n",
      "6_79 train_acc: 0.8710 train_loss: 0.341281\tval_acc: 0.912500 val_loss: 0.2735460 test_acc:0.859375\n",
      "6_96 train_acc: 0.8790 train_loss: 0.301525\tval_acc: 0.912500 val_loss: 0.2688100 test_acc:0.859375\n",
      "6_111 train_acc: 0.8548 train_loss: 0.297595\tval_acc: 0.912500 val_loss: 0.2484529 test_acc:0.868750\n",
      "6_160 train_acc: 0.8911 train_loss: 0.276668\tval_acc: 0.925000 val_loss: 0.2586792 test_acc:0.840625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_205 train_acc: 0.8911 train_loss: 0.283564\tval_acc: 0.925000 val_loss: 0.2239411 test_acc:0.840625\n",
      "6_236 train_acc: 0.8831 train_loss: 0.261085\tval_acc: 0.925000 val_loss: 0.2226407 test_acc:0.856250\n",
      "6_260 train_acc: 0.8669 train_loss: 0.310353\tval_acc: 0.925000 val_loss: 0.2181106 test_acc:0.859375\n",
      "6_268 train_acc: 0.8750 train_loss: 0.297853\tval_acc: 0.937500 val_loss: 0.2107200 test_acc:0.896875\n",
      "6_351 train_acc: 0.9194 train_loss: 0.216214\tval_acc: 0.937500 val_loss: 0.2104785 test_acc:0.865625\n",
      "6_366 train_acc: 0.8992 train_loss: 0.241132\tval_acc: 0.937500 val_loss: 0.1922111 test_acc:0.871875\n",
      "6_388 train_acc: 0.9113 train_loss: 0.209440\tval_acc: 0.950000 val_loss: 0.2072396 test_acc:0.878125\n",
      "6_457 train_acc: 0.9355 train_loss: 0.193930\tval_acc: 0.950000 val_loss: 0.1936299 test_acc:0.896875\n",
      "6_679 train_acc: 0.8992 train_loss: 0.232549\tval_acc: 0.950000 val_loss: 0.1926452 test_acc:0.853125\n",
      "6_682 train_acc: 0.8952 train_loss: 0.257253\tval_acc: 0.950000 val_loss: 0.1919731 test_acc:0.859375\n",
      "6_691 train_acc: 0.9274 train_loss: 0.175029\tval_acc: 0.950000 val_loss: 0.1876839 test_acc:0.871875\n",
      "6_793 train_acc: 0.9315 train_loss: 0.180106\tval_acc: 0.950000 val_loss: 0.1701085 test_acc:0.856250\n",
      "6_832 train_acc: 0.9032 train_loss: 0.239180\tval_acc: 0.962500 val_loss: 0.2063969 test_acc:0.837500\n",
      "6_920 train_acc: 0.9113 train_loss: 0.209746\tval_acc: 0.962500 val_loss: 0.2009706 test_acc:0.843750\n",
      "6_930 train_acc: 0.9274 train_loss: 0.172546\tval_acc: 0.962500 val_loss: 0.1719624 test_acc:0.890625\n",
      "epoch:  930 \tThe test accuracy is: 0.890625\n",
      " THE BEST ACCURACY IS 0.890625\tkappa is 0.78125\n",
      "subject 6 duration: 0:33:53.030977\n",
      "6 subject 5 fold mean: \n",
      " accuray      87.812500\n",
      "precision    88.230574\n",
      "recall       87.812500\n",
      "f1           87.777378\n",
      "kappa        75.625000\n",
      "dtype: float64\n",
      "seed is 223\n",
      "Subject 7\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 7 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "7_0 train_acc: 0.5282 train_loss: 0.733996\tval_acc: 0.500000 val_loss: 0.7022431 test_acc:0.500000\n",
      "7_1 train_acc: 0.5121 train_loss: 0.719770\tval_acc: 0.550000 val_loss: 0.6883879 test_acc:0.500000\n",
      "7_2 train_acc: 0.4476 train_loss: 0.717233\tval_acc: 0.600000 val_loss: 0.6862272 test_acc:0.600000\n",
      "7_4 train_acc: 0.4839 train_loss: 0.730879\tval_acc: 0.600000 val_loss: 0.6830832 test_acc:0.575000\n",
      "7_5 train_acc: 0.5081 train_loss: 0.701370\tval_acc: 0.637500 val_loss: 0.6806665 test_acc:0.628125\n",
      "7_7 train_acc: 0.5323 train_loss: 0.705623\tval_acc: 0.662500 val_loss: 0.6748258 test_acc:0.618750\n",
      "7_8 train_acc: 0.5202 train_loss: 0.695788\tval_acc: 0.675000 val_loss: 0.6734791 test_acc:0.615625\n",
      "7_9 train_acc: 0.6048 train_loss: 0.684643\tval_acc: 0.687500 val_loss: 0.6697154 test_acc:0.603125\n",
      "7_10 train_acc: 0.6250 train_loss: 0.669429\tval_acc: 0.712500 val_loss: 0.6635690 test_acc:0.625000\n",
      "7_16 train_acc: 0.7056 train_loss: 0.629739\tval_acc: 0.737500 val_loss: 0.6006924 test_acc:0.700000\n",
      "7_17 train_acc: 0.7339 train_loss: 0.576980\tval_acc: 0.762500 val_loss: 0.5673585 test_acc:0.784375\n",
      "7_22 train_acc: 0.7500 train_loss: 0.516561\tval_acc: 0.762500 val_loss: 0.5156662 test_acc:0.793750\n",
      "7_23 train_acc: 0.7903 train_loss: 0.452010\tval_acc: 0.800000 val_loss: 0.5194253 test_acc:0.778125\n",
      "7_24 train_acc: 0.7540 train_loss: 0.512146\tval_acc: 0.812500 val_loss: 0.4540339 test_acc:0.825000\n",
      "7_29 train_acc: 0.7863 train_loss: 0.448314\tval_acc: 0.837500 val_loss: 0.4411780 test_acc:0.850000\n",
      "7_32 train_acc: 0.8226 train_loss: 0.394299\tval_acc: 0.850000 val_loss: 0.4309103 test_acc:0.868750\n",
      "7_37 train_acc: 0.8629 train_loss: 0.369740\tval_acc: 0.862500 val_loss: 0.3770973 test_acc:0.887500\n",
      "7_40 train_acc: 0.8589 train_loss: 0.339587\tval_acc: 0.862500 val_loss: 0.3621550 test_acc:0.881250\n",
      "7_52 train_acc: 0.8629 train_loss: 0.298584\tval_acc: 0.887500 val_loss: 0.3592825 test_acc:0.906250\n",
      "7_81 train_acc: 0.9194 train_loss: 0.221186\tval_acc: 0.887500 val_loss: 0.3429772 test_acc:0.903125\n",
      "7_122 train_acc: 0.9113 train_loss: 0.230738\tval_acc: 0.887500 val_loss: 0.3412567 test_acc:0.928125\n",
      "7_126 train_acc: 0.9194 train_loss: 0.205398\tval_acc: 0.900000 val_loss: 0.3616228 test_acc:0.903125\n",
      "7_127 train_acc: 0.9274 train_loss: 0.187645\tval_acc: 0.912500 val_loss: 0.3732513 test_acc:0.921875\n",
      "7_147 train_acc: 0.8992 train_loss: 0.239700\tval_acc: 0.912500 val_loss: 0.3104136 test_acc:0.909375\n",
      "7_207 train_acc: 0.9556 train_loss: 0.150028\tval_acc: 0.925000 val_loss: 0.2202814 test_acc:0.925000\n",
      "7_288 train_acc: 0.9113 train_loss: 0.207511\tval_acc: 0.937500 val_loss: 0.2399322 test_acc:0.925000\n",
      "7_367 train_acc: 0.9395 train_loss: 0.144007\tval_acc: 0.937500 val_loss: 0.2249424 test_acc:0.925000\n",
      "7_815 train_acc: 0.9556 train_loss: 0.126105\tval_acc: 0.937500 val_loss: 0.2131700 test_acc:0.959375\n",
      "7_819 train_acc: 0.9637 train_loss: 0.121932\tval_acc: 0.937500 val_loss: 0.1731747 test_acc:0.937500\n",
      "epoch:  819 \tThe test accuracy is: 0.9375\n",
      " THE BEST ACCURACY IS 0.9375\tkappa is 0.875\n",
      "subject 7 duration: 0:06:46.230831\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 7 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "7_0 train_acc: 0.5202 train_loss: 0.764629\tval_acc: 0.550000 val_loss: 0.6909304 test_acc:0.500000\n",
      "7_4 train_acc: 0.4919 train_loss: 0.719387\tval_acc: 0.550000 val_loss: 0.6856962 test_acc:0.506250\n",
      "7_5 train_acc: 0.4879 train_loss: 0.705178\tval_acc: 0.562500 val_loss: 0.6845838 test_acc:0.518750\n",
      "7_6 train_acc: 0.5645 train_loss: 0.681222\tval_acc: 0.600000 val_loss: 0.6812447 test_acc:0.559375\n",
      "7_7 train_acc: 0.5242 train_loss: 0.696350\tval_acc: 0.625000 val_loss: 0.6821210 test_acc:0.606250\n",
      "7_12 train_acc: 0.6573 train_loss: 0.648537\tval_acc: 0.637500 val_loss: 0.6582325 test_acc:0.668750\n",
      "7_13 train_acc: 0.6089 train_loss: 0.654840\tval_acc: 0.637500 val_loss: 0.6500798 test_acc:0.725000\n",
      "7_15 train_acc: 0.6331 train_loss: 0.640534\tval_acc: 0.675000 val_loss: 0.6105875 test_acc:0.746875\n",
      "7_18 train_acc: 0.6492 train_loss: 0.637423\tval_acc: 0.800000 val_loss: 0.5371140 test_acc:0.781250\n",
      "7_30 train_acc: 0.7944 train_loss: 0.458463\tval_acc: 0.812500 val_loss: 0.4296962 test_acc:0.856250\n",
      "7_31 train_acc: 0.8024 train_loss: 0.452295\tval_acc: 0.825000 val_loss: 0.4044494 test_acc:0.868750\n",
      "7_33 train_acc: 0.7782 train_loss: 0.455337\tval_acc: 0.837500 val_loss: 0.3635710 test_acc:0.884375\n",
      "7_34 train_acc: 0.8226 train_loss: 0.388638\tval_acc: 0.850000 val_loss: 0.3789431 test_acc:0.878125\n",
      "7_38 train_acc: 0.8548 train_loss: 0.356802\tval_acc: 0.862500 val_loss: 0.3658640 test_acc:0.903125\n",
      "7_45 train_acc: 0.8266 train_loss: 0.376183\tval_acc: 0.862500 val_loss: 0.3288111 test_acc:0.909375\n",
      "7_48 train_acc: 0.8629 train_loss: 0.365411\tval_acc: 0.887500 val_loss: 0.3117550 test_acc:0.925000\n",
      "7_51 train_acc: 0.8629 train_loss: 0.336868\tval_acc: 0.900000 val_loss: 0.2945293 test_acc:0.928125\n",
      "7_71 train_acc: 0.8831 train_loss: 0.292603\tval_acc: 0.900000 val_loss: 0.2696194 test_acc:0.928125\n",
      "7_77 train_acc: 0.8750 train_loss: 0.310815\tval_acc: 0.900000 val_loss: 0.2558938 test_acc:0.943750\n",
      "7_78 train_acc: 0.8589 train_loss: 0.294704\tval_acc: 0.925000 val_loss: 0.2239783 test_acc:0.937500\n",
      "7_121 train_acc: 0.9395 train_loss: 0.202050\tval_acc: 0.925000 val_loss: 0.2224258 test_acc:0.937500\n",
      "7_125 train_acc: 0.9153 train_loss: 0.201968\tval_acc: 0.925000 val_loss: 0.2032762 test_acc:0.934375\n",
      "7_138 train_acc: 0.9315 train_loss: 0.189196\tval_acc: 0.937500 val_loss: 0.1967450 test_acc:0.950000\n",
      "7_155 train_acc: 0.9395 train_loss: 0.184423\tval_acc: 0.937500 val_loss: 0.1879577 test_acc:0.946875\n",
      "7_177 train_acc: 0.9315 train_loss: 0.182186\tval_acc: 0.950000 val_loss: 0.2232792 test_acc:0.931250\n",
      "7_183 train_acc: 0.9274 train_loss: 0.181063\tval_acc: 0.950000 val_loss: 0.2056197 test_acc:0.934375\n",
      "7_185 train_acc: 0.8952 train_loss: 0.214601\tval_acc: 0.962500 val_loss: 0.1639473 test_acc:0.940625\n",
      "7_242 train_acc: 0.9315 train_loss: 0.173809\tval_acc: 0.975000 val_loss: 0.1700854 test_acc:0.940625\n",
      "7_266 train_acc: 0.9274 train_loss: 0.186032\tval_acc: 0.975000 val_loss: 0.1517814 test_acc:0.940625\n",
      "7_779 train_acc: 0.9556 train_loss: 0.152015\tval_acc: 0.975000 val_loss: 0.1159457 test_acc:0.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  779 \tThe test accuracy is: 0.9375\n",
      " THE BEST ACCURACY IS 0.9375\tkappa is 0.875\n",
      "subject 7 duration: 0:13:30.953003\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 7 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "7_0 train_acc: 0.4194 train_loss: 0.812793\tval_acc: 0.500000 val_loss: 0.7511035 test_acc:0.500000\n",
      "7_2 train_acc: 0.5242 train_loss: 0.707848\tval_acc: 0.512500 val_loss: 0.7198338 test_acc:0.500000\n",
      "7_4 train_acc: 0.5242 train_loss: 0.711763\tval_acc: 0.525000 val_loss: 0.6805753 test_acc:0.528125\n",
      "7_5 train_acc: 0.5081 train_loss: 0.713054\tval_acc: 0.562500 val_loss: 0.6716880 test_acc:0.553125\n",
      "7_6 train_acc: 0.5363 train_loss: 0.684835\tval_acc: 0.662500 val_loss: 0.6628063 test_acc:0.600000\n",
      "7_8 train_acc: 0.5524 train_loss: 0.672927\tval_acc: 0.700000 val_loss: 0.6449684 test_acc:0.615625\n",
      "7_9 train_acc: 0.5685 train_loss: 0.677424\tval_acc: 0.700000 val_loss: 0.6288022 test_acc:0.650000\n",
      "7_10 train_acc: 0.6169 train_loss: 0.665287\tval_acc: 0.737500 val_loss: 0.5971401 test_acc:0.603125\n",
      "7_13 train_acc: 0.6855 train_loss: 0.601025\tval_acc: 0.750000 val_loss: 0.5525550 test_acc:0.721875\n",
      "7_14 train_acc: 0.6694 train_loss: 0.621402\tval_acc: 0.750000 val_loss: 0.5159034 test_acc:0.781250\n",
      "7_16 train_acc: 0.7177 train_loss: 0.579704\tval_acc: 0.812500 val_loss: 0.5003904 test_acc:0.746875\n",
      "7_19 train_acc: 0.7500 train_loss: 0.515442\tval_acc: 0.837500 val_loss: 0.3879059 test_acc:0.846875\n",
      "7_23 train_acc: 0.8185 train_loss: 0.474982\tval_acc: 0.837500 val_loss: 0.3484490 test_acc:0.868750\n",
      "7_24 train_acc: 0.7823 train_loss: 0.484469\tval_acc: 0.850000 val_loss: 0.3446689 test_acc:0.834375\n",
      "7_25 train_acc: 0.7984 train_loss: 0.491316\tval_acc: 0.850000 val_loss: 0.3277217 test_acc:0.881250\n",
      "7_28 train_acc: 0.8024 train_loss: 0.421971\tval_acc: 0.887500 val_loss: 0.3186767 test_acc:0.890625\n",
      "7_31 train_acc: 0.8185 train_loss: 0.461824\tval_acc: 0.887500 val_loss: 0.3083704 test_acc:0.893750\n",
      "7_37 train_acc: 0.8427 train_loss: 0.372996\tval_acc: 0.900000 val_loss: 0.2801947 test_acc:0.918750\n",
      "7_40 train_acc: 0.8710 train_loss: 0.343335\tval_acc: 0.925000 val_loss: 0.2583808 test_acc:0.906250\n",
      "7_50 train_acc: 0.8589 train_loss: 0.358857\tval_acc: 0.937500 val_loss: 0.2281471 test_acc:0.903125\n",
      "7_184 train_acc: 0.9194 train_loss: 0.161746\tval_acc: 0.937500 val_loss: 0.2014446 test_acc:0.934375\n",
      "7_292 train_acc: 0.9435 train_loss: 0.150656\tval_acc: 0.950000 val_loss: 0.2390037 test_acc:0.921875\n",
      "7_355 train_acc: 0.9677 train_loss: 0.113183\tval_acc: 0.950000 val_loss: 0.2252747 test_acc:0.928125\n",
      "7_377 train_acc: 0.9153 train_loss: 0.162650\tval_acc: 0.950000 val_loss: 0.2165434 test_acc:0.937500\n",
      "7_382 train_acc: 0.9395 train_loss: 0.173935\tval_acc: 0.950000 val_loss: 0.1843344 test_acc:0.928125\n",
      "7_447 train_acc: 0.9395 train_loss: 0.155354\tval_acc: 0.962500 val_loss: 0.1757787 test_acc:0.931250\n",
      "7_460 train_acc: 0.9476 train_loss: 0.137799\tval_acc: 0.962500 val_loss: 0.1598505 test_acc:0.921875\n",
      "7_651 train_acc: 0.9395 train_loss: 0.137039\tval_acc: 0.962500 val_loss: 0.1510480 test_acc:0.928125\n",
      "7_729 train_acc: 0.9435 train_loss: 0.163744\tval_acc: 0.975000 val_loss: 0.1519779 test_acc:0.937500\n",
      "epoch:  729 \tThe test accuracy is: 0.9375\n",
      " THE BEST ACCURACY IS 0.9375\tkappa is 0.875\n",
      "subject 7 duration: 0:20:15.315650\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 7 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "7_0 train_acc: 0.5161 train_loss: 0.724677\tval_acc: 0.500000 val_loss: 0.7243787 test_acc:0.500000\n",
      "7_2 train_acc: 0.5202 train_loss: 0.723599\tval_acc: 0.500000 val_loss: 0.7016746 test_acc:0.500000\n",
      "7_3 train_acc: 0.5081 train_loss: 0.716363\tval_acc: 0.500000 val_loss: 0.6910941 test_acc:0.496875\n",
      "7_4 train_acc: 0.5645 train_loss: 0.677538\tval_acc: 0.512500 val_loss: 0.6878707 test_acc:0.490625\n",
      "7_6 train_acc: 0.5403 train_loss: 0.687685\tval_acc: 0.525000 val_loss: 0.6831103 test_acc:0.506250\n",
      "7_8 train_acc: 0.5202 train_loss: 0.684021\tval_acc: 0.650000 val_loss: 0.6705065 test_acc:0.553125\n",
      "7_10 train_acc: 0.5565 train_loss: 0.683537\tval_acc: 0.675000 val_loss: 0.6465083 test_acc:0.596875\n",
      "7_11 train_acc: 0.5968 train_loss: 0.673511\tval_acc: 0.675000 val_loss: 0.6247023 test_acc:0.571875\n",
      "7_13 train_acc: 0.6089 train_loss: 0.643079\tval_acc: 0.712500 val_loss: 0.5842190 test_acc:0.687500\n",
      "7_14 train_acc: 0.6573 train_loss: 0.602007\tval_acc: 0.812500 val_loss: 0.4805728 test_acc:0.756250\n",
      "7_34 train_acc: 0.8710 train_loss: 0.347305\tval_acc: 0.825000 val_loss: 0.4487930 test_acc:0.865625\n",
      "7_35 train_acc: 0.8508 train_loss: 0.361544\tval_acc: 0.850000 val_loss: 0.4384267 test_acc:0.862500\n",
      "7_49 train_acc: 0.8468 train_loss: 0.367524\tval_acc: 0.850000 val_loss: 0.3674508 test_acc:0.909375\n",
      "7_54 train_acc: 0.8669 train_loss: 0.303254\tval_acc: 0.875000 val_loss: 0.4232066 test_acc:0.906250\n",
      "7_57 train_acc: 0.8831 train_loss: 0.293038\tval_acc: 0.875000 val_loss: 0.4082250 test_acc:0.909375\n",
      "7_58 train_acc: 0.8589 train_loss: 0.344722\tval_acc: 0.875000 val_loss: 0.3820640 test_acc:0.921875\n",
      "7_65 train_acc: 0.8790 train_loss: 0.333786\tval_acc: 0.875000 val_loss: 0.3757754 test_acc:0.918750\n",
      "7_70 train_acc: 0.8871 train_loss: 0.235339\tval_acc: 0.875000 val_loss: 0.3679594 test_acc:0.934375\n",
      "7_71 train_acc: 0.8992 train_loss: 0.274118\tval_acc: 0.875000 val_loss: 0.3410181 test_acc:0.940625\n",
      "7_107 train_acc: 0.8952 train_loss: 0.239064\tval_acc: 0.875000 val_loss: 0.3356453 test_acc:0.937500\n",
      "7_131 train_acc: 0.9153 train_loss: 0.237143\tval_acc: 0.875000 val_loss: 0.3124058 test_acc:0.950000\n",
      "7_135 train_acc: 0.9194 train_loss: 0.232713\tval_acc: 0.875000 val_loss: 0.2788217 test_acc:0.934375\n",
      "7_136 train_acc: 0.8992 train_loss: 0.261723\tval_acc: 0.887500 val_loss: 0.2847751 test_acc:0.934375\n",
      "7_163 train_acc: 0.9153 train_loss: 0.216106\tval_acc: 0.887500 val_loss: 0.2600217 test_acc:0.946875\n",
      "7_167 train_acc: 0.9274 train_loss: 0.198290\tval_acc: 0.900000 val_loss: 0.2709597 test_acc:0.943750\n",
      "7_172 train_acc: 0.9153 train_loss: 0.229751\tval_acc: 0.900000 val_loss: 0.2612781 test_acc:0.943750\n",
      "7_214 train_acc: 0.9234 train_loss: 0.224881\tval_acc: 0.900000 val_loss: 0.2430175 test_acc:0.928125\n",
      "7_217 train_acc: 0.9194 train_loss: 0.202453\tval_acc: 0.912500 val_loss: 0.2555403 test_acc:0.934375\n",
      "7_239 train_acc: 0.9274 train_loss: 0.178578\tval_acc: 0.912500 val_loss: 0.2453968 test_acc:0.937500\n",
      "7_261 train_acc: 0.9234 train_loss: 0.181702\tval_acc: 0.925000 val_loss: 0.2288635 test_acc:0.934375\n",
      "7_280 train_acc: 0.9516 train_loss: 0.128878\tval_acc: 0.925000 val_loss: 0.2000740 test_acc:0.909375\n",
      "7_284 train_acc: 0.9516 train_loss: 0.143705\tval_acc: 0.925000 val_loss: 0.1918586 test_acc:0.934375\n",
      "7_350 train_acc: 0.9355 train_loss: 0.206805\tval_acc: 0.925000 val_loss: 0.1893341 test_acc:0.921875\n",
      "7_353 train_acc: 0.9355 train_loss: 0.162690\tval_acc: 0.925000 val_loss: 0.1842375 test_acc:0.928125\n",
      "7_358 train_acc: 0.9556 train_loss: 0.142080\tval_acc: 0.937500 val_loss: 0.1828846 test_acc:0.934375\n",
      "7_588 train_acc: 0.9435 train_loss: 0.170192\tval_acc: 0.937500 val_loss: 0.1745842 test_acc:0.937500\n",
      "7_605 train_acc: 0.9355 train_loss: 0.161632\tval_acc: 0.937500 val_loss: 0.1690507 test_acc:0.934375\n",
      "7_609 train_acc: 0.9355 train_loss: 0.171876\tval_acc: 0.937500 val_loss: 0.1403113 test_acc:0.934375\n",
      "7_613 train_acc: 0.9516 train_loss: 0.116726\tval_acc: 0.950000 val_loss: 0.1927864 test_acc:0.928125\n",
      "7_631 train_acc: 0.9476 train_loss: 0.126045\tval_acc: 0.950000 val_loss: 0.1348207 test_acc:0.946875\n",
      "7_814 train_acc: 0.9597 train_loss: 0.118691\tval_acc: 0.962500 val_loss: 0.1412081 test_acc:0.946875\n",
      "epoch:  814 \tThe test accuracy is: 0.946875\n",
      " THE BEST ACCURACY IS 0.946875\tkappa is 0.89375\n",
      "subject 7 duration: 0:27:00.648169\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 7 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "7_0 train_acc: 0.5242 train_loss: 0.746587\tval_acc: 0.500000 val_loss: 0.7043636 test_acc:0.500000\n",
      "7_1 train_acc: 0.5040 train_loss: 0.710132\tval_acc: 0.500000 val_loss: 0.6974376 test_acc:0.503125\n",
      "7_2 train_acc: 0.5202 train_loss: 0.727206\tval_acc: 0.512500 val_loss: 0.6819660 test_acc:0.506250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7_3 train_acc: 0.4919 train_loss: 0.716983\tval_acc: 0.537500 val_loss: 0.6792852 test_acc:0.506250\n",
      "7_4 train_acc: 0.5645 train_loss: 0.695164\tval_acc: 0.725000 val_loss: 0.6653610 test_acc:0.581250\n",
      "7_9 train_acc: 0.5847 train_loss: 0.676397\tval_acc: 0.750000 val_loss: 0.5858803 test_acc:0.678125\n",
      "7_11 train_acc: 0.6411 train_loss: 0.629894\tval_acc: 0.750000 val_loss: 0.5786523 test_acc:0.700000\n",
      "7_12 train_acc: 0.6290 train_loss: 0.629341\tval_acc: 0.787500 val_loss: 0.5352093 test_acc:0.703125\n",
      "7_13 train_acc: 0.6371 train_loss: 0.636659\tval_acc: 0.787500 val_loss: 0.5125898 test_acc:0.709375\n",
      "7_19 train_acc: 0.7863 train_loss: 0.514949\tval_acc: 0.800000 val_loss: 0.4692780 test_acc:0.800000\n",
      "7_32 train_acc: 0.7863 train_loss: 0.489108\tval_acc: 0.812500 val_loss: 0.4583374 test_acc:0.828125\n",
      "7_55 train_acc: 0.8911 train_loss: 0.273865\tval_acc: 0.837500 val_loss: 0.4712297 test_acc:0.893750\n",
      "7_62 train_acc: 0.8911 train_loss: 0.308432\tval_acc: 0.837500 val_loss: 0.3979265 test_acc:0.921875\n",
      "7_80 train_acc: 0.8831 train_loss: 0.281553\tval_acc: 0.850000 val_loss: 0.4071736 test_acc:0.934375\n",
      "7_92 train_acc: 0.8952 train_loss: 0.236143\tval_acc: 0.850000 val_loss: 0.4024306 test_acc:0.921875\n",
      "7_93 train_acc: 0.8589 train_loss: 0.287977\tval_acc: 0.850000 val_loss: 0.3949366 test_acc:0.934375\n",
      "7_96 train_acc: 0.8992 train_loss: 0.254919\tval_acc: 0.862500 val_loss: 0.3927639 test_acc:0.937500\n",
      "7_108 train_acc: 0.9355 train_loss: 0.203682\tval_acc: 0.875000 val_loss: 0.3997242 test_acc:0.934375\n",
      "7_110 train_acc: 0.9234 train_loss: 0.215078\tval_acc: 0.875000 val_loss: 0.3882162 test_acc:0.928125\n",
      "7_118 train_acc: 0.9476 train_loss: 0.159606\tval_acc: 0.875000 val_loss: 0.3760332 test_acc:0.937500\n",
      "7_121 train_acc: 0.9073 train_loss: 0.272394\tval_acc: 0.900000 val_loss: 0.3395919 test_acc:0.940625\n",
      "7_160 train_acc: 0.9153 train_loss: 0.232253\tval_acc: 0.912500 val_loss: 0.3143256 test_acc:0.934375\n",
      "7_327 train_acc: 0.9274 train_loss: 0.227255\tval_acc: 0.912500 val_loss: 0.2502202 test_acc:0.937500\n",
      "7_527 train_acc: 0.9435 train_loss: 0.129352\tval_acc: 0.912500 val_loss: 0.2490742 test_acc:0.934375\n",
      "7_614 train_acc: 0.9355 train_loss: 0.143842\tval_acc: 0.912500 val_loss: 0.2412605 test_acc:0.940625\n",
      "7_652 train_acc: 0.9476 train_loss: 0.109479\tval_acc: 0.912500 val_loss: 0.2162397 test_acc:0.928125\n",
      "7_685 train_acc: 0.9677 train_loss: 0.129400\tval_acc: 0.925000 val_loss: 0.2717647 test_acc:0.943750\n",
      "7_711 train_acc: 0.9516 train_loss: 0.147505\tval_acc: 0.925000 val_loss: 0.2619597 test_acc:0.931250\n",
      "7_712 train_acc: 0.9194 train_loss: 0.203436\tval_acc: 0.925000 val_loss: 0.2409532 test_acc:0.940625\n",
      "7_824 train_acc: 0.9435 train_loss: 0.176936\tval_acc: 0.925000 val_loss: 0.2169973 test_acc:0.937500\n",
      "7_920 train_acc: 0.9234 train_loss: 0.169313\tval_acc: 0.937500 val_loss: 0.2570248 test_acc:0.940625\n",
      "epoch:  920 \tThe test accuracy is: 0.940625\n",
      " THE BEST ACCURACY IS 0.940625\tkappa is 0.88125\n",
      "subject 7 duration: 0:33:46.034560\n",
      "7 subject 5 fold mean: \n",
      " accuray      94.000000\n",
      "precision    94.036362\n",
      "recall       94.000000\n",
      "f1           93.998723\n",
      "kappa        88.000000\n",
      "dtype: float64\n",
      "seed is 494\n",
      "Subject 8\n",
      "-------------------- raw train size： (440, 1, 3, 1000) test size： (320, 3, 1000) subject: 8 fold: 1\n",
      "-------------------- train size： (352, 1, 3, 1000) val size： (88, 1, 3, 1000)\n",
      "8_0 train_acc: 0.5250 train_loss: 0.734736\tval_acc: 0.545455 val_loss: 0.6738547 test_acc:0.578125\n",
      "8_1 train_acc: 0.5964 train_loss: 0.698568\tval_acc: 0.556818 val_loss: 0.6587124 test_acc:0.584375\n",
      "8_2 train_acc: 0.5429 train_loss: 0.706402\tval_acc: 0.715909 val_loss: 0.6050694 test_acc:0.725000\n",
      "8_3 train_acc: 0.5893 train_loss: 0.668692\tval_acc: 0.715909 val_loss: 0.5345794 test_acc:0.775000\n",
      "8_4 train_acc: 0.6929 train_loss: 0.607881\tval_acc: 0.761364 val_loss: 0.4593353 test_acc:0.868750\n",
      "8_5 train_acc: 0.7321 train_loss: 0.552989\tval_acc: 0.772727 val_loss: 0.4443569 test_acc:0.871875\n",
      "8_6 train_acc: 0.7536 train_loss: 0.507658\tval_acc: 0.818182 val_loss: 0.4116130 test_acc:0.906250\n",
      "8_8 train_acc: 0.7571 train_loss: 0.535709\tval_acc: 0.840909 val_loss: 0.3939466 test_acc:0.912500\n",
      "8_23 train_acc: 0.8036 train_loss: 0.453598\tval_acc: 0.840909 val_loss: 0.3897380 test_acc:0.921875\n",
      "8_25 train_acc: 0.8036 train_loss: 0.444436\tval_acc: 0.840909 val_loss: 0.3572816 test_acc:0.912500\n",
      "8_27 train_acc: 0.8036 train_loss: 0.430887\tval_acc: 0.852273 val_loss: 0.4116774 test_acc:0.925000\n",
      "8_57 train_acc: 0.8571 train_loss: 0.334243\tval_acc: 0.852273 val_loss: 0.3412750 test_acc:0.915625\n",
      "8_103 train_acc: 0.8393 train_loss: 0.353604\tval_acc: 0.875000 val_loss: 0.3684710 test_acc:0.946875\n",
      "8_127 train_acc: 0.8536 train_loss: 0.325151\tval_acc: 0.886364 val_loss: 0.3499462 test_acc:0.934375\n",
      "8_137 train_acc: 0.8893 train_loss: 0.288704\tval_acc: 0.886364 val_loss: 0.3206179 test_acc:0.940625\n",
      "8_168 train_acc: 0.8500 train_loss: 0.334673\tval_acc: 0.897727 val_loss: 0.3038583 test_acc:0.931250\n",
      "8_206 train_acc: 0.8679 train_loss: 0.332515\tval_acc: 0.897727 val_loss: 0.2977229 test_acc:0.937500\n",
      "8_237 train_acc: 0.9214 train_loss: 0.230867\tval_acc: 0.909091 val_loss: 0.3107705 test_acc:0.940625\n",
      "8_341 train_acc: 0.8857 train_loss: 0.288363\tval_acc: 0.931818 val_loss: 0.2703893 test_acc:0.950000\n",
      "8_541 train_acc: 0.9393 train_loss: 0.189533\tval_acc: 0.931818 val_loss: 0.2676735 test_acc:0.956250\n",
      "epoch:  541 \tThe test accuracy is: 0.95625\n",
      " THE BEST ACCURACY IS 0.95625\tkappa is 0.9125\n",
      "subject 8 duration: 0:06:40.752153\n",
      "-------------------- raw train size： (440, 1, 3, 1000) test size： (320, 3, 1000) subject: 8 fold: 2\n",
      "-------------------- train size： (352, 1, 3, 1000) val size： (88, 1, 3, 1000)\n",
      "8_0 train_acc: 0.5500 train_loss: 0.738911\tval_acc: 0.500000 val_loss: 0.7379132 test_acc:0.500000\n",
      "8_1 train_acc: 0.5536 train_loss: 0.715629\tval_acc: 0.511364 val_loss: 0.6955278 test_acc:0.500000\n",
      "8_2 train_acc: 0.5357 train_loss: 0.724703\tval_acc: 0.556818 val_loss: 0.6637524 test_acc:0.562500\n",
      "8_3 train_acc: 0.5750 train_loss: 0.658232\tval_acc: 0.579545 val_loss: 0.6565841 test_acc:0.618750\n",
      "8_4 train_acc: 0.6357 train_loss: 0.613274\tval_acc: 0.704545 val_loss: 0.5638624 test_acc:0.828125\n",
      "8_5 train_acc: 0.7214 train_loss: 0.538431\tval_acc: 0.715909 val_loss: 0.5618471 test_acc:0.900000\n",
      "8_6 train_acc: 0.7286 train_loss: 0.542908\tval_acc: 0.738636 val_loss: 0.6034278 test_acc:0.918750\n",
      "8_9 train_acc: 0.8214 train_loss: 0.427750\tval_acc: 0.761364 val_loss: 0.6342286 test_acc:0.921875\n",
      "8_18 train_acc: 0.7964 train_loss: 0.432463\tval_acc: 0.772727 val_loss: 0.5971407 test_acc:0.921875\n",
      "8_21 train_acc: 0.8143 train_loss: 0.388262\tval_acc: 0.772727 val_loss: 0.5923749 test_acc:0.921875\n",
      "8_24 train_acc: 0.8393 train_loss: 0.404948\tval_acc: 0.784091 val_loss: 0.6453335 test_acc:0.921875\n",
      "8_29 train_acc: 0.7857 train_loss: 0.442541\tval_acc: 0.784091 val_loss: 0.5852786 test_acc:0.925000\n",
      "8_45 train_acc: 0.8321 train_loss: 0.378410\tval_acc: 0.784091 val_loss: 0.5189555 test_acc:0.928125\n",
      "8_55 train_acc: 0.8393 train_loss: 0.414761\tval_acc: 0.784091 val_loss: 0.4814946 test_acc:0.921875\n",
      "8_60 train_acc: 0.8071 train_loss: 0.424133\tval_acc: 0.806818 val_loss: 0.4726600 test_acc:0.921875\n",
      "8_64 train_acc: 0.8429 train_loss: 0.388668\tval_acc: 0.818182 val_loss: 0.4788879 test_acc:0.928125\n",
      "8_94 train_acc: 0.8571 train_loss: 0.367794\tval_acc: 0.829545 val_loss: 0.4842044 test_acc:0.928125\n",
      "8_99 train_acc: 0.8214 train_loss: 0.389447\tval_acc: 0.829545 val_loss: 0.4494916 test_acc:0.931250\n",
      "8_100 train_acc: 0.8536 train_loss: 0.340345\tval_acc: 0.829545 val_loss: 0.4481868 test_acc:0.937500\n",
      "8_104 train_acc: 0.8500 train_loss: 0.367829\tval_acc: 0.840909 val_loss: 0.4467199 test_acc:0.925000\n",
      "8_117 train_acc: 0.8750 train_loss: 0.293674\tval_acc: 0.840909 val_loss: 0.4392081 test_acc:0.925000\n",
      "8_118 train_acc: 0.8893 train_loss: 0.288994\tval_acc: 0.863636 val_loss: 0.4563624 test_acc:0.921875\n",
      "8_132 train_acc: 0.8714 train_loss: 0.306145\tval_acc: 0.863636 val_loss: 0.3943201 test_acc:0.918750\n",
      "8_162 train_acc: 0.8964 train_loss: 0.252831\tval_acc: 0.863636 val_loss: 0.3769213 test_acc:0.918750\n",
      "8_184 train_acc: 0.8750 train_loss: 0.327823\tval_acc: 0.863636 val_loss: 0.3355547 test_acc:0.915625\n",
      "8_188 train_acc: 0.8571 train_loss: 0.342781\tval_acc: 0.875000 val_loss: 0.3971333 test_acc:0.918750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_190 train_acc: 0.8750 train_loss: 0.305935\tval_acc: 0.886364 val_loss: 0.3556738 test_acc:0.915625\n",
      "8_244 train_acc: 0.8643 train_loss: 0.301197\tval_acc: 0.886364 val_loss: 0.3229134 test_acc:0.918750\n",
      "8_245 train_acc: 0.8786 train_loss: 0.272556\tval_acc: 0.886364 val_loss: 0.3063569 test_acc:0.918750\n",
      "8_272 train_acc: 0.9107 train_loss: 0.235992\tval_acc: 0.909091 val_loss: 0.2662140 test_acc:0.918750\n",
      "8_361 train_acc: 0.8821 train_loss: 0.272445\tval_acc: 0.909091 val_loss: 0.2540729 test_acc:0.934375\n",
      "epoch:  361 \tThe test accuracy is: 0.934375\n",
      " THE BEST ACCURACY IS 0.934375\tkappa is 0.86875\n",
      "subject 8 duration: 0:13:24.984174\n",
      "-------------------- raw train size： (440, 1, 3, 1000) test size： (320, 3, 1000) subject: 8 fold: 3\n",
      "-------------------- train size： (352, 1, 3, 1000) val size： (88, 1, 3, 1000)\n",
      "8_0 train_acc: 0.5179 train_loss: 0.744946\tval_acc: 0.500000 val_loss: 0.7707474 test_acc:0.500000\n",
      "8_1 train_acc: 0.5500 train_loss: 0.701707\tval_acc: 0.500000 val_loss: 0.7189288 test_acc:0.500000\n",
      "8_2 train_acc: 0.5571 train_loss: 0.679824\tval_acc: 0.511364 val_loss: 0.7387955 test_acc:0.500000\n",
      "8_3 train_acc: 0.6750 train_loss: 0.626092\tval_acc: 0.590909 val_loss: 0.6896798 test_acc:0.615625\n",
      "8_4 train_acc: 0.7036 train_loss: 0.590826\tval_acc: 0.659091 val_loss: 0.6593618 test_acc:0.781250\n",
      "8_5 train_acc: 0.6107 train_loss: 0.660965\tval_acc: 0.670455 val_loss: 0.6940421 test_acc:0.787500\n",
      "8_6 train_acc: 0.7786 train_loss: 0.502932\tval_acc: 0.727273 val_loss: 0.5666547 test_acc:0.868750\n",
      "8_7 train_acc: 0.6964 train_loss: 0.557787\tval_acc: 0.750000 val_loss: 0.5273253 test_acc:0.878125\n",
      "8_10 train_acc: 0.7821 train_loss: 0.474696\tval_acc: 0.795455 val_loss: 0.5478954 test_acc:0.909375\n",
      "8_15 train_acc: 0.7643 train_loss: 0.471132\tval_acc: 0.795455 val_loss: 0.5032434 test_acc:0.909375\n",
      "8_17 train_acc: 0.7964 train_loss: 0.445985\tval_acc: 0.795455 val_loss: 0.4845019 test_acc:0.903125\n",
      "8_22 train_acc: 0.7607 train_loss: 0.506383\tval_acc: 0.795455 val_loss: 0.4834734 test_acc:0.900000\n",
      "8_23 train_acc: 0.8000 train_loss: 0.446803\tval_acc: 0.806818 val_loss: 0.4808814 test_acc:0.912500\n",
      "8_78 train_acc: 0.8607 train_loss: 0.324981\tval_acc: 0.806818 val_loss: 0.4419839 test_acc:0.918750\n",
      "8_87 train_acc: 0.8893 train_loss: 0.291894\tval_acc: 0.818182 val_loss: 0.4728145 test_acc:0.925000\n",
      "8_112 train_acc: 0.8357 train_loss: 0.341193\tval_acc: 0.818182 val_loss: 0.4520947 test_acc:0.918750\n",
      "8_139 train_acc: 0.8857 train_loss: 0.275012\tval_acc: 0.818182 val_loss: 0.4324306 test_acc:0.934375\n",
      "8_141 train_acc: 0.8964 train_loss: 0.253223\tval_acc: 0.829545 val_loss: 0.4750721 test_acc:0.928125\n",
      "8_162 train_acc: 0.9071 train_loss: 0.274254\tval_acc: 0.829545 val_loss: 0.4433334 test_acc:0.925000\n",
      "8_164 train_acc: 0.8929 train_loss: 0.259950\tval_acc: 0.829545 val_loss: 0.4380845 test_acc:0.925000\n",
      "8_177 train_acc: 0.9000 train_loss: 0.248748\tval_acc: 0.840909 val_loss: 0.4885898 test_acc:0.937500\n",
      "8_178 train_acc: 0.9250 train_loss: 0.219373\tval_acc: 0.840909 val_loss: 0.4763199 test_acc:0.934375\n",
      "8_181 train_acc: 0.8821 train_loss: 0.264576\tval_acc: 0.840909 val_loss: 0.4453550 test_acc:0.918750\n",
      "8_185 train_acc: 0.8929 train_loss: 0.272113\tval_acc: 0.840909 val_loss: 0.4255961 test_acc:0.918750\n",
      "8_199 train_acc: 0.8929 train_loss: 0.265460\tval_acc: 0.840909 val_loss: 0.4060834 test_acc:0.921875\n",
      "8_219 train_acc: 0.8714 train_loss: 0.323042\tval_acc: 0.863636 val_loss: 0.3844966 test_acc:0.925000\n",
      "8_337 train_acc: 0.9429 train_loss: 0.192736\tval_acc: 0.875000 val_loss: 0.4673202 test_acc:0.934375\n",
      "8_346 train_acc: 0.9107 train_loss: 0.199272\tval_acc: 0.886364 val_loss: 0.4331903 test_acc:0.934375\n",
      "8_508 train_acc: 0.8964 train_loss: 0.239685\tval_acc: 0.886364 val_loss: 0.4150874 test_acc:0.946875\n",
      "8_553 train_acc: 0.9393 train_loss: 0.184901\tval_acc: 0.886364 val_loss: 0.4065068 test_acc:0.937500\n",
      "8_557 train_acc: 0.9357 train_loss: 0.165646\tval_acc: 0.886364 val_loss: 0.3870414 test_acc:0.940625\n",
      "8_663 train_acc: 0.8929 train_loss: 0.263829\tval_acc: 0.886364 val_loss: 0.3716757 test_acc:0.940625\n",
      "8_720 train_acc: 0.8964 train_loss: 0.222310\tval_acc: 0.897727 val_loss: 0.3860280 test_acc:0.943750\n",
      "8_937 train_acc: 0.9286 train_loss: 0.180652\tval_acc: 0.897727 val_loss: 0.3852778 test_acc:0.946875\n",
      "epoch:  937 \tThe test accuracy is: 0.946875\n",
      " THE BEST ACCURACY IS 0.946875\tkappa is 0.89375\n",
      "subject 8 duration: 0:20:05.896232\n",
      "-------------------- raw train size： (440, 1, 3, 1000) test size： (320, 3, 1000) subject: 8 fold: 4\n",
      "-------------------- train size： (352, 1, 3, 1000) val size： (88, 1, 3, 1000)\n",
      "8_0 train_acc: 0.5321 train_loss: 0.760592\tval_acc: 0.500000 val_loss: 0.6865684 test_acc:0.525000\n",
      "8_1 train_acc: 0.4929 train_loss: 0.742972\tval_acc: 0.534091 val_loss: 0.6735861 test_acc:0.656250\n",
      "8_2 train_acc: 0.5821 train_loss: 0.690403\tval_acc: 0.545455 val_loss: 0.6840393 test_acc:0.618750\n",
      "8_3 train_acc: 0.5786 train_loss: 0.677802\tval_acc: 0.647727 val_loss: 0.6485921 test_acc:0.787500\n",
      "8_4 train_acc: 0.6964 train_loss: 0.603885\tval_acc: 0.681818 val_loss: 0.6881962 test_acc:0.825000\n",
      "8_5 train_acc: 0.6250 train_loss: 0.693869\tval_acc: 0.704545 val_loss: 0.7200326 test_acc:0.868750\n",
      "8_8 train_acc: 0.7464 train_loss: 0.543342\tval_acc: 0.715909 val_loss: 0.7822498 test_acc:0.890625\n",
      "8_40 train_acc: 0.8179 train_loss: 0.396681\tval_acc: 0.738636 val_loss: 0.7859334 test_acc:0.909375\n",
      "8_47 train_acc: 0.8607 train_loss: 0.357923\tval_acc: 0.738636 val_loss: 0.7087325 test_acc:0.925000\n",
      "8_58 train_acc: 0.8214 train_loss: 0.412567\tval_acc: 0.750000 val_loss: 0.6836989 test_acc:0.928125\n",
      "8_69 train_acc: 0.8321 train_loss: 0.367609\tval_acc: 0.750000 val_loss: 0.6640341 test_acc:0.915625\n",
      "8_74 train_acc: 0.8500 train_loss: 0.338267\tval_acc: 0.750000 val_loss: 0.6432903 test_acc:0.915625\n",
      "8_77 train_acc: 0.8607 train_loss: 0.323283\tval_acc: 0.772727 val_loss: 0.6552109 test_acc:0.928125\n",
      "8_88 train_acc: 0.8643 train_loss: 0.308348\tval_acc: 0.772727 val_loss: 0.6180897 test_acc:0.925000\n",
      "8_89 train_acc: 0.8643 train_loss: 0.322504\tval_acc: 0.772727 val_loss: 0.5909399 test_acc:0.921875\n",
      "8_100 train_acc: 0.8536 train_loss: 0.362900\tval_acc: 0.784091 val_loss: 0.5492383 test_acc:0.912500\n",
      "8_126 train_acc: 0.8679 train_loss: 0.318419\tval_acc: 0.795455 val_loss: 0.5611546 test_acc:0.925000\n",
      "8_134 train_acc: 0.9071 train_loss: 0.240748\tval_acc: 0.806818 val_loss: 0.5640876 test_acc:0.918750\n",
      "8_146 train_acc: 0.8964 train_loss: 0.268778\tval_acc: 0.806818 val_loss: 0.5396920 test_acc:0.918750\n",
      "8_158 train_acc: 0.8821 train_loss: 0.300660\tval_acc: 0.818182 val_loss: 0.5595235 test_acc:0.940625\n",
      "8_163 train_acc: 0.8464 train_loss: 0.348482\tval_acc: 0.818182 val_loss: 0.5023426 test_acc:0.928125\n",
      "8_207 train_acc: 0.8750 train_loss: 0.280831\tval_acc: 0.818182 val_loss: 0.4958967 test_acc:0.940625\n",
      "8_216 train_acc: 0.8750 train_loss: 0.287880\tval_acc: 0.829545 val_loss: 0.4705215 test_acc:0.928125\n",
      "8_228 train_acc: 0.9214 train_loss: 0.227295\tval_acc: 0.840909 val_loss: 0.4967643 test_acc:0.925000\n",
      "8_234 train_acc: 0.9036 train_loss: 0.234964\tval_acc: 0.840909 val_loss: 0.4811558 test_acc:0.925000\n",
      "8_247 train_acc: 0.8929 train_loss: 0.238722\tval_acc: 0.852273 val_loss: 0.4658554 test_acc:0.934375\n",
      "8_418 train_acc: 0.9179 train_loss: 0.215716\tval_acc: 0.852273 val_loss: 0.4017070 test_acc:0.937500\n",
      "8_462 train_acc: 0.9071 train_loss: 0.269087\tval_acc: 0.852273 val_loss: 0.3954744 test_acc:0.940625\n",
      "8_470 train_acc: 0.8786 train_loss: 0.289119\tval_acc: 0.863636 val_loss: 0.4091883 test_acc:0.940625\n",
      "8_561 train_acc: 0.8929 train_loss: 0.220166\tval_acc: 0.863636 val_loss: 0.4015652 test_acc:0.940625\n",
      "8_820 train_acc: 0.9214 train_loss: 0.197517\tval_acc: 0.886364 val_loss: 0.3385058 test_acc:0.953125\n",
      "epoch:  820 \tThe test accuracy is: 0.953125\n",
      " THE BEST ACCURACY IS 0.953125\tkappa is 0.90625\n",
      "subject 8 duration: 0:26:48.514689\n",
      "-------------------- raw train size： (440, 1, 3, 1000) test size： (320, 3, 1000) subject: 8 fold: 5\n",
      "-------------------- train size： (352, 1, 3, 1000) val size： (88, 1, 3, 1000)\n",
      "8_0 train_acc: 0.5571 train_loss: 0.718257\tval_acc: 0.488636 val_loss: 0.6878273 test_acc:0.503125\n",
      "8_1 train_acc: 0.5179 train_loss: 0.738643\tval_acc: 0.534091 val_loss: 0.6712113 test_acc:0.621875\n",
      "8_2 train_acc: 0.5286 train_loss: 0.694209\tval_acc: 0.647727 val_loss: 0.6423078 test_acc:0.768750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_3 train_acc: 0.6679 train_loss: 0.638616\tval_acc: 0.670455 val_loss: 0.6063108 test_acc:0.818750\n",
      "8_4 train_acc: 0.7107 train_loss: 0.562976\tval_acc: 0.681818 val_loss: 0.6083933 test_acc:0.850000\n",
      "8_7 train_acc: 0.7143 train_loss: 0.565500\tval_acc: 0.727273 val_loss: 0.5564250 test_acc:0.893750\n",
      "8_8 train_acc: 0.7679 train_loss: 0.523723\tval_acc: 0.738636 val_loss: 0.5994162 test_acc:0.865625\n",
      "8_9 train_acc: 0.7714 train_loss: 0.519314\tval_acc: 0.738636 val_loss: 0.5505543 test_acc:0.871875\n",
      "8_11 train_acc: 0.7643 train_loss: 0.476076\tval_acc: 0.761364 val_loss: 0.5089976 test_acc:0.893750\n",
      "8_12 train_acc: 0.7286 train_loss: 0.528536\tval_acc: 0.761364 val_loss: 0.5017951 test_acc:0.900000\n",
      "8_15 train_acc: 0.8250 train_loss: 0.409372\tval_acc: 0.772727 val_loss: 0.4942768 test_acc:0.884375\n",
      "8_17 train_acc: 0.8036 train_loss: 0.426530\tval_acc: 0.772727 val_loss: 0.4929410 test_acc:0.884375\n",
      "8_23 train_acc: 0.8429 train_loss: 0.424363\tval_acc: 0.795455 val_loss: 0.4931730 test_acc:0.896875\n",
      "8_29 train_acc: 0.7893 train_loss: 0.467427\tval_acc: 0.795455 val_loss: 0.4856260 test_acc:0.893750\n",
      "8_30 train_acc: 0.8500 train_loss: 0.363675\tval_acc: 0.818182 val_loss: 0.4994541 test_acc:0.896875\n",
      "8_41 train_acc: 0.8286 train_loss: 0.413893\tval_acc: 0.818182 val_loss: 0.4500413 test_acc:0.909375\n",
      "8_43 train_acc: 0.8500 train_loss: 0.352522\tval_acc: 0.840909 val_loss: 0.4576968 test_acc:0.903125\n",
      "8_45 train_acc: 0.8357 train_loss: 0.370896\tval_acc: 0.840909 val_loss: 0.4402113 test_acc:0.903125\n",
      "8_57 train_acc: 0.8464 train_loss: 0.347588\tval_acc: 0.852273 val_loss: 0.4495824 test_acc:0.909375\n",
      "8_123 train_acc: 0.8750 train_loss: 0.320215\tval_acc: 0.875000 val_loss: 0.4130801 test_acc:0.921875\n",
      "8_229 train_acc: 0.8857 train_loss: 0.292661\tval_acc: 0.875000 val_loss: 0.3824275 test_acc:0.934375\n",
      "8_337 train_acc: 0.9321 train_loss: 0.221375\tval_acc: 0.875000 val_loss: 0.3719400 test_acc:0.946875\n",
      "epoch:  337 \tThe test accuracy is: 0.946875\n",
      " THE BEST ACCURACY IS 0.946875\tkappa is 0.89375\n",
      "subject 8 duration: 0:33:29.679853\n",
      "8 subject 5 fold mean: \n",
      " accuray      94.750000\n",
      "precision    94.769710\n",
      "recall       94.750000\n",
      "f1           94.749454\n",
      "kappa        89.500000\n",
      "dtype: float64\n",
      "seed is 173\n",
      "Subject 9\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 9 fold: 1\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "9_0 train_acc: 0.4960 train_loss: 0.760541\tval_acc: 0.500000 val_loss: 0.6959755 test_acc:0.500000\n",
      "9_2 train_acc: 0.5363 train_loss: 0.711991\tval_acc: 0.612500 val_loss: 0.6867661 test_acc:0.556250\n",
      "9_3 train_acc: 0.5040 train_loss: 0.713771\tval_acc: 0.662500 val_loss: 0.6830641 test_acc:0.609375\n",
      "9_8 train_acc: 0.6492 train_loss: 0.636394\tval_acc: 0.687500 val_loss: 0.6138285 test_acc:0.737500\n",
      "9_9 train_acc: 0.5968 train_loss: 0.728320\tval_acc: 0.725000 val_loss: 0.5961637 test_acc:0.737500\n",
      "9_16 train_acc: 0.7097 train_loss: 0.565284\tval_acc: 0.750000 val_loss: 0.6929564 test_acc:0.790625\n",
      "9_23 train_acc: 0.7581 train_loss: 0.480844\tval_acc: 0.750000 val_loss: 0.6365646 test_acc:0.803125\n",
      "9_34 train_acc: 0.8266 train_loss: 0.419721\tval_acc: 0.762500 val_loss: 0.6662427 test_acc:0.821875\n",
      "9_52 train_acc: 0.8065 train_loss: 0.423335\tval_acc: 0.762500 val_loss: 0.6550559 test_acc:0.843750\n",
      "9_58 train_acc: 0.8629 train_loss: 0.350132\tval_acc: 0.775000 val_loss: 0.6699260 test_acc:0.825000\n",
      "9_78 train_acc: 0.8145 train_loss: 0.409141\tval_acc: 0.775000 val_loss: 0.5937290 test_acc:0.875000\n",
      "9_79 train_acc: 0.8548 train_loss: 0.365811\tval_acc: 0.775000 val_loss: 0.5610629 test_acc:0.862500\n",
      "9_93 train_acc: 0.8427 train_loss: 0.396266\tval_acc: 0.812500 val_loss: 0.5705919 test_acc:0.890625\n",
      "9_143 train_acc: 0.8871 train_loss: 0.276460\tval_acc: 0.812500 val_loss: 0.5526195 test_acc:0.884375\n",
      "9_165 train_acc: 0.8871 train_loss: 0.283167\tval_acc: 0.812500 val_loss: 0.5002286 test_acc:0.875000\n",
      "9_198 train_acc: 0.8871 train_loss: 0.277143\tval_acc: 0.825000 val_loss: 0.5134353 test_acc:0.881250\n",
      "9_276 train_acc: 0.8750 train_loss: 0.328205\tval_acc: 0.825000 val_loss: 0.4535535 test_acc:0.875000\n",
      "9_393 train_acc: 0.9113 train_loss: 0.225977\tval_acc: 0.837500 val_loss: 0.4126314 test_acc:0.878125\n",
      "9_444 train_acc: 0.9274 train_loss: 0.228700\tval_acc: 0.837500 val_loss: 0.4055613 test_acc:0.896875\n",
      "9_485 train_acc: 0.8911 train_loss: 0.233449\tval_acc: 0.850000 val_loss: 0.3854063 test_acc:0.887500\n",
      "9_513 train_acc: 0.8952 train_loss: 0.257082\tval_acc: 0.850000 val_loss: 0.3796252 test_acc:0.887500\n",
      "9_692 train_acc: 0.9032 train_loss: 0.210822\tval_acc: 0.850000 val_loss: 0.3533692 test_acc:0.887500\n",
      "9_760 train_acc: 0.9355 train_loss: 0.167042\tval_acc: 0.862500 val_loss: 0.4004218 test_acc:0.903125\n",
      "9_767 train_acc: 0.9194 train_loss: 0.213477\tval_acc: 0.862500 val_loss: 0.3618199 test_acc:0.906250\n",
      "9_804 train_acc: 0.9194 train_loss: 0.234006\tval_acc: 0.875000 val_loss: 0.4048180 test_acc:0.881250\n",
      "9_941 train_acc: 0.8871 train_loss: 0.254877\tval_acc: 0.875000 val_loss: 0.3563581 test_acc:0.893750\n",
      "epoch:  941 \tThe test accuracy is: 0.89375\n",
      " THE BEST ACCURACY IS 0.89375\tkappa is 0.7875\n",
      "subject 9 duration: 0:06:50.076686\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 9 fold: 2\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "9_0 train_acc: 0.5000 train_loss: 0.728740\tval_acc: 0.500000 val_loss: 0.7730911 test_acc:0.500000\n",
      "9_1 train_acc: 0.5403 train_loss: 0.722336\tval_acc: 0.500000 val_loss: 0.7113009 test_acc:0.500000\n",
      "9_3 train_acc: 0.4516 train_loss: 0.733436\tval_acc: 0.500000 val_loss: 0.7066547 test_acc:0.500000\n",
      "9_4 train_acc: 0.4960 train_loss: 0.710592\tval_acc: 0.562500 val_loss: 0.6861493 test_acc:0.500000\n",
      "9_5 train_acc: 0.5524 train_loss: 0.695437\tval_acc: 0.562500 val_loss: 0.6779833 test_acc:0.600000\n",
      "9_6 train_acc: 0.5444 train_loss: 0.703427\tval_acc: 0.762500 val_loss: 0.6738826 test_acc:0.700000\n",
      "9_18 train_acc: 0.7702 train_loss: 0.515479\tval_acc: 0.787500 val_loss: 0.4840544 test_acc:0.765625\n",
      "9_29 train_acc: 0.7702 train_loss: 0.483532\tval_acc: 0.812500 val_loss: 0.4543405 test_acc:0.815625\n",
      "9_34 train_acc: 0.7863 train_loss: 0.509004\tval_acc: 0.812500 val_loss: 0.4080253 test_acc:0.821875\n",
      "9_39 train_acc: 0.7823 train_loss: 0.443117\tval_acc: 0.825000 val_loss: 0.4317571 test_acc:0.818750\n",
      "9_43 train_acc: 0.7984 train_loss: 0.435636\tval_acc: 0.837500 val_loss: 0.4880051 test_acc:0.756250\n",
      "9_48 train_acc: 0.8024 train_loss: 0.420153\tval_acc: 0.850000 val_loss: 0.4188607 test_acc:0.812500\n",
      "9_54 train_acc: 0.8508 train_loss: 0.384058\tval_acc: 0.850000 val_loss: 0.4148395 test_acc:0.846875\n",
      "9_59 train_acc: 0.8468 train_loss: 0.330393\tval_acc: 0.862500 val_loss: 0.4120370 test_acc:0.840625\n",
      "9_67 train_acc: 0.8387 train_loss: 0.361205\tval_acc: 0.862500 val_loss: 0.3879917 test_acc:0.834375\n",
      "9_75 train_acc: 0.8548 train_loss: 0.334625\tval_acc: 0.875000 val_loss: 0.3829433 test_acc:0.837500\n",
      "9_84 train_acc: 0.8548 train_loss: 0.343058\tval_acc: 0.875000 val_loss: 0.3749135 test_acc:0.875000\n",
      "9_85 train_acc: 0.8871 train_loss: 0.301801\tval_acc: 0.887500 val_loss: 0.3696326 test_acc:0.856250\n",
      "9_103 train_acc: 0.8871 train_loss: 0.311798\tval_acc: 0.887500 val_loss: 0.3530124 test_acc:0.884375\n",
      "9_106 train_acc: 0.8226 train_loss: 0.383711\tval_acc: 0.887500 val_loss: 0.3448672 test_acc:0.878125\n",
      "9_107 train_acc: 0.8468 train_loss: 0.342804\tval_acc: 0.900000 val_loss: 0.3501946 test_acc:0.878125\n",
      "9_141 train_acc: 0.8347 train_loss: 0.347673\tval_acc: 0.912500 val_loss: 0.3508823 test_acc:0.875000\n",
      "9_182 train_acc: 0.8629 train_loss: 0.309197\tval_acc: 0.912500 val_loss: 0.3271951 test_acc:0.896875\n",
      "9_315 train_acc: 0.8911 train_loss: 0.270634\tval_acc: 0.912500 val_loss: 0.3049551 test_acc:0.881250\n",
      "9_326 train_acc: 0.9234 train_loss: 0.206728\tval_acc: 0.912500 val_loss: 0.3032846 test_acc:0.896875\n",
      "9_388 train_acc: 0.9194 train_loss: 0.207311\tval_acc: 0.925000 val_loss: 0.3141703 test_acc:0.906250\n",
      "9_393 train_acc: 0.8952 train_loss: 0.213137\tval_acc: 0.925000 val_loss: 0.2941566 test_acc:0.909375\n",
      "9_449 train_acc: 0.9113 train_loss: 0.216371\tval_acc: 0.925000 val_loss: 0.2878310 test_acc:0.900000\n",
      "9_459 train_acc: 0.8992 train_loss: 0.230461\tval_acc: 0.925000 val_loss: 0.2755781 test_acc:0.893750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_515 train_acc: 0.9194 train_loss: 0.206637\tval_acc: 0.937500 val_loss: 0.3021417 test_acc:0.890625\n",
      "9_608 train_acc: 0.8911 train_loss: 0.240185\tval_acc: 0.937500 val_loss: 0.2754226 test_acc:0.890625\n",
      "9_898 train_acc: 0.9073 train_loss: 0.247302\tval_acc: 0.937500 val_loss: 0.2376166 test_acc:0.881250\n",
      "epoch:  898 \tThe test accuracy is: 0.88125\n",
      " THE BEST ACCURACY IS 0.88125\tkappa is 0.7625\n",
      "subject 9 duration: 0:13:36.773288\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 9 fold: 3\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "9_0 train_acc: 0.5282 train_loss: 0.754011\tval_acc: 0.500000 val_loss: 0.7175877 test_acc:0.500000\n",
      "9_1 train_acc: 0.5403 train_loss: 0.708905\tval_acc: 0.525000 val_loss: 0.6950948 test_acc:0.500000\n",
      "9_9 train_acc: 0.6492 train_loss: 0.637504\tval_acc: 0.650000 val_loss: 0.6754226 test_acc:0.771875\n",
      "9_12 train_acc: 0.7258 train_loss: 0.549865\tval_acc: 0.662500 val_loss: 0.6974292 test_acc:0.781250\n",
      "9_17 train_acc: 0.7540 train_loss: 0.517634\tval_acc: 0.687500 val_loss: 0.6058507 test_acc:0.815625\n",
      "9_18 train_acc: 0.7823 train_loss: 0.509155\tval_acc: 0.737500 val_loss: 0.6282681 test_acc:0.825000\n",
      "9_21 train_acc: 0.8105 train_loss: 0.412341\tval_acc: 0.750000 val_loss: 0.6233263 test_acc:0.828125\n",
      "9_27 train_acc: 0.8185 train_loss: 0.432743\tval_acc: 0.750000 val_loss: 0.5974477 test_acc:0.834375\n",
      "9_31 train_acc: 0.8105 train_loss: 0.429810\tval_acc: 0.762500 val_loss: 0.5426005 test_acc:0.831250\n",
      "9_41 train_acc: 0.7742 train_loss: 0.447179\tval_acc: 0.775000 val_loss: 0.5507604 test_acc:0.843750\n",
      "9_57 train_acc: 0.8548 train_loss: 0.357263\tval_acc: 0.787500 val_loss: 0.5038420 test_acc:0.871875\n",
      "9_63 train_acc: 0.8266 train_loss: 0.388426\tval_acc: 0.800000 val_loss: 0.5630627 test_acc:0.884375\n",
      "9_66 train_acc: 0.8468 train_loss: 0.379090\tval_acc: 0.800000 val_loss: 0.5618072 test_acc:0.878125\n",
      "9_70 train_acc: 0.8145 train_loss: 0.395945\tval_acc: 0.800000 val_loss: 0.5308785 test_acc:0.881250\n",
      "9_71 train_acc: 0.8468 train_loss: 0.335675\tval_acc: 0.812500 val_loss: 0.4936356 test_acc:0.893750\n",
      "9_83 train_acc: 0.8589 train_loss: 0.370730\tval_acc: 0.825000 val_loss: 0.5421259 test_acc:0.887500\n",
      "9_89 train_acc: 0.8871 train_loss: 0.319428\tval_acc: 0.825000 val_loss: 0.5077524 test_acc:0.893750\n",
      "9_92 train_acc: 0.8589 train_loss: 0.364799\tval_acc: 0.825000 val_loss: 0.4988235 test_acc:0.896875\n",
      "9_93 train_acc: 0.8347 train_loss: 0.381079\tval_acc: 0.862500 val_loss: 0.4753726 test_acc:0.890625\n",
      "9_129 train_acc: 0.8589 train_loss: 0.341580\tval_acc: 0.862500 val_loss: 0.4585780 test_acc:0.890625\n",
      "9_154 train_acc: 0.8548 train_loss: 0.350749\tval_acc: 0.862500 val_loss: 0.4493378 test_acc:0.903125\n",
      "9_174 train_acc: 0.9234 train_loss: 0.226666\tval_acc: 0.862500 val_loss: 0.4399390 test_acc:0.906250\n",
      "9_186 train_acc: 0.8790 train_loss: 0.272825\tval_acc: 0.875000 val_loss: 0.4582509 test_acc:0.903125\n",
      "9_390 train_acc: 0.9153 train_loss: 0.201784\tval_acc: 0.875000 val_loss: 0.3897277 test_acc:0.900000\n",
      "9_412 train_acc: 0.8831 train_loss: 0.240595\tval_acc: 0.887500 val_loss: 0.3341298 test_acc:0.893750\n",
      "9_567 train_acc: 0.8669 train_loss: 0.319825\tval_acc: 0.887500 val_loss: 0.3282239 test_acc:0.896875\n",
      "9_585 train_acc: 0.9315 train_loss: 0.183036\tval_acc: 0.887500 val_loss: 0.3210163 test_acc:0.896875\n",
      "9_598 train_acc: 0.9315 train_loss: 0.195996\tval_acc: 0.900000 val_loss: 0.3180564 test_acc:0.896875\n",
      "9_674 train_acc: 0.9194 train_loss: 0.185968\tval_acc: 0.900000 val_loss: 0.2799547 test_acc:0.900000\n",
      "9_969 train_acc: 0.9355 train_loss: 0.183224\tval_acc: 0.912500 val_loss: 0.3004525 test_acc:0.887500\n",
      "epoch:  969 \tThe test accuracy is: 0.8875\n",
      " THE BEST ACCURACY IS 0.8875\tkappa is 0.775\n",
      "subject 9 duration: 0:20:06.472526\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 9 fold: 4\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "9_0 train_acc: 0.5363 train_loss: 0.712925\tval_acc: 0.525000 val_loss: 0.6883185 test_acc:0.490625\n",
      "9_5 train_acc: 0.5282 train_loss: 0.700138\tval_acc: 0.587500 val_loss: 0.6866474 test_acc:0.600000\n",
      "9_8 train_acc: 0.5685 train_loss: 0.687061\tval_acc: 0.687500 val_loss: 0.6643173 test_acc:0.643750\n",
      "9_10 train_acc: 0.5968 train_loss: 0.673481\tval_acc: 0.712500 val_loss: 0.6077532 test_acc:0.681250\n",
      "9_15 train_acc: 0.7379 train_loss: 0.538180\tval_acc: 0.725000 val_loss: 0.5472530 test_acc:0.768750\n",
      "9_16 train_acc: 0.7339 train_loss: 0.553176\tval_acc: 0.737500 val_loss: 0.5948765 test_acc:0.700000\n",
      "9_19 train_acc: 0.7419 train_loss: 0.519825\tval_acc: 0.737500 val_loss: 0.4928034 test_acc:0.775000\n",
      "9_23 train_acc: 0.7137 train_loss: 0.541837\tval_acc: 0.762500 val_loss: 0.4948245 test_acc:0.787500\n",
      "9_27 train_acc: 0.7742 train_loss: 0.508597\tval_acc: 0.775000 val_loss: 0.4963218 test_acc:0.803125\n",
      "9_48 train_acc: 0.8669 train_loss: 0.323990\tval_acc: 0.812500 val_loss: 0.4620427 test_acc:0.803125\n",
      "9_69 train_acc: 0.8387 train_loss: 0.397371\tval_acc: 0.837500 val_loss: 0.4807257 test_acc:0.884375\n",
      "9_77 train_acc: 0.8710 train_loss: 0.321404\tval_acc: 0.837500 val_loss: 0.4469519 test_acc:0.853125\n",
      "9_82 train_acc: 0.8387 train_loss: 0.361814\tval_acc: 0.862500 val_loss: 0.4464424 test_acc:0.859375\n",
      "9_135 train_acc: 0.8992 train_loss: 0.279551\tval_acc: 0.862500 val_loss: 0.4377221 test_acc:0.875000\n",
      "9_148 train_acc: 0.8831 train_loss: 0.272785\tval_acc: 0.862500 val_loss: 0.4069772 test_acc:0.871875\n",
      "9_154 train_acc: 0.8669 train_loss: 0.310288\tval_acc: 0.862500 val_loss: 0.4061568 test_acc:0.896875\n",
      "9_177 train_acc: 0.8508 train_loss: 0.328254\tval_acc: 0.875000 val_loss: 0.4041853 test_acc:0.896875\n",
      "9_352 train_acc: 0.8911 train_loss: 0.237383\tval_acc: 0.875000 val_loss: 0.3430269 test_acc:0.878125\n",
      "9_362 train_acc: 0.8589 train_loss: 0.337164\tval_acc: 0.887500 val_loss: 0.3429272 test_acc:0.878125\n",
      "epoch:  362 \tThe test accuracy is: 0.878125\n",
      " THE BEST ACCURACY IS 0.878125\tkappa is 0.75625\n",
      "subject 9 duration: 0:26:31.127329\n",
      "-------------------- raw train size： (400, 1, 3, 1000) test size： (320, 3, 1000) subject: 9 fold: 5\n",
      "-------------------- train size： (320, 1, 3, 1000) val size： (80, 1, 3, 1000)\n",
      "9_0 train_acc: 0.5161 train_loss: 0.741426\tval_acc: 0.500000 val_loss: 0.7240864 test_acc:0.500000\n",
      "9_1 train_acc: 0.5202 train_loss: 0.729183\tval_acc: 0.500000 val_loss: 0.7014656 test_acc:0.500000\n",
      "9_2 train_acc: 0.5242 train_loss: 0.706481\tval_acc: 0.500000 val_loss: 0.6924602 test_acc:0.503125\n",
      "9_3 train_acc: 0.5363 train_loss: 0.691627\tval_acc: 0.512500 val_loss: 0.6889772 test_acc:0.500000\n",
      "9_4 train_acc: 0.5484 train_loss: 0.710624\tval_acc: 0.537500 val_loss: 0.6779354 test_acc:0.606250\n",
      "9_5 train_acc: 0.5887 train_loss: 0.688454\tval_acc: 0.600000 val_loss: 0.6698158 test_acc:0.615625\n",
      "9_6 train_acc: 0.5000 train_loss: 0.717037\tval_acc: 0.650000 val_loss: 0.6623492 test_acc:0.615625\n",
      "9_8 train_acc: 0.5524 train_loss: 0.681240\tval_acc: 0.725000 val_loss: 0.6473503 test_acc:0.693750\n",
      "9_10 train_acc: 0.6411 train_loss: 0.647913\tval_acc: 0.762500 val_loss: 0.5591375 test_acc:0.740625\n",
      "9_13 train_acc: 0.7177 train_loss: 0.602794\tval_acc: 0.762500 val_loss: 0.5315939 test_acc:0.768750\n",
      "9_14 train_acc: 0.7137 train_loss: 0.551221\tval_acc: 0.762500 val_loss: 0.5142984 test_acc:0.790625\n",
      "9_16 train_acc: 0.7500 train_loss: 0.518326\tval_acc: 0.762500 val_loss: 0.5045341 test_acc:0.796875\n",
      "9_17 train_acc: 0.8226 train_loss: 0.435351\tval_acc: 0.775000 val_loss: 0.5872846 test_acc:0.768750\n",
      "9_21 train_acc: 0.7944 train_loss: 0.476704\tval_acc: 0.800000 val_loss: 0.4895037 test_acc:0.803125\n",
      "9_33 train_acc: 0.7823 train_loss: 0.472922\tval_acc: 0.812500 val_loss: 0.5065743 test_acc:0.821875\n",
      "9_104 train_acc: 0.8589 train_loss: 0.347563\tval_acc: 0.837500 val_loss: 0.5068511 test_acc:0.850000\n",
      "9_369 train_acc: 0.8952 train_loss: 0.269525\tval_acc: 0.850000 val_loss: 0.4522277 test_acc:0.884375\n",
      "9_990 train_acc: 0.9032 train_loss: 0.208427\tval_acc: 0.887500 val_loss: 0.4297806 test_acc:0.903125\n",
      "epoch:  990 \tThe test accuracy is: 0.903125\n",
      " THE BEST ACCURACY IS 0.903125\tkappa is 0.80625\n",
      "subject 9 duration: 0:32:54.647670\n",
      "9 subject 5 fold mean: \n",
      " accuray      88.875000\n",
      "precision    89.333736\n",
      "recall       88.875000\n",
      "f1           88.841873\n",
      "kappa        77.750000\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The average Best accuracy is: 87.99603174603175kappa is: 75.9920634920635\n",
      "\n",
      "best epochs:  [950, 952, 984, 943, 570, 809, 713, 706, 705, 986, 838, 862, 933, 907, 941, 930, 906, 984, 906, 840, 977, 434, 698, 990, 982, 945, 844, 944, 850, 930, 819, 779, 729, 814, 920, 541, 361, 937, 820, 337, 941, 898, 969, 362, 990]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     78.062500  82.740546  78.062500  77.229602  56.125000\n",
      "1     71.214286  71.435455  71.214286  71.141761  42.428571\n",
      "2     82.750000  83.089528  82.750000  82.703109  65.500000\n",
      "3     97.687500  97.705734  97.687500  97.687271  95.375000\n",
      "4     96.812500  97.128354  96.812500  96.800028  93.625000\n",
      "5     87.812500  88.230574  87.812500  87.777378  75.625000\n",
      "6     94.000000  94.036362  94.000000  93.998723  88.000000\n",
      "7     94.750000  94.769710  94.750000  94.749454  89.500000\n",
      "8     88.875000  89.333736  88.875000  88.841873  77.750000\n",
      "mean  87.996032  88.718889  87.996032  87.881022  75.992063\n",
      "std    9.096342   8.537669   9.096342   9.231151  18.192685\n",
      "****************************************\n",
      "Sat Aug 17 23:55:25 2024\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attention-based multi-scale convolutional neural network for motor imagery classification\n",
    "\n",
    "author: zhaowei701@163.com\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "gpu_number = 1\n",
    "gpus = [gpu_number]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_number)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 f1=16, \n",
    "                 pooling_size=52, \n",
    "                 dropout_rate=0.5, \n",
    "                 number_channel=22):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 85), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 65), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )        \n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, 45), (1, 1), padding='same'),\n",
    "            nn.Conv2d(f1, f1, (number_channel, 1), (1, 1), groups=f1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1,pooling_size)), \n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x1 = self.cnn1(x)\n",
    "        x2 = self.cnn2(x)\n",
    "        x3 = self.cnn3(x)\n",
    "        #通道方向合并\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.projection(x)\n",
    "        return x    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 先求和再LayerNorm\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, parameters,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=parameters.f1, \n",
    "                                 pooling_size=parameters.pooling_size, \n",
    "                                 dropout_rate=parameters.dropout_rate,\n",
    "                                 number_channel=parameters.number_channel,\n",
    "                                 ),\n",
    "#             TransformerEncoder(heads, depth, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 parameters,\n",
    "                 database_type='A', \n",
    "                 \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = parameters.emb_size\n",
    "        parameters.number_channel = self.number_channel\n",
    "        self.cnn = BranchEEGNetTransformer(parameters)\n",
    "        self.position = PositioinalEncoding(parameters.emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(parameters.heads, \n",
    "                                        parameters.depth, \n",
    "                                        parameters.emb_size)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.emb_size , self.number_class) \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        b, l, e = x.shape\n",
    "        x = torch.cat((torch.zeros((b, 1, e),requires_grad=True).cuda(),x), 1)\n",
    "        x = x * math.sqrt(self.emb_size)\n",
    "        x = self.position(x)\n",
    "        trans = self.trans(x)\n",
    "        features = trans[:, 0, :]\n",
    "        \n",
    "        out = self.classification(features)\n",
    "        return features, out\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name,\n",
    "                 parameters,\n",
    "                 evaluate_mode = 'LOSO-no',\n",
    "                 dataset_type='A',\n",
    "                 n_fold = 0,\n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.n_fold = n_fold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = parameters.batch_size\n",
    "        self.lr = parameters.learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = parameters.epochs\n",
    "        self.nSub = nsub\n",
    "        self.nFold = n_fold\n",
    "        self.number_augmentation = parameters.number_aug\n",
    "        self.number_seg = parameters.number_seg\n",
    "        self.root = data_dir\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "            database_type=self.dataset_type, \n",
    "            parameters = parameters, \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_nsub_{}_nfold_{}.pth'.format(self.nSub, n_fold+1)\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        # print(\"len(self.allData):\", len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        # print(\"shuffle_num\", shuffle_num)\n",
    "        # print(\"self.allLabel\", self.allLabel)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "\n",
    "        print('-'*20, \n",
    "              \"raw train size：\", self.train_data.shape, \n",
    "              \"test size：\", self.test_data.shape, \n",
    "              \"subject:\", self.nSub,\n",
    "              \"fold:\", self.nFold+1)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "    \n",
    "    def test_model(self, model, dataloader):\n",
    "        model.eval()\n",
    "        outputs_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(dataloader):\n",
    "                # val model\n",
    "                img = img.type(self.Tensor).cuda()\n",
    "                label = label.type(self.LongTensor).cuda()\n",
    "                _, Cls = model(img)\n",
    "                outputs_list.append(Cls)\n",
    "                del img, Cls\n",
    "                torch.cuda.empty_cache()\n",
    "                label_list.append(label)\n",
    "            \n",
    "        Cls = torch.cat(outputs_list)\n",
    "        val_label = torch.cat(label_list)\n",
    "        val_loss = self.criterion_cls(Cls, val_label)\n",
    "        val_pred = torch.max(Cls, 1)[1]\n",
    "        val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "        return val_acc, val_loss, val_pred\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        timg, label, test_data, test_label = self.get_source_data()\n",
    "        \n",
    "        # 训练数据集合做5折交叉验证\n",
    "        # 先分类别\n",
    "        train_data_list_per_class = []\n",
    "        train_label_list_per_class = []\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]        \n",
    "            train_data_list_per_class.append(tmp_data)\n",
    "            train_label_list_per_class.append(tmp_label)\n",
    "            \n",
    "        # 再从每个类别里取出一折用于测试，其他用于训练\n",
    "        train_data_list = []\n",
    "        train_label_list = []\n",
    "        val_data_list, val_label_list =  [], []\n",
    "        seed = 1234+self.nSub\n",
    "        for clsAug in range(self.number_class):\n",
    "            # 计算每个类别的数量\n",
    "            number_samples = len(train_data_list_per_class[clsAug])\n",
    "            number_test = number_samples // 5\n",
    "            # 索引乱序，用于切分训练集和测试集\n",
    "            index_list = list(range(number_samples))\n",
    "            np.random.seed(seed+clsAug)\n",
    "            # 乱序的索引，根据索引取数，相当于对数据集做shuffle\n",
    "            index_shuffled = np.random.permutation(index_list)    \n",
    "            # print(index_shuffled[:10])\n",
    "            # 训练集和测试的索引的索引序号, 用于取出第几折的乱序索引\n",
    "            if self.n_fold!=4 :\n",
    "                index_val = [i for i in range(self.n_fold*number_test, (self.n_fold+1)*number_test)]\n",
    "            else:\n",
    "                #     由于288无法被5整除，最后一折取剩下所有的\n",
    "                index_val = [i for i in range(self.n_fold*number_test, number_samples)]\n",
    "\n",
    "            index_train = [i for i in range(number_samples) if i not in index_val]\n",
    "            # 训练集和测试集的索引\n",
    "            index_train = index_shuffled[index_train]\n",
    "            index_val = index_shuffled[index_val]   \n",
    "            \n",
    "            train_data_class = train_data_list_per_class[clsAug][index_train]\n",
    "            train_label_class = train_label_list_per_class[clsAug][index_train]\n",
    "            train_data_list.append(train_data_class)\n",
    "            train_label_list.append(train_label_class)\n",
    "            \n",
    "            val_data_class = train_data_list_per_class[clsAug][index_val]\n",
    "            val_label_class = train_label_list_per_class[clsAug][index_val]            \n",
    "            val_data_list.append(val_data_class)\n",
    "            val_label_list.append(val_label_class)        \n",
    "        # 合并各类别的各折的数据\n",
    "        img = np.concatenate(train_data_list)\n",
    "        label = np.concatenate(train_label_list)\n",
    "        val_data = np.concatenate(val_data_list)\n",
    "        val_label = np.concatenate(val_label_list)\n",
    "#         print(\"image size:\", img.shape, label.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        print('-'*20, \n",
    "              \"train size：\", img.shape, \n",
    "              \"val size：\", val_data.shape, )\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        val_data = torch.from_numpy(val_data)\n",
    "        val_label = torch.from_numpy(val_label - 1)\n",
    "        val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "                \n",
    "        \n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        weight_decay = 0 if TYPE=='A' else 0.001\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2),\n",
    "                                         weight_decay=weight_decay\n",
    "                                         )\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        max_acc = 0\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img\n",
    "                train_label = label\n",
    "\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_acc, val_loss, _ = self.test_model(self.model, self.val_dataloader)\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if max_acc<val_acc or (max_acc==val_acc and min_loss>val_loss):\n",
    "                    max_acc = val_acc\n",
    "                    min_loss = val_loss\n",
    "                    \n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    test_acc, test_loss, y_pred  = self.test_model(self.model, self.test_dataloader)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f} test_acc:{:.6f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                           test_acc                                     \n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch, outputs\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "def main(dirs,           \n",
    "         paramters,\n",
    "         evaluate_mode = 'subject-dependent', # 评估模式：LOSO（跨个体）或其他（subject-dependent, subject-specific），\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    pred_softmax = ExcelWriter(dirs+\"/pred_softmax.xlsx\")\n",
    "    subjects_result = []\n",
    "    \n",
    "    \n",
    "    best_epochs = []\n",
    "    result_fold = []\n",
    "    for i in range(paramters.subject_number):      \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        # 每个人做5折交叉验证\n",
    "        subjects_result_fold = []\n",
    "        for n_fold in range(5):\n",
    "            exp = ExP(i + 1, DATA_DIR, dirs, \n",
    "                      paramters,\n",
    "                      evaluate_mode = evaluate_mode,\n",
    "                      dataset_type=dataset_type,\n",
    "                      n_fold=n_fold,\n",
    "                      )\n",
    "\n",
    "            testAcc, Y_true, Y_pred, df_process, best_epoch,pred_output = exp.train()\n",
    "            probs = torch.softmax(pred_output, dim=1).cpu().numpy()\n",
    "            df_probs = pd.DataFrame(probs)\n",
    "            df_probs.to_excel(pred_softmax, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "            pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "            df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "            df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "            accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "            subject_result = {'accuray': accuracy*100,\n",
    "                              'precision': precison*100,\n",
    "                              'recall': recall*100,\n",
    "                              'f1': f1*100, \n",
    "                              'kappa': kappa*100\n",
    "                              }\n",
    "            df_process.to_excel(process_write, sheet_name=str(i+1)+'_'+str(n_fold))\n",
    "            best_epochs.append(best_epoch)\n",
    "            print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "            endtime = datetime.datetime.now()\n",
    "            print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "            if i == 0:\n",
    "                yt = Y_true\n",
    "                yp = Y_pred\n",
    "            else:\n",
    "                yt = torch.cat((yt, Y_true))\n",
    "                yp = torch.cat((yp, Y_pred))\n",
    "            subjects_result_fold.append(subject_result)\n",
    "            # 每个人每一折的结果都写入excel存盘\n",
    "            df_result_fold = pd.DataFrame(subjects_result)\n",
    "            \n",
    "        df = pd.DataFrame(subjects_result_fold)\n",
    "        df.to_excel(result_write_metric, index=False,  sheet_name=str(i+1))\n",
    "        result_fold_mean = df.mean()\n",
    "        print(\"{} subject {} fold mean: \\n {}\".format(i+1, n_fold+1, result_fold_mean))\n",
    "        subjects_result.append(result_fold_mean)\n",
    "    df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "    pred_softmax.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False, sheet_name='mean')\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "\n",
    "class Parameters():\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.heads = 8\n",
    "        self.depth = 5\n",
    "        self.emb_size = 16*3\n",
    "        self.f1 = 16\n",
    "        self.pooling_size = 52\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.subject_number = 9\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 72 \n",
    "        \n",
    "        self.epochs=1000\n",
    "        self.number_aug=3\n",
    "        # 训练一个batch的真实数量为 self.batch_size*(1+self.number_aug)\n",
    "        self.number_seg=8\n",
    "        self.gpus=gpus        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'../mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    TYPE = 'B'\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        CNN_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        CNN_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    \n",
    "    parameters = Parameters(CNN_DROPOUT_RATE)\n",
    "\n",
    "    \n",
    "    parameters_list = ['B']\n",
    "    for i in parameters_list:\n",
    "#         parameters.heads = i\n",
    "        TYPE = i\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"MSCFormer_画ROC_{}_heads_{}_depth_{}_pool_{}\".format(TYPE, \n",
    "                                                                       parameters.heads,\n",
    "                                                                       parameters.depth,\n",
    "                                                                       parameters.pooling_size)\n",
    "    \n",
    "        sModel = EEGTransformer(\n",
    "            database_type=TYPE, \n",
    "            parameters = parameters,  \n",
    "            ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "    \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        \n",
    "        result = main(RESULT_NAME,\n",
    "                      parameters,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        dataset_type=TYPE,\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0935c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
